# Bayes factors  {#ch-bf}

This chapter is based on a longer manuscript available on arXiv: @SchadEtAlBF; our terminology used here is based on the conventions used in that paper. A published version of the arXiv article appears in Schad, D. J., Nicenboim, B., Bürkner, P. C., Betancourt, M., and Vasishth, S. (2022). Workflow techniques for the robust use of Bayes factors. Psychological Methods. In this chapter, whenever we refer to the published version of the arXiv paper, we mean this article.

Bayesian approaches provide tools for different aspects of data analysis. A key contribution of Bayesian data analysis to cognitive science is that it furnishes probabilistic ways to quantify the evidence that data provide in support of one model or another. Models provide ways to implement \index{Scientific hypothesis} scientific hypotheses; as a consequence, \index{Model comparison} model comparison and \index{Hypothesis testing} hypothesis testing are closely related.  Bayesian hypothesis testing comparing any kind of hypotheses is implemented using \index{Bayes factor} Bayes factors [@rouder2018bayesian; @schonbrodt2018bayes; @wagenmakers2010BayesianHypothesisTesting; @kass1995bayes; @gronauTutorialBridgeSampling2017; @jeffreys1939theory], which quantify evidence in favor of one statistical (or computational) model over another. This chapter will focus on Bayes factors as the way to compare models and to obtain evidence for (general) hypotheses.

There are subtleties associated with Bayes factors that are not widely appreciated. For example, the results of Bayes factors analyses are highly sensitive to and crucially depend on prior assumptions about model parameters (we will illustrate this below); these priors can vary depending on the experiment/research problem and even differ subjectively among researchers. Many authors use or recommend so-called default prior distributions, where the prior parameters are fixed, and are independent of the scientific problem in question [e.g., @navarro2015learning]. However, \index{Default prior} default priors can result in an overly simplistic perspective on Bayesian hypothesis testing, and can be misleading. For this reason, even though leading experts in the use of the Bayes factor, such as @rouder2009bayesian, often provide default priors for computing Bayes factors, they also make it clear that: "simply put, principled inference is a thoughtful process that cannot be performed by rigid adherence to defaults" [@rouder2009bayesian, p. 235]. However, this observation does not seem to have had much impact on how Bayes factors are used in fields like  psychology and psycholinguistics; the use of default priors when computing Bayes factors seems to be widespread.

Given the key \index{Influence of priors} influence of priors on Bayes factors, \index{Defining priors} defining priors becomes a central issue when using Bayes factors. The priors determine which models will be compared.

In this chapter, we demonstrate how Bayes factors should be used in practical settings in cognitive science. In doing so, we demonstrate the strength of this approach and some important pitfalls that researchers should be aware of.


## Hypothesis testing using the Bayes factor

### Marginal likelihood

\index{Bayes' rule} Bayes' rule can be written with reference to a specific statistical model $\mathcal{M}_1$.

\begin{equation}
p(\boldsymbol{\Theta} \mid \boldsymbol{y}, \mathcal{M}_1) = \frac{p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1)}{p(\boldsymbol{y} \mid \mathcal{M}_1)}
\end{equation}

Here, $\boldsymbol{y}$ refers to the data and $\boldsymbol{\Theta}$ is a vector of parameters; for example, this vector could include the intercept, slope, and variance component in a linear regression model.

The denominator $p(\boldsymbol{y} \mid \mathcal{M}_1)$ is the \index{Marginal likelihood} marginal likelihood, and is a single number that gives us the likelihood of the observed data $\boldsymbol{y}$ given the model $\mathcal{M}_1$ (and only in the discrete case, it gives us the probability of the observed data $\boldsymbol{y}$ given the model; see section \@ref(sec-marginal)). Because in general it's not a probability, it should be interpreted relative to another marginal likelihood (evaluated with the same $\boldsymbol{y}$).

In \index{Frequentist statistics} frequentist statistics, it's also common to quantify evidence for the model by determining the maximum likelihood, that is, the likelihood of the data given the \index{Best-fitting model parameter} best-fitting model parameter. Thus, the data is used twice: once for fitting the parameter, and then for evaluating the likelihood. Importantly, this inference completely hinges upon this best-fitting parameter to be a meaningful value that represents well what we know about the parameter, and doesn't take the uncertainty of the estimates into account. Bayesian inference quantifies the uncertainty that is associated with a parameter; that is, one accepts that the knowledge about the parameter value is uncertain. Computing the marginal likelihood entails computing the likelihood given all plausible values for the model parameter.

One difficulty in the above equation showing Bayes' rule is that the marginal likelihood $p(\boldsymbol{y} \mid \mathcal{M}_1)$ in the denominator cannot in general be easily computed:

\begin{equation}
p(\boldsymbol{\Theta} \mid \boldsymbol{y}, \mathcal{M}_1)  = \frac{p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1)}{p(\boldsymbol{y} \mid \mathcal{M}_1)}
\end{equation}

In the marginal likelihood, the parameters are being integrated out:

\begin{equation}
p(\boldsymbol{y} \mid \mathcal{M}_1) = \int p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1)\, \mathrm{d} \boldsymbol{\Theta}
(\#eq:marginall)
\end{equation}

The likelihood is evaluated for every possible parameter value, weighted by the \index{Prior plausibility} prior plausibility of the parameter values. The product $p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1)$ is then summed up (that is what the integral does).

For this reason, the prior is as important as the likelihood. Equation \@ref(eq:marginall) also looks almost identical to the \index{Prior predictive distribution} prior predictive distribution from section \@ref(sec-priorpred) (that is, the predictions that the model makes before seeing any data). The prior predictive distribution is repeated below for convenience:

\begin{equation}
\begin{aligned}
p(\boldsymbol{y_{pred}}) &= p(y_{pred_1},\dots,y_{pred_n})\\
&= \int_{\boldsymbol{\Theta}} p(y_{pred_1}|\boldsymbol{\Theta})\cdot p(y_{pred_2}|\boldsymbol{\Theta})\cdots p(y_{pred_N}|\boldsymbol{\Theta}) p(\boldsymbol{\Theta}) \, d\boldsymbol{\Theta}
\end{aligned}
\end{equation}

However, while the prior predictive distribution describes possible observations, the marginal likelihood is evaluated on the actually observed data.

Let's compute the Bayes factor for a very simple example case. We assume a study where we assess the number of "successes" observed in a fixed number of trials. For example, suppose that we have 80 "successes" out of 100 trials. A simple model of this data can be built by assuming, as we did in section  \@ref(sec-binomialcloze), that the data are distributed according to a \index{Binomial distribution} binomial distribution.
In a binomial distribution, $n$ independent trials are performed, where the result of each experiment is either a "success" or "no success," with the probability of "success" being $\theta$. The binomial distribution is the probability distribution of the \index{Number of successes} number of successes $k$ (number of "success" responses) in this situation for a given sample of experiments $X$.


Suppose now that we have prior information about the probability parameter $\theta$. As we explained in section \@ref(sec-analytical), a typical prior distribution for $\theta$ is a \index{Beta distribution} beta distribution. The beta distribution defines a probability distribution on the interval $[0, 1]$, which is the interval on which the probability $\theta$ is defined. It has two parameters $a$ and $b$, which determine the shape of the distribution. The prior parameters $a$ and $b$ can be interpreted as the a priori number of "successes" versus "failures."  These could be based on previous evidence, or on the researcher's beliefs, drawing on their domain knowledge [@ohagan2006uncertain].

Here, to illustrate the calculation of the Bayes factor, we assume that the parameters of the beta distribution are $a=4$ and $b=2$. As mentioned above, these parameters can be interpreted as representing "success" ($4$ prior observations representing success), and "no success" ($2$ prior observations representing "no success"). The resulting prior distribution is visualized in Figure \@ref(fig:beta24). A $\mathit{Beta}(a=4,b=2)$ prior on $\theta$ amounts to a \index{Regularizing prior} regularizing prior with some, but no clear prior evidence for more than 50% of success.

```{r beta24, fig.cap = "The beta distribution with parameters a = 4 and b = 2.", echo = FALSE, fig.height =2.2}
ggplot(tibble(theta = c(0, 1)), aes(theta)) +
  stat_function(fun = dbeta, args = list(
    shape1 = 4,
    shape2 = 2
  ))
```



```{r LikelihoodPrior, fig=TRUE, include=TRUE, echo=FALSE, cache=FALSE, fig.width=7, fig.height=6, fig.cap = "Likelihood, priors, and posteriors for two models with differing priors. a) + d) binomial likelihood for 80 successes out of 100 trials. b) + e) Prior beta distribution with parameters a = 4 and b = 2 (b) and with parameters a = 1 and b = 1 (e). c) + f) The likelihood times the prior, i.e., the posterior for each model.", eval = FALSE}
x <- seq(0, 1, 0.01)
y <- dbeta(x, 4, 2)
p <- dbinom(80, 100, x)
dat <- data.frame(x, y, p, z = y * p)
gdat <- dat %>% gather(y, p, z, key = "distr", value = "value")
gdat$distr <- factor(gdat$distr)
levels(gdat$distr) <- c("binomial likelih.", "beta prior", "Likelihood x Prior")
p1 <- ggplot(data = gdat, aes(x = x, y = value)) +
  geom_line() +
  facet_wrap(~distr, scales = "free_y") +
  labs(x = "Probability of success", y = "")
p1a <- ggplot(data = subset(gdat, distr == "binomial likelih."), aes(x = x, y = value)) +
  geom_line() +
  facet_wrap(~distr, scales = "free_y") +
  labs(x = expression(theta), y = "")
p1b <- ggplot(data = subset(gdat, distr == "beta prior"), aes(x = x, y = value)) +
  geom_line() +
  facet_wrap(~distr, scales = "free_y") +
  coord_cartesian(ylim = c(0, 2.2)) +
  labs(x = expression(theta), y = "")
p1c <- ggplot(data = subset(gdat, distr == "Likelihood x Prior"), aes(x = x, y = value)) +
  geom_line() +
  facet_wrap(~distr, scales = "free_y") +
  coord_cartesian(ylim = c(0, 0.22)) +
  labs(x = expression(theta), y = "")



x <- seq(0, 1, 0.01)
y <- dbeta(x, 1, 1)
p <- dbinom(80, 100, x)
dat2 <- data.frame(x, y, p, z = y * p)
gdat2 <- dat2 %>% gather(y, p, z, key = "distr", value = "value")
gdat2$distr <- factor(gdat2$distr)
levels(gdat2$distr) <- c("binomial likelih.", "beta prior", "Likelihood x Prior")
p2 <- ggplot(data = gdat2, aes(x = x, y = value)) +
  geom_line() +
  facet_wrap(~distr, scales = "free_y") +
  labs(x = "Prob. of success", y = "")

p2a <- ggplot(data = subset(gdat2, distr == "binomial likelih."), aes(x = x, y = value)) +
  geom_line() +
  facet_wrap(~distr, scales = "free_y") +
  labs(x = expression(theta), y = "")
p2b <- ggplot(data = subset(gdat2, distr == "beta prior"), aes(x = x, y = value)) +
  geom_line() +
  facet_wrap(~distr, scales = "free_y") +
  coord_cartesian(ylim = c(0, 2.2)) +
  labs(x = expression(theta), y = "")
p2c <- ggplot(data = subset(gdat2, distr == "Likelihood x Prior"), aes(x = x, y = value)) +
  geom_line() +
  facet_wrap(~distr, scales = "free_y") +
  coord_cartesian(ylim = c(0, 0.22)) +
  labs(x = expression(theta), y = "")

# gridExtra::grid.arrange(p1,p2,nrow=2)
gridExtra::grid.arrange(p1a, p1b, p1c, p2a, p2b, p2c, nrow = 2)
library(grid)
grid.text(label = "a)", x = 0.05, y = 0.97, gp = gpar(fontsize = 20))
grid.text(label = "b)", x = 0.39, y = 0.97, gp = gpar(fontsize = 20))
grid.text(label = "c)", x = 0.71, y = 0.97, gp = gpar(fontsize = 20))

grid.text(label = "d)", x = 0.05, y = 0.47, gp = gpar(fontsize = 20))
grid.text(label = "e)", x = 0.39, y = 0.47, gp = gpar(fontsize = 20))
grid.text(label = "f)", x = 0.71, y = 0.47, gp = gpar(fontsize = 20))
```

To compute the marginal likelihood, Equation\ \@ref(eq:marginall) shows that we need to multiply the likelihood with the prior. The marginal likelihood is then the \index{Area under the curve} area under the curve, that is, the likelihood averaged across all possible values for the model parameter (the probability of success).

Based on this data, likelihood, and prior we can calculate the marginal likelihood, that is, the area under the curve, in the following way using `R`:

```{r}
# First we multiply the likelihood with the prior
plik1 <- function(theta) {
  dbinom(x = 80, size = 100, prob = theta) *
    dbeta(x = theta, shape1 = 4, shape2 = 2)
}
# Then we integrate (compute the area under the curve):
(MargLik1 <- integrate(f = plik1, lower = 0, upper = 1)$value)
```

One would prefer a model that gives a higher marginal likelihood, i.e., a higher likelihood of observing the data after integrating out the influence of the model parameter(s) (here: $\theta$). A model will yield a high marginal likelihood if it makes a high proportion of good predictions [i.e., model 2 in Figure \@ref(fig:OccamFactor); the figure is adapted from @bishop2006pattern].
Model predictions are \index{Normalized} normalized, that is, the total probability that models assign to different expected data patterns is the same for all models.
Models that are too flexible (model 3 in Figure \@ref(fig:OccamFactor)) will divide their prior predictive probability density across all of their predictions. Such models can predict many different outcomes. Thus, they likely can also predict the actually observed outcome. However, due to the normalization, they cannot predict it with high probability, because they also predict all kinds of other outcomes. This is true for both models with priors that are too wide or for models with too many parameters. Bayesian model comparison inherently penalizes more complex models, a principle known as *Occam's razor* [@mackay]. This principle favors simpler explanations when multiple viable explanations are possible.


```{r OccamFactor, echo=FALSE, fig.height=2.6, fig.width=5.1825,out.width = "100%", fig.cap="Shown are the schematic marginal likelihoods that each of three models assigns to different possible data sets. The total probability each model assigns to the data is equal to one, i.e., the areas under the curves of all three models are the same. Model 1 (black), the low complexity model, assigns all the probability to a narrow range of possible data, and can predict these possible data sets with high likelihood. Model 3 (light grey) assigns its probability to a large range of different possible outcomes, but predicts each individual observed data set with low likelihood (high complexity model). Model 2 (dark grey) takes an intermediate position (intermediate complexity). The vertical dashed line (dark grey) illustrates where the actual empirically observed data fall. The data most support model 2, since this model predicts the data with highest likelihood. The figure is closely based on Figure 3.13 in Bishop (2006)."}
knitr::include_graphics("cc_figure/OccamFactorBW.png", dpi = 20)
```

By contrast, good models (Figure \@ref(fig:OccamFactor), model 2) will make very specific predictions, where the \index{Specific prediction} specific predictions are consistent with the observed data. Here, all the \index{Predictive probability density} predictive probability density is located at the "location" where the observed data fall, and little probability density is located at other places, providing good support for the model. Of course, specific predictions can also be wrong, when expectations differ from what the observed data actually look like (Figure \@ref(fig:OccamFactor), model 1).

Having a natural Occam's razor is good for posterior inference, i.e., for assessing how much (continuous) evidence there is for one model or another. However, it doesn't necessarily imply good \index{Decision making} decision making or \index{Hypothesis testing} hypothesis testing, i.e., to make discrete decisions about which model explains the data best, or on which model to base further actions.

Here, we provide two examples of more flexible models. First, the following model assumes the same likelihood  and the same distribution function for the prior. However, we assume a flat, \index{Uninformative prior} uninformative prior, with prior parameters $a = 1$ and $b = 1$ (i.e., only one prior "success" and one prior "failure"), which provides more prior spread than the first model. Again, we can formulate our model as multiplying the likelihood with the prior, and integrate out the influence of the parameter $\theta$:

```{r}
plik2 <- function(theta) {
  dbinom(x = 80, size = 100, prob = theta) *
    dbeta(x = theta, shape1 = 1, shape2 = 1)
}
(MargLik2 <- integrate(f = plik2, lower = 0, upper = 1)$value)
```

We can see that this second model is more flexible: due to the more spread-out prior, it is compatible with a larger range of possible observed data patterns. However, when we integrate out the $\theta$ parameter to obtain the marginal likelihood, we can see that this flexibility also comes with a cost: the model has a smaller marginal likelihood ($0.0099$) than the first model ($0.02$). Thus, on average (averaged across all possible values of $\theta$) the second model performs worse in explaining the specific data that we observed compared to the first model, and has less support from the data.

A model might be more "complex" because it has a more spread-out prior, or alternatively because it has a more \index{Complex likelihood function} complex likelihood function, which uses a larger number of parameters to explain the same data. Here we implement a third model, which assumes a more complex likelihood by using a \index{Beta-binomial distribution} beta-binomial distribution. The beta-binomial distribution is similar to the binomial distribution, with one important difference: In the binomial distribution the probability of success $\theta$ is fixed across trials. In the beta-binomial distribution, the probability of success is fixed for each trial, but is drawn from a beta distribution across trials. Thus, $\theta$ can differ between trials. In the beta-binomial distribution, we thus assume that the likelihood function is a combination of a binomial distribution and a beta distribution of the probability $\theta$, which yields:

\begin{equation}
p(X = k \mid a, b) = \frac{B(k+a, n-k+b)}{B(a,b)}
\end{equation}

What is important here is that this more complex distribution has two parameters ($a$ and $b$; rather than one, $\theta$) to explain the same data. We assume \index{Log-normally distributed prior} log-normally distributed priors for the $a$ and $b$ parameters, with location zero and scale $100$. The likelihood of this combined beta-binomial distribution is given by the `R` function `dbbinom()` in the package `extraDistr`. We can now write down the likelihood times the priors (given as log-normal densities, `dlnorm()`), and integrate out the influence of the two free model parameters $a$ and $b$ using numerical integration (applying \index{\texttt{integrate}} `integrate()` twice):

```{r}
plik3 <- function(a, b) {
  dbbinom(x = 80, size = 100, alpha = a, beta = b) *
    dlnorm(x = a, meanlog = 0, sdlog = 100) *
    dlnorm(x = b, meanlog = 0, sdlog = 100)
}
# Compute marginal likelihood by applying integrate twice
f <- function(b) {
  integrate(function(a) plik3(a, b), lower = 0, upper = Inf)$value
  }
# integrate requires a vectorized function:
(MargLik3 <- integrate(Vectorize(f), lower = 0, upper = Inf)$value)
```

The results show that this third model has an even lower marginal likelihood compared to the first two ($0.00000707$). With its two parameters $a$ and $b$, this third model has a lot of flexibility to explain a lot of different patterns of observed empirical results. However, again, this increased flexibility comes at a cost, and the simple pattern of observed data does not seem to require such complex model assumptions. The small value for the marginal likelihood indicates that this complex model has less support from the data.

That is, for this present simple example case, we would prefer model 1 over the other two, since it has the largest marginal likelihood ($0.02$), and we would prefer model 2 over model 3, since the marginal likelihood of model 2 ($0.0099$) is larger than that of model 3 ($0.00000707$). The decision about which model is preferred is based on comparing the marginal likelihoods.

### The Bayes factor

The \index{Bayes factor} Bayes factor is a measure of \index{Relative evidence} relative evidence, the comparison of the \index{Predictive performance} predictive performance of one model against another one. This comparison is a \index{Ratio of marginal likelihoods} ratio of marginal likelihoods:

\begin{equation}
BF_{12} = \frac{P(\boldsymbol{y} \mid \mathcal{M}_1)}{P(\boldsymbol{y} \mid \mathcal{M}_2)}
\end{equation}

$BF_{12}$ indicates the extent to which the data are more likely under $\mathcal{M}_1$ over $\mathcal{M}_2$, or in other words, the relative evidence that we have for $\mathcal{M}_1$ over $\mathcal{M}_2$. Values larger than one indicate evidence in favor of $\mathcal{M}_1$, smaller than one indicate evidence in favor of $\mathcal{M}_2$, and values close to one indicate that the evidence is inconclusive. This model comparison does not depend on a specific parameter value. Instead, all possible prior parameter values are taken into account simultaneously. This is in contrast with the likelihood ratio test, as it is explained in the online section \@ref(app-likR).



A scale (see Table\ \@ref(tab:BFs)) has been proposed to interpret Bayes factors according to the strength of evidence in favor of one model (corresponding to some hypothesis) over another [@jeffreys1939theory]; but this scale should not be regarded as a hard and fast rule with clear boundaries.

Table: (\#tab:BFs) The \index{Bayes factor scale} Bayes factor scale as proposed by @jeffreys1939theory. This scale should not be regarded as a hard and fast rule.

| $BF_{12}$ | Interpretation |
|--: |:--|
| $>100$ | Extreme evidence for $\mathcal{M}_1$. |
|$30-100$| Very strong evidence for $\mathcal{M}_1$. |
|$10-30$ | Strong evidence for $\mathcal{M}_1$. |
|$3-10$| Moderate evidence for $\mathcal{M}_1$. |
|$1-3$ | Anecdotal evidence for $\mathcal{M}_1$. |
|$1$| No evidence.  |
|$\frac{1}{1}-\frac{1}{3}$     | Anecdotal evidence for $\mathcal{M}_2$. |
|$\frac{1}{3}-\frac{1}{10}$  | Moderate evidence for $\mathcal{M}_2$. |
|$\frac{1}{10}-\frac{1}{30}$    | Strong evidence for $\mathcal{M}_2$. |
|$\frac{1}{30}-\frac{1}{100}$| Very strong evidence for $\mathcal{M}_2$. |
| $<\frac{1}{100}$ | Extreme evidence for $\mathcal{M}_2$. |


So if we go back to our previous example, we can calculate $BF_{12}$, $BF_{13}$, and $BF_{23}$. The subscript represents the order in which the models are compared; for example, $BF_{21}$ is simply $\frac{1}{BF_{12}}$.

\begin{equation}
BF_{12} = \frac{marginal \; likelihood \; model \; 1}{marginal \; likelihood \; model \; 2} = \frac{MargLik1}{MargLik2} = `r round(MargLik1/MargLik2,1)`
\end{equation}


\begin{equation}
BF_{13} = \frac{MargLik1}{MargLik3}=  `r round(MargLik1/MargLik3,1)`
\end{equation}

\begin{equation}
BF_{32} = \frac{MargLik3}{MargLik2} =  `r MargLik3/MargLik2` = \frac{1}{BF_{23}} =  \frac{1}{`r round(MargLik2/MargLik3,1)` }
\end{equation}

However, if we want to know, given the data $y$, what the probability for model $\mathcal{M}_1$ is, or how much more probable model $\mathcal{M}_1$ is than model $\mathcal{M}_2$, then we need the \index{Prior odds} prior odds, that is, we need to specify how probable $\mathcal{M}_1$ is compared to $\mathcal{M}_2$ *a priori*.

\begin{align}
\frac{p(\mathcal{M}_1 \mid y)}{p(\mathcal{M}_2 \mid y)} =& \frac{p(\mathcal{M}_1)}{p(\mathcal{M}_2)} \times \frac{P(y \mid \mathcal{M}_1)}{P(y \mid \mathcal{M}_2)}
\end{align}


\begin{align}
\text{Posterior odds}_{12} = & \text{Prior odds}_{12} \times BF_{12}
\end{align}

\index{Posterior odds}

The Bayes factor indicates the amount by which we should update our \index{Relative belief} relative belief between the two models in light of the data and priors.
However, **the Bayes factor alone cannot tell us which one of the models is the most probable**. Given our priors for the models and the Bayes factor, we can calculate the odds between the models.

Here we compute posterior model probabilities for the case where we compare two models against each other. However, \index{Posterior model probability} posterior model probabilities can also be computed for the more general case, where more than two models are considered:

\begin{equation}
p(\mathcal{M}_1 \mid \boldsymbol{y}) = \frac{p(\boldsymbol{y} \mid \mathcal{M}_1) p(\mathcal{M}_1)}{\sum_n p(\boldsymbol{y} \mid \mathcal{M}_n) p(\mathcal{M}_n)}
\end{equation}

For simplicity, we mostly constrain ourselves to two models. (However, the sensitivity analyses we carry out below compare more than two models.)

Bayes factors (and posterior model probabilities) tell us how much evidence the data (and priors) provide in favor of one model or another.
Put differently, they enable us to draw conclusions about the \index{Model space} model space, that is, to determine the degree to which each hypothesis agrees with the available data.

A completely different issue, however, is the question of how to perform (discrete) decisions based on continuous evidence. The question here is: which hypothesis should one choose to maximize utility? While Bayes factors have a clear rationale and justification in terms of the (continuous) evidence they provide, there is not a clear and direct mapping from inferences to how to perform decisions based on them. To derive decisions based on posterior model probabilities, \index{Utility function} utility functions are needed. Indeed, the utility of different possible actions (i.e., to accept and act based on one hypothesis or another) can differ quite dramatically in different situations.
Erroneously rejecting a novel therapy could have significant negative consequences for a researcher attempting to implement life-saving treatment, but adopting the therapy incorrectly might have less of an adverse impact. By contrast, erroneously claiming a new discovery in fundamental research may have bad consequences (low utility), whereas erroneously missing a new \index{Discovery claim} discovery claim may be less problematic if further evidence can be accumulated. Thus, Bayesian evidence (in the form of Bayes factors or posterior model probabilities) must be combined with utility functions in order to perform decisions based on them. For example, this could imply specifying the utility of a \index{True discovery} true discovery and the utility of a \index{False discovery} false discovery. \index{Calibration} Calibration (i.e., simulations) can then be used to derive decisions that maximize overall utility.^[See Schad, D. J., Nicenboim, B., Bürkner, P. C., Betancourt, M., and Vasishth, S. (2022). Workflow techniques for the robust use of Bayes factors. Psychological Methods.]

The question now is: how do we extend the Bayes factor calculation to models that we care about, i.e., to models that represent more realistic data analysis situations. In cognitive science, we typically fit fairly complex hierarchical models with many variance components. The major problem is that we won't be able to calculate the marginal likelihood for hierarchical models (or any other complex model) either analytically or just using the `R` functions shown above. There are two very useful methods for calculating the Bayes factor for complex models: the \index{Savage--Dickey density ratio method} Savage--Dickey density ratio method [@DickeyLientz1970; @wagenmakers2010BayesianHypothesisTesting], and \index{Bridge sampling} bridge sampling [@bennettEfficientEstimationFree1976; @mengSimulatingRatiosNormalizing1996]. 

The Savage--Dickey density ratio method has the advantage that is a straightforward way to compute the Bayes factor, but it is limited to nested models, i.e., models with point null hypotheses (e.g., $\mu = 0$). At the time of writing, the  implementation of the Savage--Dickey method in `brms` can be unstable, especially in cases where the posterior is far away from zero. Despite this instability in some special cases, in many situations it can make sense to use this approach instead of bridge sampling [@Aust2024].

Bridge sampling is in general a much more powerful method, but it requires many more \index{Effective sample} effective samples than what is normally required for parameter estimation, and it may be difficult to obtain stable Bayes factors with very large data sets. In this chapter, we will primarily use bridge sampling from the \index{\texttt{bridgesampling}} `bridgesampling` package [@gronauTutorialBridgeSampling2017; @gronauBridgesamplingPackageEstimating2017] with the function  \index{\texttt{bayes\_factor}} `bayes_factor()` to calculate the Bayes factor in the first examples. 

## Examining the N400 effect with the Bayes factor {#sec-N400BF}

In section \@ref(sec-N400hierarchical) we estimated the effect of \index{Cloze probability} cloze probability on the \index{N400} N400 average signal. This yielded a posterior credible interval for the effect of cloze probability. It is certainly possible to check whether,  e.g., the 95% posterior credible interval overlaps with zero or not. However, such estimation cannot really answer the following question: How much evidence do we have in support for an effect? A 95% credible interval that doesn't overlap with zero, or a high probability mass away from zero may hint that the predictor may be needed to explain the data, but it is not really answering how much evidence we have in favor of an effect [see the online section \@ref(app-null), and for discussion, see @Royall; @wagenmakersPrinciplePredictiveIrrelevance2020; @rouder2018bayesian]. The Bayes factor answers this question about the evidence in favor of an effect by explicitly conducting a model comparison. We will compare a model that assumes the presence of an effect, with a null model that assumes no effect.

As we saw before, the Bayes factor is highly sensitive to the priors. In the example presented above, both models are identical except for the effect of interest, $\beta$, and so the prior on this parameter will play a major role in the calculation of the Bayes factor.

Next, we will run a hierarchical model which includes group-level (or random) intercepts and slopes by items and by subjects. We will use regularizing priors on all the parameters--this speeds up computation and implies realistic expectations about the parameters. However, the \index{Prior} prior on $\beta$ will be crucial for the calculation of the Bayes factor.


One possible way to build a good prior for the parameter $\beta$, which estimates the influence of cloze probability, is as follows (see the online chapter \@ref(ch-priors) for a more detailed discussion on prior selection). The reasoning below is based on domain knowledge; but there is room for differences of opinion here. In a realistic data analysis situation, we would carry out a sensitivity analysis using a range of priors to determine the extent of influence of the priors.

1. One may want to be agnostic regarding the direction of the effect; that means that we will center the prior of $\beta$ on zero by specifying that the mean of the prior distribution is zero. However, we are still not sure about the variance of the prior on $\beta$.
2. One would need to know a bit about the variation on the dependent variable that we are analyzing. After re-analyzing the data from a couple of EEG experiments available from osf.io, we can say that for N400 averages, the standard deviation of the signal is between 8-15 microvolts [@NicenboimPreactivation2019].
3. Based on published estimates of effects in psycholinguistics, we can conclude that they are generally rather small, often representing between 5%-30% of the standard deviation of the dependent variable.
4. The effect of noun predictability on the N400 is one of the most reliable and strongest effects in neurolinguistics (together with the P600 that might even be stronger), and the slope $\beta$ represents the average change in voltage when moving from a cloze probability of zero to one--the strongest prediction effect.

An additional way to obtain good priors  is to perform prior predictive checks [@schad2020toward, also see the online chapter \@ref(ch-workflow), which presents a principled Bayesian workflow]. Here, the idea is to simulate data from the model and the priors, and then to analyze the simulated data using summary statistics. For example, it would be possible to compute the summary statistic of the difference in the N400 between high versus low cloze probability. The simulations would yield a distribution of differences. Arguably, this distribution of differences, that is, the data analyses of the simulated data, are much easier to judge for plausibility than the prior parameters specifying prior distributions. That is, we might find it easier to judge whether a difference in voltage between high and low cloze probability is plausible rather than judging the parameters of the model. For reasons of brevity, we skip this step here.

Instead, we will start with the prior $\beta \sim \mathit{Normal}(0,5)$ (since 5 microvolts is roughly 30% of 15, which is the upper bound of the expected standard deviation of the EEG signal).

```{r}
priors1 <- c(prior(normal(2, 5), class = Intercept),
             prior(normal(0, 5), class = b),
             prior(normal(10, 5), class = sigma),
             prior(normal(0, 2), class = sd),
             prior(lkj(4), class = cor))
```

We load the data set on N400 amplitudes, which has data on cloze probabilities  [@nieuwlandLargescaleReplicationStudy2018]. The cloze probability measure is mean-centered to make the intercept and the random intercepts easier to interpret (i.e., after centering, they represent the grand mean and the average variability around the grand mean across subjects or items).

```{r, tidy = FALSE, message = FALSE}
data(df_eeg)
df_eeg <- df_eeg %>%
  mutate(c_cloze = cloze - mean(cloze))
```

A large number of effective samples will be needed to be able to get stable estimates of the Bayes factor with bridge sampling. For this reason a large number of sampling iterations (`n = 20000`) is specified. The parameter `adapt_delta` is set to $0.9$ to ensure that the posterior sampler is working correctly. For Bayes factors analyses, it's necessary to set the argument \index{\texttt{save\_pars}} `save_pars = save_pars(all = TRUE)`. This setting is a precondition for performing bridge sampling to compute the Bayes factor.

```{r BFmn400h, message = FALSE, results = "hide", eval = !file.exists("dataR/m_N400_h_linear.RDS")}
fit_N400_h_linear <- brm(n400 ~ c_cloze +
                           (c_cloze | subj) + (c_cloze | item),
                         prior = priors1,
                         warmup = 2000,
                         iter = 20000,
                         control = list(adapt_delta = 0.9),
                         save_pars = save_pars(all = TRUE),
                         data = df_eeg)
```


```{r, echo= FALSE}
if (!file.exists("dataR/m_N400_h_linear.RDS")) {
  saveRDS(fit_N400_h_linear, "dataR/m_N400_h_linear.RDS")
} else {
  fit_N400_h_linear <- readRDS("dataR/m_N400_h_linear.RDS")
}
```

Next, take a look at the population-level (or fixed) effects from the Bayesian modeling.

```{r}
fixef(fit_N400_h_linear)
```

```{r, echo = FALSE}
mf <- fixef(fit_N400_h_linear)["c_cloze", "Estimate"]
cl <- fixef(fit_N400_h_linear)["c_cloze", "Q2.5"]
ch <- fixef(fit_N400_h_linear)["c_cloze", "Q97.5"]
```

We can now take a look at the estimates and at the credible intervals. The effect of cloze probability (`c_cloze`) is $`r mf`$ microvolts with a 95% credible interval ranging from $`r cl`$ to $`r ch`$ microvolts. While this provides an initial hint that highly probable words may elicit a stronger N400 compared to low probable words, by just looking at the posterior there is no way to quantify evidence for the question whether this effect is different from zero. Model comparison is needed to answer this question.

To this end, we run the model again, now without the parameter of interest, i.e., the null model. This is a model where our prior for $\beta$ is that it is exactly zero.

```{r, message = FALSE, results = "hide", eval = !file.exists("dataR/m_N400_h_null.RDS")}
fit_N400_h_null <- brm(n400 ~ 1 +
                         (c_cloze | subj) + (c_cloze | item),
                       prior = priors1[priors1$class != "b", ],
                       warmup = 2000,
                       iter = 20000,
                       control = list(adapt_delta = 0.9),
                       save_pars = save_pars(all = TRUE),
                       data = df_eeg)
```
```{r, echo= FALSE}
if (!file.exists("dataR/m_N400_h_null.RDS")) {
  saveRDS(fit_N400_h_null, "dataR/m_N400_h_null.RDS")
} else {
  fit_N400_h_null <- readRDS("dataR/m_N400_h_null.RDS")
}
```


Now everything is ready for computing the log \index{Marginal likelihood} marginal likelihood, that is, the likelihood of the data given the model, after integrating out the model parameters. In the toy examples shown above, we had used the `R` function `integrate()` to perform this integration. This is not possible for the more realistic and more complex models that are considered here because the integrals that have to be solved are too high-dimensional and complex for these simple functions to do the job. Instead, a standard approach to deal with realistic complex models is to use \index{\texttt{Bridge sampling}} bridge sampling [@gronauTutorialBridgeSampling2017; @gronauBridgesamplingPackageEstimating2017]. We perform this integration using the function \index{\texttt{bridge\_sampler}} `bridge_sampler()` for each of the two models:

```{r, eval = any(!file.exists("dataR/margLogLik_linear.RDS", "dataR/margLogLik_null.RDS"))}
margLogLik_linear <- bridge_sampler(fit_N400_h_linear, silent = TRUE)
margLogLik_null <- bridge_sampler(fit_N400_h_null, silent = TRUE)
```
```{r, echo= FALSE, cache = FALSE}
if (!file.exists("dataR/margLogLik_linear.RDS")) {
  saveRDS(margLogLik_linear, "dataR/margLogLik_linear.RDS")
} else {
  margLogLik_linear <- readRDS("dataR/margLogLik_linear.RDS")
}
if (!file.exists("dataR/margLogLik_null.RDS")) {
  saveRDS(margLogLik_null, "dataR/margLogLik_null.RDS")
} else {
  margLogLik_null <- readRDS("dataR/margLogLik_null.RDS")
}
```

This gives us the marginal log likelihoods for each of the models. From these, we can compute the Bayes factors. The \index{\texttt{bayes\_factor}} `bayes_factor()` function is a convenient tool for calculating the Bayes factor.

```{r}
(BF_ln <- bayes_factor(margLogLik_linear, margLogLik_null))
```

Alternatively, the Bayes factor can be computed manually as well. First, compute the difference in marginal log likelihoods, then transform this difference in log likelihoods to the likelihood scale (using `exp()`). A difference in the exponential scale is a ratio: $exp(a-b) = exp(a)/exp(b)$.  This computation yields the Bayes factor. However, the values `exp(ml1)` and `exp(ml2)` are too small to be represented accurately by `R`. Therefore, for numerical reasons, it is important to take the difference first and only then compute the exponential $exp(a-b)$, i.e., `exp(margLogLik_linear$logml - margLogLik_null$logml)`, which yields the same result as the `bayes_factor()` command.

```{r}
exp(margLogLik_linear$logml - margLogLik_null$logml)
```

The Bayes factor is quite large in this example, and furnishes strong evidence for the alternative model, which includes a coefficient representing the effect of cloze probability. That is, under the criteria shown in Table\ \@ref(tab:BFs), the Bayes factor furnishes strong evidence for an effect of cloze probability.

In this example, there was good prior information about the model parameter $\beta$.
What transpires, though, if we are unsure of the prior for the model parameter? Because our prior for $\beta$ is inappropriate, it is possible that we end up comparing the null model with an extremely "bad" alternative model.

For example, assuming that we do not know much about N400 effects, or that we do not want to make strong assumptions, we might be inclined to use an \index{Uninformative prior} uninformative prior. The new prior could look as follows (all the priors except for `b` remain unchanged):

```{r}
priors_vague <- c(prior(normal(2, 5), class = Intercept),
                  prior(normal(0, 500), class = b),
                  prior(normal(10, 5), class = sigma),
                  prior(normal(0, 2), class = sd),
                  prior(lkj(4), class = cor))
```

We can use this uninformative prior in the Bayesian model:

```{r, message = FALSE, results = "hide", eval = !file.exists("dataR/m_N400_h_linearVague.RDS")}
fit_N400_h_linear_vague <- brm(n400 ~ c_cloze +
                                 (c_cloze | subj) + (c_cloze | item),
                               prior = priors_vague,
                               warmup = 2000,
                               iter = 20000,
                               control = list(adapt_delta = 0.9),
                               save_pars = save_pars(all = TRUE),
                               data = df_eeg)
```

```{r, echo= FALSE}
if (!file.exists("dataR/m_N400_h_linearVague.RDS")) {
  saveRDS(fit_N400_h_linear_vague, "dataR/m_N400_h_linearVague.RDS")
} else {
  fit_N400_h_linear_vague <- readRDS("dataR/m_N400_h_linearVague.RDS")
}
```

Interestingly, we can still estimate the effect of cloze probability fairly well:

```{r}
posterior_summary(fit_N400_h_linear_vague, variable = "b_c_cloze")
```

Next, we again perform the bridge sampling for the alternative model.

```{r, eval = any(!file.exists("dataR/margLogLik_linearVague.RDS", "dataR/margLogLik_nullVague.RDS"))}
margLogLik_linear_vague <- bridge_sampler(fit_N400_h_linear_vague,
                                          silent = TRUE)
```

```{r, echo= FALSE, cache = FALSE}
if (!file.exists("dataR/margLogLik_linearVague.RDS")) {
  saveRDS(margLogLik_linear_vague, "dataR/margLogLik_linearVague.RDS")
} else {
  margLogLik_linear_vague <- readRDS("dataR/margLogLik_linearVague.RDS")
}
```

We compute the Bayes factor for the alternative over the null model, $BF_{10}$:



```{r, eval = TRUE}
(BF_lnVague <- bayes_factor(margLogLik_linear_vague,
                            margLogLik_null))
```

This is easier to read as the evidence for null model over the alternative:
```{r}
1 / BF_lnVague[[1]]
```

The result is inconclusive: there is no evidence in favor of or against the effect of cloze probability. The reason for that is that priors are never uninformative when it comes to Bayes factors. The \index{Wide prior} wide prior specifies that both very small and very large effect sizes are possible (with some considerable probability), but there is relatively little evidence in the data for such large effect sizes.

The above example is related to a criticism of Bayes factors by Uri Simonsohn, that Bayes factors can provide evidence in favor of the null and against a very specific alternative model, when the researchers only know the direction of the effect (see https://datacolada.org/78a). This can happen when an uninformative prior is used.

One way to overcome this problem is to actually try to learn about the effect size that we are investigating. This can be done by first running an exploratory experiment and analysis without computing any Bayes factor, and then use the posterior distribution derived from this first experiment to calibrate the priors for the next confirmatory experiment where we do use the Bayes factor [see @verhagenBayesianTestsQuantify2014 for a Bayes Factor test calibrated to investigate replication success]. For examples of this approach used in psycholinguistics, see @lcpaape2024 and @Schoknecht2025.

Another possibility is to examine a lot of different alternative models, where each model uses different prior assumptions.
In this manner, the degree to which the Bayes factor results rely on or are sensitive to the prior assumptions can be examined.
This is an instance of a sensitivity analysis. Recall that the model is the likelihood *and* the priors. We can therefore compare models that only differ in the prior [for an example involving EEG and predictability effects, see @NicenboimPreactivation2019].

### Sensitivity analysis

Here, we perform a \index{Sensitivity analysis} sensitivity analysis by examining Bayes factors for several models. Each model has the same likelihood but a different prior for $\beta$. For all of the priors we assume a normal distribution with a mean of zero. Assuming a mean of zero asserts that we do not make any assumption a priori about the direction of the effect. The standard deviations of the various priors differ.
That is, what differs is the amount of uncertainty about the \index{Effect size} effect size that we allow for in the prior.
While a small standard deviation indicates that we expect the effect to be not very large, a large standard deviation permits very large effect sizes.
Although a model with a wide prior (i.e., large standard deviation) also allocates prior probability to small effect sizes, it allocates much less probability to small effect sizes compared to a model with a narrow prior. Thus, if the effect size is in reality small, then a model with a narrow prior (small standard deviation) will have a better chance of detecting the effect.

Next, we try out a range of standard deviations, ranging from 1 to a much wider prior that has a standard deviation of 100. In practice, for the experiment method we are discussing here, it would not be a good idea to define very large standard deviations such as 100 microvolts, since they imply unrealistically large effect sizes. However, we include such a large value here just for illustration. Such a sensitivity analysis takes a very long time: here, we are running 11 models, where each model involves a lot of iterations to obtain stable Bayes factor estimates.

```{r, results = "hide" , eval = !file.exists("dataR/BFs.RDS")}
prior_sd <- c(1, 1.5, 2, 2.5, 5, 8, 10, 20, 40, 50, 100)
BF <- c()
for (i in 1:length(prior_sd)) {
  psd <- prior_sd[i]
  # for each prior we fit the model
  fit <- brm(n400 ~ c_cloze + (c_cloze | subj) + (c_cloze | item),
    prior =
      c(prior(normal(2, 5), class = Intercept),
        set_prior(paste0("normal(0,", psd, ")"), class = "b"),
        prior(normal(10, 5), class = sigma),
        prior(normal(0, 2), class = sd),
        prior(lkj(4), class = cor)),
    warmup = 2000,
    iter = 20000,
    control = list(adapt_delta = 0.9),
    save_pars = save_pars(all = TRUE),
    data = df_eeg)
  # for each model we run a brigde sampler
  margLogLik_linear_beta <- bridge_sampler(fit, silent = TRUE)
  # we store the Bayes factor compared to the null model
  BF <- c(BF,
          bayes_factor(margLogLik_linear_beta, margLogLik_null)$bf)
}
BFs <- tibble(beta_sd = prior_sd, BF)
```
```{r, echo= FALSE}
if (!file.exists("dataR/BFs.RDS")) {
  saveRDS(BFs, "dataR/BFs.RDS")
} else {
  BFs <- readRDS("dataR/BFs.RDS")
}
```

For each model, we run bridge sampling and we compute the Bayes factor of the model against our baseline or null model, which does not contain a population-level effect of cloze probability ($BF_{10}$). Next, we need a way to visualize all the Bayes factors. We plot them in Figure \@ref(fig:BFpriorsX) as a function of the prior width.

```{r BFpriorsX, fig.cap="Prior sensitivity analysis for the Bayes factor.", echo=FALSE, fig.height = 3.5}
breaks <- c(1 / 100, 1 / 50, 1 / 20, 1 / 10, 1 / 3, 1, 3, 5, 10, 20, 50, 100)
ggplot(BFs, aes(x = beta_sd, y = BF)) +
  geom_point(size = 2) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  scale_x_continuous("Normal prior width (SD)\n") +
  scale_y_log10("BF10", breaks = breaks, labels = MASS::fractions(breaks)) +
  coord_cartesian(ylim = c(1 / 100, 100)) +
  annotate("text", x = 30 * 2, y = 30, label = "Evidence in favor of one M1", size = 5) +
  annotate("text", x = 30 * 2, y = 1 / 30, label = "Evidence in favor of M0", size = 5) +
  theme(axis.text.y = element_text(size = 8)) +
  ggtitle("Bayes factors")
```

\index{Prior sensitivity analysis}

This figure clearly shows that the Bayes factor provides evidence for the alternative model; that is, it provides evidence that the population-level (or fixed) effect cloze probability is needed to explain the data. This can be seen as the Bayes factor is quite large for a range of different values for the prior standard deviation. The Bayes factor is largest for a prior standard deviation of $2.5$, suggesting a rather small size of the effect of cloze probability. If we assume gigantic effect sizes a priori (e.g., standard deviations of 50 or 100), then the evidence for the alternative model is weaker. Conceptually, the data do not fully support such big effect sizes, but start to favor the null model relatively more, when such big effect sizes are tested against the null. Overall, we can conclude that the data provide evidence for a not too large but robust influence of cloze probability on the N400 amplitude.

### \index{Non-nested models} Non-nested models {#sec-BFnonnested}

One important advantage of computing Bayes factors using bridge sampling is that they can be used to compare models that are not nested. In nested models, the simpler model is a special case of the more complex and general model. For example, our previous model of cloze probability was a general model, allowing different influences of cloze probability on the N400. We compared this to a simpler, more specific null model, where the influence of cloze probability was not included, which means that the regression coefficient (population level or fixed effect) for cloze probability was assumed to be set to zero. Such nested models can be also compared using frequentist methods such as the likelihood ratio test (ANOVA).

By contrast, the Bayes factor also makes it possible to compare non-nested models. An example of a non-nested model would be a case where we log-transform the cloze probability variable before using it as a predictor. A model with log cloze probability as a predictor is not a special case of a model with linear cloze probability as predictor. These are just different, alternative models. With Bayes factors, we can compare these non-nested models with each other to determine which receives more evidence from the data. (For clarity: one can only apply model comparison if the same dependent variables is modeled. Hence, we can use the Bayes factor to compare a model containing log-transformed predictors with a model containing untransformed predictors, but we cannot compare a model using an untransformed dependent variable with a model using a log-transformed dependent variable.)

To do so, we first log-transform the cloze probability variable. Some cloze probabilities in the data set are equal to zero. This creates a problem when taking logs, since the log of zero is minus infinity, a value that we cannot use. We are going to overcome this problem by \index{Smoothing} "smoothing" the cloze probability in this example. We use \index{Additive smoothing} additive smoothing [also called Laplace or \index{Lidstone smoothing} Lidstone smoothing; @Lidstone1920; @ChenGoodman1999] with pseudocounts set to one, this means that the smoothed probability is calculated as the number of responses with a given gender plus one divided by the total number of responses plus two.

```{r}
df_eeg <- df_eeg %>%
  mutate(scloze = (cloze_ans + 1) / (N + 2),
         c_logscloze = log(scloze) - mean(log(scloze)))
```

Next, we center the predictor variable, and we scale it to the same standard deviation as the linear cloze probabilities. To implement this scaling, first divide the centered smoothed log cloze probability variable by its standard deviation (effectively creating $z$-scaled values). As a next step, multiply the $z$-scaled values by the standard deviation of the non-transformed cloze probability variable. This way, both predictors (log cloze and cloze) have the same standard deviation. We therefore expect them to have a similar impact on the N400. As a result of this transformation, the same priors can be used for both variables (given that we currently have no specific information about the effect of log cloze probability versus linear cloze probability):

<!--TO-DO: explain here the scaling-->
<!-- If you don't scale the predictors, then when you use raw cloze probability, beta is the change in EEG from cloze 0 to cloze 1, right? So that's more than the entire range of cloze which is from 0.1 to 0.9 (or something like this).
But when you use log_2(cloze), with one unit of change of log_2(cloze) you barely move in cloze scale, since this corresponds 2^-10 to 2^-9, or from 2^-9 to 2^-8, etc.  This is not a big issue, but the two betas will be in very different scales. If we know that the difference in N400 for very low (~.1) and very high (~.9) cloze corresponds to ~ 1microvolt from the previous lit, the beta in the first case (raw cloze) will be a bit larger than 1microvolt (I guess 1.2?), but the beta in the case of log_2 cloze will be much larger.

If one wants to put them in the same scale, to be able to use the same priors, one possibility is to make that for both of them 1 unit corresponds to the entire range (or the same range), so you can make that 0.9-0.1 would be one unit, and also log_2(.9)-log_2(.1). (I think this is what I did). The two betas won't have the same interpretation, but at least they are more comparable, and one can assign the same prior.
-->

```{r}
df_eeg <- df_eeg %>%
  mutate(c_logscloze = scale(c_logscloze) * sd(c_cloze))
```

Then, run a linear mixed-effects model with log cloze probability instead of linear cloze probability, and again carry out bridge sampling.

```{r, message = FALSE, results = "hide", eval = !file.exists("dataR/m_N400_h_log.RDS")}
fit_N400_h_log <- brm(n400 ~ c_logscloze +
                        (c_logscloze | subj) + (c_logscloze | item),
                      prior = priors1,
                      warmup = 2000,
                      iter = 20000,
                      control = list(adapt_delta = 0.9),
                      save_pars = save_pars(all = TRUE),
                      data = df_eeg)
```
```{r, echo= FALSE}
if (!file.exists("dataR/m_N400_h_log.RDS")) {
  saveRDS(fit_N400_h_log, "dataR/m_N400_h_log.RDS")
} else {
  fit_N400_h_log <- readRDS("dataR/m_N400_h_log.RDS")
}
```


```{r, eval = any(!file.exists("dataR/margLogLik_log.RDS"))}
margLogLik_log <- bridge_sampler(fit_N400_h_log, silent = TRUE)
```
```{r, echo= FALSE}
if (!file.exists("dataR/margLogLik_log.RDS")) {
  saveRDS(margLogLik_log, "dataR/margLogLik_log.RDS")
} else {
  margLogLik_log <- readRDS("dataR/margLogLik_log.RDS")
}
```

Next, compare the linear and the log model to each other using Bayes factors.

```{r}
(BF_log_lin <- bayes_factor(margLogLik_log, margLogLik_linear))
```

The results show a Bayes factor of $`r  round(BF_log_lin[[1]],1) `$ of the log model over the linear model. This shows some evidence that log cloze probability is a better predictor of N400 amplitudes than linear cloze probability. This analysis demonstrates that model comparisons using Bayes factor are not limited to nested models, but can also be used for non-nested models. Here, a specific benefit of the Bayes factor is that it considers the functional shape or complexity of the models, instead of merely counting the number of parameters (as is done in information criteria like BIC and AIC).

## The influence of the priors on Bayes factors: beyond the effect of interest

We saw above that the width (or standard deviation) of the prior distribution for the effect of interest had a strong impact on the results from Bayes factor analyses. Thus, one question is whether only the prior for the effect of interest is important, or whether priors for other model parameters can also impact the resulting Bayes factors in an analysis. It turns out that priors for other model parameters can also be important and impact Bayes factors, especially when there are non-linear components in the model, such as in \index{Generalized linear mixed effects models} generalized linear mixed effects models. We investigate this issue by using a simulated data set on a variable that has a \index{Bernoulli distribution} Bernoulli distribution; in each trial, subjects can perform either successfully (`pDV = 1`) on a task, or not (`pDV = 0`). The simulated data is from a factorial experimental design, with one between-subject factor $F$ with 2 levels ($F1$ and $F2$), and Table\ \@ref(tab:cTabXMeans) shows success probabilities for each of the experimental conditions.

```{r}
data("df_BF")
str(df_BF)
```

```{r cTabXMeans, echo=FALSE, message=FALSE, results = "asis", eval=TRUE}
tableBF <- df_BF %>%
  group_by(F) %>%
  summarize(N = length(pDV), M = mean(pDV)) %>%
  as.data.frame()
names(tableBF) <- c("Factor A", "N data", "Means")
kableExtra::kable(tableBF %>%
  mutate(across(where(is.integer), ~ paste0("$",.x, "$"))) %>%
  mutate(across(where(is.double), ~ paste0("$",formatC(round(.x, 2),format = "f", digits=2), "$"))),
 digits = 2, escape = FALSE,
 booktabs = TRUE,  vline="",
  caption = "Summary statistics per condition for the simulated data."
)

```

Our question now is whether there is evidence for a difference in success probabilities between groups $F1$ and $F2$.
As contrasts for the factor $F$, we use scaled sum coding $(-0.5, +0.5)$.

```{r, echo=TRUE}
contrasts(df_BF$F) <- c(-0.5, +0.5)
```

Next, we proceed to specify our priors. For the difference between groups ($F1$ versus $F2$),  define a normally distributed prior with a mean of $0$ and a standard deviation of $0.5$. Thus, we do not specify a direction of the difference a priori, and assume not too large effect sizes. Now run two logistic `brms` models, one with the group factor $F$ included, and one without the factor $F$, and compute Bayes factors using bridge sampling to obtain the evidence that the data provide for the alternative hypothesis that a group difference exists between levels $F1$ and $F2$.

So far, we have only specified the prior for the effect size. The question we are asking now is whether priors on other model parameters can impact the Bayes factor computations for testing the group effect. Specifically, can the prior for the intercept influence the Bayes factor for the group difference? The results show that the answer is yes, in some situations. Let's have a look at this in more detail. Let's assume that we compare two different priors for the intercept. We specify each as a normal distribution with a standard deviation of $0.1$, thus, specifying relatively high certainty a priori where the intercept of the data will fall. The only difference that we now specify, is that one time, the prior mean (on the latent logistic scale) is set to $0$, corresponding to a prior mean probability of $0.5$. In the other condition, we specify a prior mean of $2$, corresponding to a prior mean probability of $0.88$. When we look at the data (see Table\ \@ref(tab:cTabXMeans)) we see that the prior mean of $0$ (i.e., prior probability for the intercept of $0.5$) is not very compatible with the data, whereas the prior mean of $2$ (i.e., a prior probability for the intercept of $0.88$) is quite closely aligned with the actual data.

We now compute Bayes factors for the group difference ($F1$ versus $F2$) by using these different priors for the intercept. Thus, we first fit a null ($M0$) and alternative ($M1$) model under a prior that is misaligned with the data (a narrow distribution centered in zero), and perform bridge sampling for these models:

```{r bayesfactorh0h1, echo=TRUE, message=FALSE, results="hide", eval = !file.exists("dataR/mLL_binom_H0.RDS") | !file.exists("dataR/mLL_binom_H1.RDS")}
# set prior
priors_logit1 <- c(prior(normal(0, 0.1), class = Intercept),
                   prior(normal(0, 0.5), class = b))
# Bayesian GLM: M0
fit_pDV_H0 <- brm(pDV ~ 1,
                  data = df_BF,
                  family = bernoulli(link = "logit"),
                  prior = priors_logit1[-2, ],
                  save_pars = save_pars(all = TRUE))
# Bayesian GLM: M1
fit_pDV_H1 <- brm(pDV ~ 1 + F,
                  data = df_BF,
                  family = bernoulli(link = "logit"),
                  prior = priors_logit1,
                  save_pars = save_pars(all = TRUE))
# bridge sampling
mLL_binom_H0 <- bridge_sampler(fit_pDV_H0, silent = TRUE)
mLL_binom_H1 <- bridge_sampler(fit_pDV_H1, silent = TRUE)
```

```{r ,echo = FALSE}
if (!file.exists("dataR/mLL_binom_H0.RDS") | !file.exists("dataR/mLL_binom_H1.RDS")) {
  saveRDS(mLL_binom_H0, "dataR/mLL_binom_H0.RDS")
  saveRDS(mLL_binom_H1, "dataR/mLL_binom_H1.RDS")
} else {
  mLL_binom_H0 <- readRDS("dataR/mLL_binom_H0.RDS")
  mLL_binom_H1 <- readRDS("dataR/mLL_binom_H1.RDS")
}
```

Next, get ready for computing Bayes factors by again running the null ($M0$) and the alternative ($M1$) model, now assuming a more realistic prior for the intercept (prior mean $= 2$).

```{r bfh0h012, echo=TRUE, message=FALSE, results="hide", eval =  !file.exists("dataR/mLL_binom_H0_2.RDS") | !file.exists("dataR/mLL_binom_H1_2.RDS")}
priors_logit2 <- c(prior(normal(2, 0.1), class = Intercept),
                   prior(normal(0, 0.5), class = b))
# Bayesian GLM: M0
fit_pDV_H0_2 <- brm(pDV ~ 1,
                    data = df_BF,
                    family = bernoulli(link = "logit"),
                    prior = priors_logit2[-2, ],
                    save_pars = save_pars(all = TRUE))
# Bayesian GLM: M1
fit_pDV_H1_2 <- brm(pDV ~ 1 + F,
                    data = df_BF,
                    family = bernoulli(link = "logit"),
                    prior = priors_logit2,
                    save_pars = save_pars(all = TRUE))
# bridge sampling
mLL_binom_H0_2 <- bridge_sampler(fit_pDV_H0_2, silent = TRUE)
mLL_binom_H1_2 <- bridge_sampler(fit_pDV_H1_2, silent = TRUE)
```

```{r ,echo = FALSE}
if (!file.exists("dataR/mLL_binom_H0_2.RDS") | !file.exists("dataR/mLL_binom_H1_2.RDS")) {
  saveRDS(mLL_binom_H0_2, "dataR/mLL_binom_H0_2.RDS")
  saveRDS(mLL_binom_H1_2, "dataR/mLL_binom_H1_2.RDS")
} else {
  mLL_binom_H0_2 <- readRDS("dataR/mLL_binom_H0_2.RDS")
  mLL_binom_H1_2 <- readRDS("dataR/mLL_binom_H1_2.RDS")
}
```

Based on these models and bridge samples, we can now compute the Bayes factors in support for $M1$ (i.e., in support of a group-difference between $F1$ and $F2$). We can do so for the unrealistic prior for the intercept (prior mean of $0$) and the more realistic prior for the intercept (prior mean of $2$).

```{r, echo=TRUE}
(BF_binom_H1_H0 <- bayes_factor(mLL_binom_H1, mLL_binom_H0))
(BF_binom_H1_H0_2 <- bayes_factor(mLL_binom_H1_2, mLL_binom_H0_2))
```

The results show that with the realistic prior for the intercept (prior mean $= 2$), the evidence for the $M1$ is quite strong, with a Bayes factor of $BF_{10} =$ `r round(BF_binom_H1_H0_2$bf,1)`. With the unrealistic prior for the intercept (i.e., prior mean $= 0$), by contrast, the evidence for the $M1$ is much reduced, $BF_{10} =$ `r round(BF_binom_H1_H0$bf,1)`, and now only modest.

Thus, when performing Bayes factor analyses, not only can the priors for the effect of interest (here the group difference) impact the results, under certain circumstances priors for other model parameters can too, such as the prior mean for the intercept here. Such an influence will not always be strong, and can sometimes be negligible. There may be many situations, where the exact specification of the intercept does not have much of an effect on the Bayes factor for a group difference. However, such influences can in principle occur, especially in models with non-linear components. Therefore, it is very important to be careful in specifying realistic priors for all model parameters, also including the intercept. A good way to judge whether prior assumptions are realistic and plausible is prior predictive checks, where we simulate data based on the priors and the model and judge whether the simulated data is plausible and realistic.

## \index{Bayes factor in Stan} The Bayes factor in Stan {#sec-stanBF}

The package \index{\texttt{bridgesampling}} `bridgesampling` allows for a straightforward calculation of Bayes factor for Stan models as well. All the limitations and caveats of Bayes factor discussed in this chapter apply to Stan code as much as they apply to `brms` code. The sampling notation (`~`) should not be used in the Stan code; see the online section \@ref(app-tilde) for an explanation.


An advantage of using Stan in comparison with `brms` is Stan's flexibility. We revisit the model implemented before in section \@ref(sec-interstan). We want to assess the evidence for a *positive* effect of \index{Attentional load} attentional load on \index{Pupil size} pupil size against a similar model that assumes no effect. To do this, assume the following likelihood:

\begin{equation}
p\_size_n \sim \mathit{Normal}(\alpha + c\_load_n \cdot \beta_1 + c\_trial \cdot \beta_2 + c\_load \cdot c\_trial \cdot \beta_3, \sigma)
\end{equation}

Define priors for all the $\beta$s as before, with the difference that $\beta_1$ can only have positive values:

\begin{equation}
\begin{aligned}
\alpha &\sim \mathit{Normal}(1000, 500) \\
\beta_1 &\sim \mathit{Normal}_+(0, 100) \\
\beta_2 &\sim \mathit{Normal}(0, 100) \\
\beta_3 &\sim \mathit{Normal}(0, 100) \\
\sigma &\sim \mathit{Normal}_+(0, 1000)
\end{aligned}
\end{equation}

The following Stan model is the direct translation of the new priors and likelihood.

```{r pupilposstan, echo = FALSE}
pupil_pos <- system.file("stan_models", "pupil_pos.stan", package = "bcogsci")
```

```{stan output.var = "pupilintBF_stan", code = readLines(pupil_pos)[1:25],  tidy = TRUE, comment="", eval = FALSE, cache = FALSE, cache.lazy = FALSE}
```

Fit the model with 20000 iterations to ensure that the Bayes factor is stable, and increase the  `adapt_delta` parameter to avoid warnings:

```{r pupilposBF, message = FALSE, eval = !file.exists("dataR/BF_att.RDS")}
data("df_pupil")
df_pupil <- df_pupil %>%
  mutate(c_load = load - mean(load),
         c_trial = trial - mean(trial))
ls_pupil <- list(p_size = df_pupil$p_size,
                 c_load = df_pupil$c_load,
                 c_trial = df_pupil$c_trial,
                 N = nrow(df_pupil))
pupil_pos <- system.file("stan_models",
                         "pupil_pos.stan",
                         package = "bcogsci")
fit_pupil_int_pos <- stan(file = pupil_pos,
                          data = ls_pupil,
                          warmup = 1000,
                          iter = 20000,
                          control = list(adapt_delta = .95))
```

The null model that we defined has $\beta_1 = 0$ and is written in Stan as follows:

```{r pupilnullstan, echo = FALSE}
pupil_null <- system.file("stan_models", "pupil_null.stan", package = "bcogsci")
```


```{stan output.var = "pupilintBFnull_stan", code = readLines(pupil_null)[1:22],  tidy = TRUE, comment="", eval = FALSE, cache = FALSE, cache.lazy = FALSE}
```

```{r, message = FALSE , eval = !file.exists("dataR/BF_att.RDS")}
pupil_null <- system.file("stan_models",
                          "pupil_null.stan",
                          package = "bcogsci")
fit_pupil_int_null <- stan(file = pupil_null,
                           data = ls_pupil,
                           warmup = 1000,
                           iter = 20000)
```

Compare the models with bridge sampling:

```{r BFpupil , eval = !file.exists("dataR/BF_att.RDS")}
lml_pupil <- bridge_sampler(fit_pupil_int_pos, silent = TRUE)
lml_pupil_null <- bridge_sampler(fit_pupil_int_null, silent = TRUE)
BF_att <- bridgesampling::bf(lml_pupil, lml_pupil_null)
```

```{r ,echo = FALSE}
if (!file.exists("dataR/BF_att.RDS")) {
  saveRDS(BF_att, "dataR/BF_att.RDS")
} else {
  BF_att <- readRDS("dataR/BF_att.RDS")
}
```

```{r}
BF_att
```

We find that the data is `r BF_att[[1]]` more likely under a model that assumes a positive effect of load than under a model that assumes no effect.


## Bayes factors in theory and in practice

### Bayes factors in theory: Stability and \index{Accuracy} accuracy

One question that we can ask here is how stable and accurate the estimates of Bayes factors are. The bridge sampling algorithm  needs a lot of posterior samples to obtain stable estimates of the Bayes factor. Running bridge sampling based on a too small an effective sample size will yield unstable estimates of the Bayes factor, such that repeated computations will yield radically different Bayes factor values. Moreover, even if the Bayes factor is approximated in a stable way, it is unclear whether this approximate Bayes factor is equal to the true Bayes factor, or whether there is \index{Bias} bias in the computation such that the \index{Approximate Bayes factor} approximate Bayes factor has a wrong value. We show this below.

#### Instability due to the effective number of posterior samples

The \index{Number of iterations} number of iterations, which in turn affects the total \index{Number of posterior samples} number of posterior samples can have a strong impact on the \index{Robustness} robustness of the results of the bridge sampling algorithm (i.e., on the resulting Bayes factor) and there are no good theoretical guarantees that bridge sample will yield accurate estimates of Bayes factors. In the analyses presented above, we set the number of iterations to a very large number of $n = 20000$. The sensitivity analysis therefore took a considerable amount of time. It turns out that the results from this analysis were stable, as shown below.

Running the same analysis with less iterations will induce some instability in the Bayes factor estimates based on the bridge sampling, such that running the same analysis twice would yield different results. Furthermore, due to variations in starting values, bridge sampling itself may be unstable and yield inconsistent results for successive runs on the same posterior samples. This is highly concerning because if the number of effective sample sizes is not large enough, the results published in a paper may not be stable.
Indeed, the default number of iterations in `brms` is set as `iter = 2000` (and the default number of warmup iterations is `warmup = 1000`). These defaults were not set to support \index{Bridge sampling} bridge sampling, i.e., they were not defined for computation of densities to support Bayes factors calculations. These defaults are reasonable only for posterior inference on expectations (e.g., posterior means) for models that are not too complex. However, when using these defaults for estimation of densities and the computation of Bayes factors, \index{Instability} instabilities can arise.

As an illustration, we perform the same \index{Sensitivity analysis} sensitivity analysis again, now using the default number of $2000$ iterations in `brms`. The posterior sampling process now runs much quicker. Moreover, we check the stability of the Bayes factors in the sensitivity analyses by repeating both sensitivity analyses (with $n = 20000$ iterations and with the default number of $n = 2000$ iterations) a second time, to see whether the results for Bayes factors are stable.

```{r, echo=FALSE, message=FALSE, results = "hide", eval = !file.exists("dataR/m_N400_h_null2.RDS")}
m_N400_h_null2 <- brm(n400 ~ 1 + (c_cloze | subj) + (c_cloze | item),
                      prior = priors1[-2, ],
                      # default settings
                      # warmup  = 1000,
                      # iter    = 2000,
                      control = list(adapt_delta = 0.9),
                      save_pars = save_pars(all = TRUE),
                      data = df_eeg)
lml_null2 <- bridge_sampler(m_N400_h_null2, silent = TRUE)
```
```{r, echo= FALSE}
if (!file.exists("dataR/m_N400_h_null2.RDS")) {
  saveRDS(m_N400_h_null2, "dataR/m_N400_h_null2.RDS")
} else {
  m_N400_h_null2 <- readRDS("dataR/m_N400_h_null2.RDS")
}
if (!file.exists("dataR/lml_null2.RDS")) {
  saveRDS(lml_null2, "dataR/lml_null2.RDS")
} else {
  lml_null2 <- readRDS("dataR/lml_null2.RDS")
}
```

```{r, echo=FALSE, message = FALSE, results = "hide" , eval = !file.exists("dataR/BFs2.RDS")}
prior_sd <- c(1, 1.5, 2, 2.5, 5, 8, 10, 20, 40, 50, 100)
BFs2 <- map_dfr(prior_sd, function(psd) {
  gc()
  # for each prior we fit the model
  fit <- brm(n400 ~ c_cloze + (c_cloze | subj) + (c_cloze | item),
             prior = c(prior(normal(2, 5), class = Intercept),
                       set_prior(paste0("normal(0,", psd, ")"), class = "b"),
                       prior(normal(10, 5), class = sigma),
                       prior(normal(0, 2), class = sd),
                       prior(lkj(4), class = cor)),
             # default settings
             # warmup  = 1000,
             # iter    = 2000,
             control = list(adapt_delta = 0.9),
             save_pars = save_pars(all = TRUE),
             data = df_eeg)
  # for each model we run a brigde sampler
  lml_linear_beta <- bridge_sampler(fit, silent = TRUE)
  # we store the Bayes factor compared to the null model
  tibble(beta_sd = psd, BF = bayes_factor(lml_linear_beta, lml_null2)$bf)
})
```
```{r, results = "hide", echo=FALSE , eval = !file.exists("dataR/BFs3.RDS"), echo=FALSE}
prior_sd <- c(1, 1.5, 2, 2.5, 5, 8, 10, 20, 40, 50, 100)
BFs3 <- map_dfr(prior_sd, function(psd) {
  gc()
  # for each prior we fit the model
  fit <- brm(n400 ~ c_cloze + (c_cloze | subj) + (c_cloze | item),
             prior = c(prior(normal(2, 5), class = Intercept),
                       set_prior(paste0("normal(0,", psd, ")"), class = "b"),
                       prior(normal(10, 5), class = sigma),
                       prior(normal(0, 2), class = sd),
                       prior(lkj(4), class = cor)),
             # default settings
             # warmup  = 1000,
             # iter    = 2000,
             control = list(adapt_delta = 0.9),
             save_pars = save_pars(all = TRUE),
             data = df_eeg
             )
  # for each model we run a brigde sampler
  lml_linear_beta <- bridge_sampler(fit, silent = TRUE)
  # we store the Bayes factor compared to the null model
  tibble(beta_sd = psd, BF = bayes_factor(lml_linear_beta, lml_null2)$bf)
})
```
```{r, echo= FALSE}
if (!file.exists("dataR/BFs2.RDS")) {
  saveRDS(BFs2, "dataR/BFs2.RDS")
} else {
  BFs2 <- readRDS("dataR/BFs2.RDS")
}
if (!file.exists("dataR/BFs3.RDS")) {
  saveRDS(BFs3, "dataR/BFs3.RDS")
} else {
  BFs3 <- readRDS("dataR/BFs3.RDS")
}
```
```{r, echo=FALSE, results = "hide" , eval = !file.exists("dataR/BFs1.RDS")}
prior_sd <- c(1, 1.5, 2, 2.5, 5, 8, 10, 20, 40, 50, 100)
BFs1 <- map_dfr(prior_sd, function(psd) {
  gc()
  # for each prior we fit the model
  fit <- brm(n400 ~ c_cloze + (c_cloze | subj) + (c_cloze | item),
             prior = c(prior(normal(2, 5), class = Intercept),
                       set_prior(paste0("normal(0,", psd, ")"), class = "b"),
                       prior(normal(10, 5), class = sigma),
                       prior(normal(0, 2), class = sd),
                       prior(lkj(4), class = cor)),
             warmup = 2000,
             iter = 20000,
             control = list(adapt_delta = 0.9),
             save_pars = save_pars(all = TRUE),
             data = df_eeg)
  # for each model we run a brigde sampler
  lml_linear_beta <- bridge_sampler(fit, silent = TRUE)
  # we store the Bayes factor compared to the null model
  tibble(beta_sd = psd, BF = bayes_factor(lml_linear_beta, margLogLik_null)$bf)
})
```
```{r, echo= FALSE}
if (!file.exists("dataR/BFs1.RDS")) {
  saveRDS(BFs1, "dataR/BFs1.RDS")
} else {
  BFs1 <- readRDS("dataR/BFs1.RDS")
}
```


```{r, echo=FALSE, results = "hide", echo=FALSE , eval = !file.exists("dataR/BFsX.RDS")}
prior_sd <- c(1, 1.5, 2, 2.5, 5, 8, 10, 20, 40, 50, 100)
BFsX <- data.frame()
for (i in 1:20) {
  BFsXX <- map_dfr(prior_sd, function(psd) {
    gc()
    # for each prior we fit the model
    fit <- brm(n400 ~ c_cloze + (c_cloze | subj) + (c_cloze | item),
               prior = c(prior(normal(2, 5), class = Intercept),
                         set_prior(paste0("normal(0,", psd, ")"), class = "b"),
                         prior(normal(10, 5), class = sigma),
                         prior(normal(0, 2), class = sd),
                         prior(lkj(4), class = cor)),
               # default settings
               # warmup  = 1000,
               # iter    = 2000,
               control = list(adapt_delta = 0.9),
               save_pars = save_pars(all = TRUE),
               data = df_eeg
               )
    # for each model we run a brigde sampler
    lml_linear_beta <- bridge_sampler(fit, silent = TRUE)
    # we store the Bayes factor compared to the null model
    tibble(beta_sd = psd, BF = bayes_factor(lml_linear_beta, lml_null2)$bf)
  })
  BFsXX$iter <- i
  BFsX <- rbind(BFsX, BFsXX)
}
```
```{r, echo= FALSE}
if (!file.exists("dataR/BFsX.RDS")) {
  saveRDS(BFsX, "dataR/BFsX.RDS")
} else {
  BFsX <- readRDS("dataR/BFsX.RDS")
}
```

```{r BFpriors2, echo=FALSE, message=FALSE, fig.cap="The effect of the number of samples on a prior sensitivity analysis for the Bayes factor. Black lines show 2 runs with 20,000 iterations. Grey lines show 20 runs with default number of iterations (2000).", fig.height=3.5, warning=FALSE}

BFs12 <- rbind(
  cbind(BFs, iter = "Many iterations (20000); run 1"),
  cbind(BFs1, iter = "Many iterations (20000); run 2")#,
  #cbind(BFs2, iter = "Default iterations (2000); run 1"),
  #cbind(BFs3, iter = "Default iterations (2000); run 2")
)
BFs12$iter <- factor(BFs12$iter)
tmpYY <- (as.numeric(BFs12$iter) / 2) %% 1
BFs12$beta_SD <- BFs12$beta_sd + tmpYY - 0.25
breaks <- c(1 / 1000, 1 / 500, 1 / 200, 1 / 100, 1 / 50, 1 / 20, 1 / 10, 1 / 3, 1, 3, 5, 10, 20, 50, 100, 200, 500, 1000)

ggplot() +
  geom_line(data = BFsX, aes(x = beta_sd, y = BF, group = iter), colour = "grey") +
  geom_point(data = BFs12, aes(x = beta_SD, y = BF, shape = iter), size = 2) +
  geom_line(data = BFs12, aes(x = beta_SD, y = BF, linetype = iter)) +
  geom_hline(yintercept = 1, linetype = "dashed") +
  scale_x_continuous("Normal prior width (SD)\n") +
  scale_y_log10("BF10", breaks = breaks, labels = MASS::fractions(breaks)) +
  scale_shape_discrete(name = "") +
  scale_linetype_discrete(name = "") +
  coord_cartesian(ylim = c(1 / 1000, 1000)) +
  annotate("text", x = 38 * 2, y = 200, label = "Evidence in favor of M1", size = 5) +
  annotate("text", x = 38 * 2, y = 1 / 200, label = "Evidence in favor of M0", size = 5) +
  theme(
    axis.text.y = element_text(size = 8),
    legend.position = c(0.25, 0.25),
    legend.background = element_blank()
  ) +
  ggtitle("Bayes factors")

```

The results displayed in Figure \@ref(fig:BFpriors2) show that the Bayes factors are highly unstable when the number of iterations is low. They clearly deviate from the Bayes factors estimated with $20000$ iterations, resulting in very \index{Unstable estimate} unstable estimates. By contrast, the analyses using $20000$ iterations provide nearly the same results in both analyses. The two lines lie virtually directly on top of each other; the points are jittered horizontally for better visibility.

This demonstrates that it is necessary to use a large number of iterations when computing Bayes factors using bridge sampling. In practice, one should compute the sensitivity analysis (or at least one of the models or priors) twice (as we did here) to make sure that the results are stable and sufficiently similar, in order to provide a good basis for reporting results.

By contrast, as mentioned earlier as well, Bayes factors based on the Savage--Dickey method (as implemented in `brms`) can be unstable even when using a large number of posterior samples. This problem can arise especially when the posterior is very far from zero, and thus very large or very small Bayes factors are obtained. (In such cases, the precise value of the Bayes factor may sometimes not matter in practical applications.) However, because of this instability of the Savage-Dickey method in `brms`, it is a good idea to use bridge sampling, and to check the stability of the estimates using more than one method.

#### Inaccuracy of Bayes factor estimates: Does the estimate approximate the true \index{Bayes factor} Bayes factor well?

An important point about approximate estimates of Bayes factors using bridge sampling is that there are no strong guarantees for their \index{Accuracy} accuracy. That is, even if we can show that the \index{Approximated Bayes factor estimate} approximated Bayes factor estimate using bridge sampling is stable (i.e., when using sufficient effective samples, see the analyses above), even then it remains unclear whether the Bayes factor estimate actually is close to the true Bayes factor.
The estimated Bayes factors based on bridge sampling may be stable, but, in theory, could be biased: they may not be very close to the true (correct) Bayes factor.
The technique of (marginal) simulation-based calibration \index{SBC} [SBC; @geweke2004getting; @talts2018validating; @schad2020toward] can be used to investigate this question (SBC is also discussed in section \@ref(sec-validSBC) in chapter \@ref(ch-custom)). We investigate this question next.^[For details, see Schad, D. J., Nicenboim, B., Bürkner, P. C., Betancourt, M., and Vasishth, S. (2022). Workflow techniques for the robust use of Bayes factors. Psychological Methods.] 

In the SBC approach, the priors are used to simulate data. Then, posterior inference is done on the simulated data, and the posterior can be compared to the prior. If the posteriors are equal to the priors, then this supports accurate computations. Applied to Bayes factor analyses, one defines a prior on the hypothesis space, i.e., one defines the prior probabilities for a null and an alternative model, specifying how likely each model is a priori. From these priors, one can randomly draw one hypothesis (model), e.g., $nsim = 500$ times. Thus, in each of $500$ draws one randomly chooses one model (either $M0$ or $M1$), with the probabilities given by the model priors. For each draw, one first samples model parameters from their prior distributions, and then uses these sampled model parameters to simulate data. For each simulated data set, one can then compute marginal likelihoods and Bayes factor estimates using posterior samples and bridge sampling, and one can then compute the posterior probabilities for each hypothesis (i.e., how likely each model is a posteriori). As the last, and critical step in marginal SBC, one can then compare the posterior model probabilities to the prior model probabilities. A key result in marginal SBC (also termed data-averaged posterior) is that if the computation of marginal likelihoods and posterior model probabilities is performed accurately (without bias) by the bridge sampling procedure; that is, if the Bayes factor estimate is close to the true Bayes factor, then the posterior model probabilities should be the same as the prior model probabilities.

Here, we perform this marginal SBC (data-averaged posterior) approach. Across the $500$ simulations, we systematically vary the prior model probability from zero to one. For each of the $500$ simulations we sample a model (hypothesis) from the model prior, then sample parameters from the priors over parameters, use the sampled parameters to simulate data, fit the null and the alternative model on the simulated data, perform bridge sampling for each model, compute the Bayes factor estimate between them, and compute posterior model probabilities. If the bridge sampling works accurately, then the posterior model probabilities should be the same as the prior model probabilities. Given that we varied the prior model probabilities from zero to one, the posterior model probabilities should also vary from zero to one. In Figure \@ref(fig:SBC3plot), we plot the posterior model probabilities as a function of the prior model probabilities. If the posterior probabilities are the same as the priors, then the local regression line and all points should lie on the diagonal.

```{r, echo=FALSE}
#----------------------------------------
# load data and functions
#----------------------------------------
load("dataR/external/DataAll1.rda")
# name <- "wagersE4"
name <- "lagoE1"
# unique(dat$expt)
fakedata <- subset(dat, expt == name)
fakedata$subj <- factor(fakedata$subj)
fakedata$item <- factor(fakedata$item)
Nsj <- length(unique(fakedata$subj))
Nit <- length(unique(fakedata$item))
fakedata$subj <- as.numeric(fakedata$subj)
fakedata$item <- as.numeric(fakedata$item)
```

```{r, echo=FALSE}
nsim <- 500
priorsHypothesis <- matrix(NA, nrow = nsim, ncol = 2)
priorsHypothesis[, 1] <- seq(0, 1, length = nsim)
priorsHypothesis[, 2] <- 1 - priorsHypothesis[, 1]
```

```{r, echo=FALSE}
priorsLagoPost <- c(
  # population-level effects
  set_prior("normal( 6.02  ,0.0570 )", class = "Intercept"),
  set_prior("normal(-0.0284,0.00754)", class = "b"),
  # SD parameters items
  set_prior("normal(0.04,0.02)", class = "sd", coef = "Intercept", group = "item"),
  set_prior("normal(0.02,0.01)", class = "sd", coef = "x", group = "item"),
  # SD parameters subjects
  set_prior("normal(0.31,0.04)", class = "sd", coef = "Intercept", group = "subj"),
  set_prior("normal(0.03,0.02)", class = "sd", coef = "x", group = "subj"),
  # residual variance + correlation
  set_prior("normal(0.41,0.01)", class = "sigma"),
  set_prior("lkj(2)", class = "cor")
)
```

```{r, echo=FALSE, eval=!file.exists("dataR/SBC_BF_lagoE1x_hypSamp.RDS")}
u <- runif(nsim)
hypothesis_samples <- (u > priorsHypothesis[, 1]) / apply(priorsHypothesis, 1, sum)
```
```{r, echo=FALSE}
if (!file.exists("dataR/SBC_BF_lagoE1x_hypSamp.RDS")) {
  saveRDS(hypothesis_samples, "dataR/SBC_BF_lagoE1x_hypSamp.RDS")
} else {
  hypothesis_samples <- readRDS("dataR/SBC_BF_lagoE1x_hypSamp.RDS")
}
```

```{r, echo=FALSE, eval=!file.exists("dataR/SBC_BF_lagoE1x_parSamp.RDS")}
beta0 <- beta1 <- sigma_u0 <- sigma_u1 <- sigma_w0 <-
  sigma_w1 <- rho_u <- rho_w <- sigma <- NA
rtfakemat <- matrix(NA, nrow(fakedata), nsim)
set.seed(123)
source("functions/SimFromPrior.R")
for (i in 1:nsim) { # i <- 1
  tmp <- -1
  while (tmp < 0) {
    tmp <- SimFromPrior(priorsLagoPost, class = "Intercept", coef = "")
  }
  beta0[i] <- tmp
  beta1[i] <- SimFromPrior(priorsLagoPost, class = "b")
  sigma_u0[i] <- rnorm(1, 0.31, 0.04) # subjects
  sigma_u1[i] <- rnorm(1, 0.03, 0.02)
  sigma_w0[i] <- rnorm(1, 0.04, 0.02) # items
  sigma_w1[i] <- rnorm(1, 0.02, 0.01)
  rho_u[i] <- SimFromPrior(priorsLagoPost, class = "cor") # subjects
  rho_w[i] <- SimFromPrior(priorsLagoPost, class = "cor") # items
  sigma[i] <- SimFromPrior(priorsLagoPost, class = "sigma")
}
```

```{r, echo=FALSE, eval=!file.exists("dataR/SBC_BF_lagoE1x_parSamp.RDS")}
beta1[hypothesis_samples == 0] <- 0
```
```{r, echo=FALSE}
if (!file.exists("dataR/SBC_BF_lagoE1x_parSamp.RDS")) {
  trueParameters <- data.frame(beta0, beta1, sigma_u0, sigma_u1, sigma_w0, sigma_w1, rho_u, rho_w, sigma)
  saveRDS(trueParameters, "dataR/SBC_BF_lagoE1x_parSamp.RDS")
} else {
  trueParameters <- readRDS("dataR/SBC_BF_lagoE1x_parSamp.RDS")
}

beta0 <- trueParameters$beta0
beta1 <- trueParameters$beta1
sigma_u0 <- trueParameters$sigma_u0
sigma_u1 <- trueParameters$sigma_u1
sigma_w0 <- trueParameters$sigma_w0
sigma_w1 <- trueParameters$sigma_w1
rho_u <- trueParameters$rho_u
rho_w <- trueParameters$rho_w
sigma <- trueParameters$sigma
```

```{r, echo=FALSE, eval=!file.exists("dataR/SBC_BF_lagoE1x_fakeDat.RDS")}
source("functions/genfake.R")
for (i in 1:nsim) {
  rtfakemat[, i] <- genfake(fakedata, Nsj, Nit,
    beta0 = beta0[i],
    beta1 = beta1[i],
    sigma_u0 = sigma_u0[i], # subjects
    sigma_u1 = sigma_u1[i],
    sigma_w0 = sigma_w0[i], # items
    sigma_w1 = sigma_w1[i],
    rho_u = rho_u[i],
    rho_w = rho_w[i],
    sigma = sigma[i]
  )
}
```
```{r, echo=FALSE}
if (!file.exists("dataR/SBC_BF_lagoE1x_fakeDat.RDS")) {
  saveRDS(rtfakemat, "dataR/SBC_BF_lagoE1x_fakeDat.RDS")
} else {
  rtfakemat <- readRDS("dataR/SBC_BF_lagoE1x_fakeDat.RDS")
}
```

```{r SBCBF3, echo=FALSE, eval=!file.exists("dataR/SBC_BF_lagoE1x.RDS")}
BF10_SBC <- c()
FIX.EF <- data.frame()
for (i in 1:nsim) {
  fakedata$fakert <- rtfakemat[, i]
  # estimate model for alternative hypothesis
  brm1 <- brm(fakert ~ x + (1 + x | subj) + (1 + x | item), fakedata,
    family = lognormal(), prior = priorsLagoPost, cores = 4,
    save_pars = save_pars(all = TRUE),
    warmup = 2000, iter = 10000,
    control = list(adapt_delta = 0.99, max_treedepth = 15)
  )
  lml_Full <- bridge_sampler(brm1, silent = TRUE)
  rm(brm1)
  # estimate model for null hypothesis
  brm0 <- brm(fakert ~ 1 + (1 + x | subj) + (1 + x | item), fakedata,
    family = lognormal(), prior = priorsLagoPost[-2, ], cores = 4,
    save_pars = save_pars(all = TRUE),
    warmup = 2000, iter = 10000,
    control = list(adapt_delta = 0.99, max_treedepth = 15)
  )
  lml_Null <- bridge_sampler(brm0, silent = TRUE)
  rm(brm0)
  BF10_SBC <- c(BF10_SBC, bayes_factor(lml_Full, lml_Null)$bf)
}
```
```{r, echo=FALSE}
if (!file.exists("dataR/SBC_BF_lagoE1x.RDS")) {
  saveRDS(BF10_SBC, "dataR/SBC_BF_lagoE1x.RDS")
} else {
  BF10_SBC <- readRDS("dataR/SBC_BF_lagoE1x.RDS")
}
```

```{r, echo=FALSE}
postModelRat <- BF10_SBC * priorsHypothesis[, 2] / priorsHypothesis[, 1]
```

```{r, echo=FALSE}
postModelProbsH1 <- postModelRat / (postModelRat + 1)
postModelProbsH0 <- 1 / (postModelRat + 1)
```

```{r SBC3plot, fig=TRUE, include=TRUE, echo=FALSE, cache=FALSE, fig.height=3.5, fig.cap = "The posterior probabilities for M0 are plotted as a function of prior probabilities for M0. If the approximation of the Bayes factor using bridge sampling is unbiased, then the data should be aligned along the diagonal (see dashed black line). The thick black line is a prediction from a local regression analysis. The points are average posterior probabilities as a function of a priori selected hypotheses for 50 simulation runs each. Error bars represent 95 percent confidence intervals.", message=FALSE}
prob <- data.frame(
  postH0 = postModelProbsH0 * 100,
  priH0 = priorsHypothesis[, 1] * 100,
  priH0s = (1 - hypothesis_samples) * 100,
  sim = 1:nsim
)
prob$simC <- (floor((prob$sim - 1) / nsim * 10) + 0.5) * (nsim / 10)
# table(prob$simC)
probP <- prob %>%
  group_by(simC) %>%
  summarize(
    postM = mean(postH0), postSE = sd(postH0) / sqrt(n()),
    postCI = sd(postH0) / sqrt(n()) * qt(0.975, n()),
    priM = mean(priH0s), priSE = sd(priH0s) / sqrt(n()),
    priCI = sd(priH0s) / sqrt(n()) * qt(0.975, n())
  )
ggplot() +
  geom_abline(intercept = 0, slope = 1, linetype = 2) +
  geom_smooth(data = prob, aes(x = priH0, y = postH0), colour = "black") +
  geom_point(data = probP, aes(x = priM, y = postM)) +
  geom_errorbar(
    data = probP,
    aes(x = priM, ymin = postM - postCI, ymax = postM + postCI), colour = "black"
  ) +
  geom_errorbarh(
    data = probP,
    aes(y = postM, xmin = priM - priCI, xmax = priM + priCI), colour = "black"
  ) +
  xlab("Prior probability for M0") +
  ylab("Posterior probability for M0") +
  theme(aspect.ratio = 1 / 1)
```

The results of this analysis in Figure \@ref(fig:SBC3plot) show that the local regression line is very close to the diagonal, and that the data points (each summarizing results from 50 simulations, with means and confidence intervals) also lie close to the diagonal. This demonstrates that the estimated posterior model probabilities are close to their a priori values. This result shows that posterior model probabilities, which are based on the Bayes factor estimates from the bridge sampling, are unbiased for a large range of different a priori model probabilities.

This result is very important as it shows one example case where the Bayes factor approximation is accurate. However, this demonstration is valid only for this one specific application case, i.e., with a particular data set, particular models, specific priors for the parameters, and a specific comparison between nested models. Strictly speaking, if one wants to be sure that the Bayes factor estimate is accurate for a particular data analysis, then such a SBC validation analysis would have to be computed for every data analysis. For details, including code, on how to perform such a marginal SBC (data-averaged posterior), see https://osf.io/y354c/. However, the fact that the SBC yields such promising results for this first application case also gives some hope that the bridge sampling may be accurate also for other comparable data analysis situations.

Based on these results on the average theoretical performance of Bayes factor estimation, we next turn to a different issue: how Bayes factors depend on and vary with varying data, leading to bad performance in individual cases despite good average performance.


### Bayes factors in practice: Variability with the data {#sec-BFvar}

#### Variation associated with the data (subjects, items, and residual noise)

A second, and very different, source limiting robustness of Bayes factor estimates derives from the variability that is observed with the data, i.e., among subjects, items, and residual noise. Thus, when repeating an experiment a second time in a replication analysis, using different subjects and items, will lead to different outcomes of the statistical analysis every time a new replication run is conducted.
The \index{Dance of $p$-values} "dance of $p$-values" [@cumming2014new] illustrates this well-known limit to robustness in frequentist analyses, where $p$-values are not consistently significant across studies over repeated replication attempts. Instead, every time a study is repeated, the outcomes produce wildly disparate $p$-values.
This can also be observed when simulating data from some known truth and re-running analyses on simulated data sets.

Additionally, Bayesian analyses ought to incorporate this same kind of variability (also refer to https://daniellakens.blogspot.com/2016/07/dance-of-bayes-factors.html). Here we show this type of variability in Bayes factor analyses by looking at a new example data analysis: We look at research on sentence comprehension, and specifically on effects of cue-based retrieval interference [@lewisvasishth:cogsci05].

#### An example: The facilitatory interference effect

The experiments investigating the cognitive processes behind a well-researched phenomenon in sentence comprehension will be examined next. The \index{Agreement attraction} agreement attraction configuration below serves as the example for this discussion. In it, the grammatically incorrect sentence (2) appears more grammatical than the equally grammatically incorrect sentence (1):

(1) The key to the cabinet are in the kitchen.
(2) The key to the cabinets are in the kitchen.

Both sentences are ungrammatical because the subject ("key") does not agree with the verb in number ("are").
When compared to (1), sentences like (2) are frequently found to have shorter reading times at (or immediately after) the verb ("are"); for a meta-analysis, see @JaegerEngelmannVasishth2017. These shorter reading times are sometimes called \index{Facilitatory interference} "facilitatory interference" [@dillon2011structured]; facilitatory in this context simply refers to the fact that reading times at the relevant word are shorter in (2) compared to (1), without necessarily implying that processing is easier. One explanation for the shorter reading times is that there is an \index{Illusion of grammaticality} illusion of grammaticality because the attractor word (cabinets in this case) agrees locally in number with the verb.
This is an interesting phenomenon because the plural versus singular feature of the attractor noun ("cabinet/s") is not the subject, and therefore, under the rules of English grammar, is not supposed to agree with the number marking on the verb. That agreement attraction effects are consistently observed indicates that some \index{Non-compositional process} non-compositional processes are taking place.

Using a computational implementation [formulated in the ACT-R framework, @abbl02], an account of agreement attraction effects in language processing explains how \index{Retrieval-based working memory} retrieval-based working memory mechanisms lead to such agreement attraction effects in ungrammatical sentences [@EngelmannJaegerVasishth2019; see also @hammerly2019grammaticality; and @YadavetalJML2022]. Numerous studies have examined agreement attraction in grammatically incorrect sentences using comparable experimental setups and various dependent measures, including eye tracking and self-paced reading.
It is generally believed to be a robust empirical phenomenon, and we choose it for analysis here for that reason.

Here, we look at a self-paced reading study on agreement attraction in Spanish by @lago2015agreement.
For the experimental condition agreement attraction ($x$; sentence type), we estimate a population-level effect against a null model in which the sentence type population-level effect is not included.
For the agreement attraction effect of sentence type, we use sum contrast coding (i.e., -1 and +1). We run a hierarchical model with the following formula in `brms`: `rt ~ x + (x | subj) + (x | item)`, where `rt` is reading time, we have random variation associated with subjects and with items, and we assume that reading times follow a log-normal distribution: `family = lognormal()`.

First, load the data:

```{r LoadLagoData}
data("df_lagoE1")
head(df_lagoE1)
```

```{r RenameLagoData,echo=FALSE}
## renamed for code below:
lagoE1 <- df_lagoE1
```

As a next step, determine priors for the analysis of these data.

#### Determine priors using meta-analysis

One good way to obtain priors for Bayesian analyses, and specifically for Bayes factor analyses, is to use results from meta-analyses on the subject. Here, we take the prior for the experimental manipulation of agreement attraction from a published meta-analysis [@JaegerEngelmannVasishth2017].^[This meta-analysis already includes the data that we want to make inference about; thus, this meta-analysis estimate is not really the right estimate to use, since it involves using the data twice. We ignore this detail here because our goal is simply to illustrate the approach.]

The mean effect size (difference in reading time between the two experimental conditions) in the meta-analysis is $-22$ milliseconds (ms), with $95\% \;CI = [-36 \; -9]$ [@JaegerEngelmannVasishth2017, Table 4]. This means that on average, the target word (i.e., the verb) in sentences such as (2) is on average read $22$ milliseconds faster than in sentences such as (1). The size of the effect is measured on the millisecond scale, assuming a normal distribution of effect sizes across studies.

However, individual reading times usually do not follow a normal distribution. Instead, a better assumption about the distribution of reading times is a \index{Log-normal distribution} log-normal distribution. This is what we will assume in the `brms` model. Therefore, to use the prior from the meta-analysis in the Bayesian analysis, we have to transform the prior values from the millisecond scale to log millisecond scale.

We have performed this transformation in the published version of the arXiv paper. Based on these calculations, the prior for the experimental factor of interference effects is set to a normal distribution with mean $= -0.03$ and standard deviation = $0.009$. 

One can check that the prior on the slope parameter amounts to assigning a prior on the millisecond scale such that the effect has mean  $-22$ ms, with an approximate $95$% CI $[-36 \; -9]$ ms:

```{r}
# contrast coding:
high <- 1
low <- -1
# mean:
exp( 6 + ( - 0.03) * high ) - 
  exp( 6 + ( - 0.03) * low )
# confidence bounds on log scale:
lower <- - 0.03 - 2 * 0.009
upper <- - 0.03 + 2 * 0.009
# confidence bounds on ms scale:
exp( 6 + lower * high  ) - exp( 6 + lower * low )
exp( 6 + upper * high ) - exp( 6 + upper * low  )
```

For the other model parameters, we use \index{Principled prior} principled priors.

```{r SetPriors}
priors <- c(prior(normal(6, 0.5), class = Intercept),
            prior(normal(-0.03, 0.009), class = b),
            prior(normal(0, 0.5), class = sd),
            prior(normal(0, 1), class = sigma),
            prior(lkj(2), class = cor))
```

#### Running a hierarchical Bayesian analysis

Next, run a `brms` model on the data. We use a large number of iterations (`iter = 10000`) with bridge sampling to estimate the Bayes factor of the "full" model, which includes a population-level effect for the experimental condition agreement attraction (`x`; i.e., sentence type). As mentioned above, for the agreement attraction effect of sentence type, we use sum contrast coding (i.e., $-1$ and $+1$).



```{r RunModel1a, message=FALSE, echo=FALSE, results="hide", eval = any(!file.exists("dataR/lagoE1_m1.RDS", "dataR/lagoE1_h.RDS")) }

# run alternative model
m1_lagoE1 <- brm(rt ~ 1 + x + (1 + x | subj) + (1 + x | item),
  data = lagoE1,
  family = lognormal(),
  prior = priors,
  warmup = 2000,
  iter = 10000,
  cores = 4,
  save_pars = save_pars(all = TRUE),
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 15
  )
)
# run null model
m0_lagoE1 <- brm(rt ~ 1 + (1 + x | subj) + (1 + x | item),
  data = lagoE1,
  family = lognormal(),
  prior = priors[-2, ],
  warmup = 2000,
  iter = 10000,
  cores = 4,
  save_pars = save_pars(all = TRUE),
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 15
  )
)
# run bridge sampler
lml_m1_lagoE1 <- bridge_sampler(m1_lagoE1, silent = TRUE)
lml_m0_lagoE1 <- bridge_sampler(m0_lagoE1, silent = TRUE)
# compute Bayes factor
h_lagoE1 <- bayes_factor(lml_m1_lagoE1, lml_m0_lagoE1)
```

```{r, echo= FALSE}
if (!file.exists("dataR/lagoE1_m1.RDS")) {
  saveRDS(m1_lagoE1, "dataR/lagoE1_m1.RDS")
} else {
  m1_lagoE1 <- readRDS("dataR/lagoE1_m1.RDS")
}
if (!file.exists("dataR/lagoE1_h.RDS")) {
  saveRDS(h_lagoE1, "dataR/lagoE1_h.RDS")
} else {
  h_lagoE1 <- readRDS("dataR/lagoE1_h.RDS")
}
```
We first show the population-level (or fixed) effects from the posterior analyses:

```{r}
fixef(m1_lagoE1)
```


They show that for the population-level effect `x`, capturing the agreement attraction effect, the 95% credible interval does not overlap with zero. This indicates that there is some hint that the effect may have the expected negative direction, reflecting shorter reading times in the plural condition. As mentioned earlier, this does not provide a direct test of the hypothesis that the effect exists and is not zero. This is not tested here, because we did not specify the null hypothesis of zero effect explicitly. We can, however, draw inferences about this null hypothesis by using the Bayes factor.

Estimate Bayes factors between a full model, where the effect of agreement attraction is included, and a null model, where the effect of agreement attraction is absent, using the command `bayes_factor(lml_m1_lagoE1, lml_m0_lagoE1)`.
The Bayes factor $BF_{10}$, or the strength of the alternative over the null, is calculated by the function.


```{r}
h_lagoE1$bf
```

With a Bayes factor of $6$, the output indicates that the alternative model--which incorporates the population-level effect of agreement attraction--may have some merit.
That is, this provides evidence for the alternative hypothesis that there is a difference between the experimental conditions, i.e., a facilitatory effect in the plural condition of the size derived from the meta-analysis.

As discussed earlier, the `bayes_factor` command should be run several times to check the stability of the Bayes factor calculation.

#### Variability of the Bayes factor: \index{Posterior simulation} Posterior simulations

One way to investigate how variable the outcome of Bayes factor analyses can be (given that the Bayes factor is computed in a stable and accurate way), is to run prior predictive simulations. A key question then is how to set priors that yield realistic simulated data sets. Here, we choose the priors based on the posterior from a previous real empirical data set.
That is, one can use the posterior from the model above, and use it as a prior in future prior predictive simulations. Computing the Bayes factor analysis again on the simulated data can provide some insight into how variable the Bayes factor will be.

Here, we perform the prior predictive simulations. To determine the priors, we use a single given data set and model [namely the Bayesian hierarchical model fitted to the data from @lago2015agreement], and use the posterior distributions from this model as the prior for our analysis.
In these simulations, one takes posterior samples for the model parameters (i.e., $p(\boldsymbol{\Theta} \mid \boldsymbol{y})$), and for each posterior sample of the model parameters, one can simulate new data $\tilde{\boldsymbol{y}}$ from the model $p(\tilde{\boldsymbol{y}} \mid \boldsymbol{\Theta})$.

```{r PostPred, echo=TRUE, eval = !file.exists("dataR/m1_lageoE1_postPred.RDS") }
pred_lagoE1 <- posterior_predict(m1_lagoE1)
```
```{r, echo=FALSE}
if (!file.exists("dataR/m1_lageoE1_postPred.RDS")) {
  saveRDS(pred_lagoE1, "dataR/m1_lageoE1_postPred.RDS")
} else {
  pred_lagoE1 <- readRDS("dataR/m1_lageoE1_postPred.RDS")
}
```

The question that we are interested in here now is, how much information is contained in this simulated data. That is, we can run Bayesian models on this simulated data and compute Bayes factors to test whether in the simulated data there is evidence for agreement attraction effects.
The interesting question here is how variable the outcomes of these Bayes factor analyses will be among various simulated replications of the same study.

Now, using $50$ different simulated data sets simulated from the fitted model, we carry out this analysis.
For each of these data sets, we can proceed in exactly the same way as we did for the original observed experimental data. That is, we again fit the same `brms` model $50$ times, now to the simulated data, and using the same prior as before. For each simulated data set, we use bridge sampling to compute the Bayes factor of the alternative model compared to a null model where the agreement attraction effect (population-level effect predictor of sentence type, `x`) is set to $0$. For each simulated predictive data set, we store the resulting Bayes factor. We again use the prior from the meta-analysis.

```{r, echo=FALSE}
priors <- c(
  set_prior("normal(6, 0.5)", class = "Intercept"),
  set_prior("normal(-0.03, 0.009)", class = "b"),
  set_prior("normal(0, 0.5)", class = "sd"),
  set_prior("normal(0, 1)", class = "sigma"),
  set_prior("lkj(2)", class = "cor")
)
```

```{r runModelsBridge, message=FALSE, echo=FALSE, eval = !file.exists("dataR/LagoE1_PosPredModels.RDS")}

nsim <- 50 # 100
m1_lagoE1_list <- h_lagoE1_list <- list()
BF10_lagoE1 <- c()
datT <- data.frame()
FIX.EF <- as.data.frame(matrix(NA, nrow = nsim, ncol = 4))
datTmp <- lagoE1
for (i in 1:nsim) {
  print(i)
  datTmp$rt <- pred_lagoE1[i, ]
  m1_lagoE1_list <- brm(rt ~ 1 + x + (1 + x | subj) + (1 + x | item),
    data = datTmp,
    family = lognormal(),
    prior = priors,
    warmup = 2000,
    iter = 10000,
    cores = 4,
    save_pars = save_pars(all = TRUE),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15
    )
  )
  m1_lagoE1_null <- brm(rt ~ 1 + (1 + x | subj) + (1 + x | item),
    data = datTmp,
    family = lognormal(),
    prior = priors[-2, ],
    warmup = 2000,
    iter = 10000,
    cores = 4,
    save_pars = save_pars(all = TRUE),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15
    )
  )
  lml_Full <- bridge_sampler(m1_lagoE1_list, silent = TRUE)
  lml_Null <- bridge_sampler(m1_lagoE1_null, silent = TRUE)
  BF10_lagoE1[i] <- bayes_factor(lml_Full, lml_Null)$bf

  FIX.EF[i, ] <- fixef(m1_lagoE1_list)[2, ]

  mTmp <- plyr::ddply(datTmp, "x", summarize, RT = mean(rt))$RT
  datT <- rbind(datT, c(i, mTmp))
}

names(FIX.EF) <- c("Estimate", "Est.Error", "Q2.5", "Q97.5")
FIX.EF$Study <- 1:50

names(datT) <- c("simNr", "x1", "xm1")
datT$dif <- datT$x1 - datT$xm1
datT$BF10 <- BF10_lagoE1

datT <- cbind(datT, FIX.EF)
# saveRDS(datT,"dataR/LagoE1_PosPredModels.RDS")
```
```{r, echo= FALSE}
if (!file.exists("dataR/LagoE1_PosPredModels.RDS")) {
  saveRDS(datT, "dataR/LagoE1_PosPredModels.RDS")
} else {
  datT <- readRDS("dataR/LagoE1_PosPredModels.RDS")
}
```

#### Visualize distribution of Bayes factors

We can now visualize the distribution of Bayes factors ($BF_{10}$) across prior predictive distributions by plotting a histogram.
In this histogram, values greater than one support the alternative model (M1) that there are agreement attraction effects (i.e., the sentence type effect differs from zero), and values of the Bayes factor less than one support the null model (M0), which states that there is no agreement attraction effect (i.e., there is no difference in reading times between experimental conditions).

```{r plotBFdistrn, echo=FALSE, message=FALSE, fig.height=4, fig.cap="Estimates of the retrieval interference facilitatory effect and the 95% credible intervals for all simulations (solid lines) and the empirically observed data (dashed line) are shown in the left panel. An illustration of the alternative model's Bayes factors (BF10) over the null model in 50 simulated data sets is shown in the right panel. The horizontal error bar displays 95% of all Bayes factors, the dashed line displays the Bayes factor calculated from the empirical data, and the vertical solid black line displays equal evidence for both hypotheses."}

datT$logBF10 <- log10(datT$BF10)
qtBF <- quantile(datT$logBF10, c(0.025, 0.975))
breaks <- c(1 / 10, 1 / 3, 1, 3, 10)
p1 <- ggplot() +
  stat_bin(data = datT, aes(x = logBF10), geom = "bar") +
  geom_vline(xintercept = log10(h_lagoE1$bf), lty = 2, colour = "black") +
  geom_vline(xintercept = 0) +
  geom_errorbarh(aes(y = +0.5, xmin = qtBF[1], xmax = qtBF[2]), linewidth = 1, height = 0.3) +
  scale_x_continuous(
    breaks = log10(breaks),
    labels = MASS::fractions(breaks)
  ) +
  scale_y_continuous(breaks = seq(0, 10, 2)) +
  labs(x = "Bayes factor [BF10]", y="count")
# gridExtra::grid.arrange(p1, p2, layout_matrix=lay)

datTp <- datT[, c("Study", "Estimate", "Q2.5", "Q97.5")]
datTp <- rbind(c(0, fixef(m1_lagoE1)[2, c("Estimate", "Q2.5", "Q97.5")]), datTp)
datTp$data <- ifelse(datTp$Study == 0, "Empirical data", "Simulated data")
idx <- order(datTp$Estimate)
datTp <- datTp[idx, ]
datTp$Study <- 1:nrow(datTp)

p2 <- ggplot(
  data = datTp,
  aes(x = Study, y = Estimate, colour = data, linetype = data)
) +
  geom_hline(yintercept = 0) +
  geom_point() +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5)) +
  scale_colour_manual(name = "", values = c("black", "black")) +
  scale_linetype_manual(name = "", values = c(2, 1)) +
  labs(y = "Estimate [95% CrI]", x = "Simulated Study No.") +
  theme(legend.position= "none")

# quartz(width=4*2.5, height=4)
#lay <- rbind(c(1, 1, 2))
#gridExtra::grid.arrange(p2, p1, layout_matrix = lay)


cowplot::plot_grid(p1, p2, ncol=2)
```

The results show that the Bayes factors are quite variable. The Bayes factor results differ in that they either provide strong evidence for the alternative model ($BF_{10} > 10$) or moderate evidence for the null model ($BF_{10} < 1/3$), despite the fact that all data sets are simulated from the same distribution. The majority of the simulated data sets support the alternative model with moderate to weak evidence. In other words, this analysis reveals a \index{Dance of the Bayes factors} "dance of the Bayes factors" with simulated repetitions of the same study, similar to the "dance of $p$-values" [@cumming2014new]. The variation in these findings demonstrates a very important point that is not widely appreciated: the evidence we get from a particular Bayes factors calculation may not hold up if the same study is replicated. Just obtaining a large Bayes factor alone is not necessarily informative; the variation in the Bayes factor under (hypothetical or actual) repeated sampling needs to be considered as well. For a real-life example, see the replicationa attempt reported in @Schoknecht2025.

Why are there variations in the Bayes factors amongst the simulated data sets? The difference between the two sentence types' reading times, and thus the experimental effect from which we want to draw conclusions, could vary depending on the noise and uncertainty in the posterior predictive simulations. This is one obvious explanation for why the results could be so different. Plotting the Bayes factors from this simulated data set against the estimated difference in simulated reading times between the two sentence types (as determined by the Bayesian model) therefore provides an interesting perspective. In other words, we take the population-level effects of the Bayesian model and extract the estimated mean difference in reading times at the verb between plural and singular attractor conditions. Then, we plot the Bayes factor as a function of this difference (along with 95% credible intervals).

```{r BFregression, echo=FALSE, message=FALSE, fig.width=6, fig.heigth=4, fig.cap="The Bayes factor (BF10) as a function of the estimate (with 95 percent credible intervals) of the facilitatory effect of retrieval interference across 50 simulated data sets. The prior is from a meta-analysis."}

breaks <- c(1 / 10, 1 / 3, 1, 3, 10)
p1 <- ggplot(data = datT, aes(y = logBF10, x = Estimate)) +
  geom_point() +
  geom_smooth() +
  scale_y_continuous(
    breaks = log10(breaks),
    labels = MASS::fractions(breaks)
  ) +
  labs(x = "Difference in reading times (ms)", y = "BF10")

breaks <- c(1 / 10, 1 / 3, 1, 3, 10)
ggplot(data = datT, aes(y = logBF10, x = Estimate)) +
  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5), colour = "grey60") +
  geom_point() +
  geom_smooth() +
  scale_y_continuous(
    breaks = log10(breaks),
    labels = MASS::fractions(breaks)
  ) +
  labs(x = "Effect estimate [95% CrI]", y = "BF10")
```

The findings (illustrated in Figure \@ref(fig:BFregression)) demonstrate that there is quite a lot of variability in the average difference in reading times between experimental conditions amongst the posterior predictive simulations. This suggests that there is little information about the effect of interest in the experimental data and design.
Of course, if the data is noisy, Bayes factor analyses based on this simulated data cannot be stable across simulations either.
Therefore, as is evident from Figure \@ref(fig:BFregression), one of the main factors influencing the Bayes factor computations is, in fact, the variation in mean reading times between experimental conditions [other model parameters don't show a close association; see the published version of the arXiv article @SchadEtAlBF].

The Bayes factor BF10 in Figure \@ref(fig:BFregression) increases with the difference in reading times, meaning that the more quickly the plural noun condition (in this case, "cabinets" in example sentence 2) is read in comparison to the singular noun condition (i.e., "cabinet"; example sentence 1), the stronger the evidence is in favor of the alternative model.
Conversely, when the difference in reading times becomes less negative, that is, when the plural condition (sentence 2) is not read noticeably faster than the singular condition (sentence 1), the Bayes factor BF10 drops to values smaller than 1.
Crucially, this behavior arises from the fact that we are utilizing informative priors from the meta-analysis, where the agreement attraction effect's prior mean has a negative value (i.e., a prior mean of $-0.03$) instead of being centered at a mean of zero. A null model of no effect is therefore more consistent with reading time differences that are less negative or more positive than this prior mean. This also leads to the startling conclusion that, in contrast to the much more variable Bayes factor results, the 95\% credible intervals are fairly consistent and do not overlap with zero. This should alarm researchers who exclusively use the 95% credible interval to decide whether an effect is present or not, i.e., to make a discovery claim.

The precise question of whether the data provide more support for the effect size found in the meta-analysis than the absence of any effect is addressed by computing Bayes factors for such a prior with a non-zero mean.

The important lesson to learn from this analysis is that Bayes factors can be quite variable for different data sets assessing the same phenomenon. Individual data sets in the cognitive sciences often do not contain a lot of information about the phenomenon of interest, even when--as is the case here with agreement attraction--the phenomenon is thought to be a relatively robust phenomenon. For a more detailed investigation of how Bayes factors can vary with data, in both simulated and real replication studies, we refer the reader to the published version of the arXiv article, and @SampleSizeCBB2021.


### A cautionary note about Bayes factors {#sec-caution}

Just like frequentist $p$-values [@pvals], Bayes factors are easy to misuse and  misinterpret, and have the potential to mislead the scientist if used in an automated manner. A recent article [@tendeiro] reviews many of the \index{Misuse of Bayes factors} misuses of Bayes factors analyses in psychology and related areas. As discussed in this chapter, Bayes factors (and Bayesian analysis in general) require a great deal of thought; there is no substitute for sensitivity analyses, and the development of sensible priors. Using default priors and deriving black and white conclusions from Bayes factor analyses is never a good idea.

## Sample size determination using Bayes factors

This section contains text adapted from @SampleSizeCBB2021.

When planning a new experiment, it is possible to take what is a fundamentally frequentist approach to work out what sample size one would need in order to cross a certain Bayes factor threshold of evidence.

It may sound surprising to Bayesian modelers that sample size planning is even something to plan for: One of the many advantages of Bayesian modeling is that it is straightforward to plan an experiment without necessarily specifying the sample size in advance [e.g., @spiegelhalter2004bayesian]. Indeed, in our own research, running an experiment until some precision criterion in the posterior distribution is reached [@Freedman1984;@spiegelhalter1994bayesian;@kruschke2014doing;@kruschke2018bayesian] is our method of choice  [@JaegerMertzenVanDykeVasishth2019;@VasishthMertzenJaegerGelman2018;@stoneNOL2021]. This approach is possible to implement if one has sufficient financial resources (and time) to keep running an experiment till a particular precision criterion is reached.

However, even when planning a Bayesian analysis, there can be situations where one needs to determine sample size in advance. One important situation where this becomes necessary is when one applies for research funding. In a funding proposal, one obviously has to specify the sample size in advance in order to ask for the necessary funds for conducting the study.  Other situations where sample size planning is needed is in the design of clinical trials, the design of replication trials, and when pre-registering experiments and/or preparing registered reports.

There exist good proposals on how to work out sample sizes in advance, specifically in the case of Bayesian analyses. For example,  @wang2002simulation aims to ensure that the researcher obtains strong evidence for the effect being estimated.

In the present paper, we unpack the approach taken in  @wang2002simulation. The approach is important because it provides an easy-to-implement workflow for doing sample size calculations using complex hierarchical models of the type we discuss in the present book.

The @wang2002simulation approach is as follows.   We have adapted the procedure outlined below slightly for our purposes, but the essential ideas are due to these authors.

1. Decide on a distribution for the effect sizes you wish to detect.
2. Choose a criterion that counts as a threshold for a decision. This can be a Bayes factor of, say, 10 [@jeffreys1939theory].^[The Bayes factor is just one of many possible performance criteria; see @wang2002simulation for some other alternatives.]
3. Then do the following for increasing sample sizes $n$:
    1. Simulate prior predictive data $niter$ times (say, $niter=100$) for sample size $n$; use informative priors [these are referred to as sampling priors in @wang2002simulation].
    2. Fit the model to the simulated data using uninformative priors [these are called fitting priors in @wang2002simulation], and derive the posterior distribution each time, and compute the Bayes factor using a null model that assumes a zero effect for the parameter of interest.
    3. Display, in one plot, the $niter$ posterior distributions and the Bayes factors. If the chosen decision criterion is met reasonably well under repeated sampling for a given sample size, choose that sample size.


In @SampleSizeCBB2021, we show how this approach can be adapted for the
types of models we discuss in the present chapter.

## Summary

Bayes factors are a very important tool in Bayesian data analysis. They allow the researcher to quantify the evidence in favor of certain effects in the data by comparing a full model, which contains a parameter corresponding to the effect of interest, with a null model that does not contain that parameter. We saw that Bayes factor analyses are highly sensitive to priors specified for the parameters; this is true both for the parameter corresponding to the effect of interest, but also sometimes for priors relating to other parameters in the model, such as the intercept. It is therefore very important to perform prior predictive checks to select good and plausible priors. Moreover, sensitivity analyses, where Bayes factors are investigated for differing prior assumptions, should be standardly reported in any analysis involving Bayes factors. We studied theoretical aspects of Bayes factors and saw that bridge sampling requires a very large effective sample size in order to obtain stable results for approximate Bayes factors. Therefore, one should always perform a Bayes factor analysis with bridge sampling at least twice to ensure that the results are stable.  Bridge sampling comes with no strong guarantees concerning its accuracy, and we saw that simulation-based calibration can be used to evaluate the accuracy of Bayes factor estimates. Finally, we learned that Bayes factors can strongly vary with the data. In the cognitive sciences, the data are--even for relatively robust effects--often not stable due to small effect sizes and limited sample size. Therefore,  the resulting Bayes factors can strongly vary across  data sets investigating the same phenomenon. As a consequence, only large effect sizes, large sample studies, and/or replication studies can lead to reliable inferences from empirical data in the cognitive sciences.

One topic that was not discussed in detail in this chapter is data aggregation. In repeated measures data, null hypothesis Bayes factor analyses can be performed on the raw data, i.e., without aggregation, by using Bayesian hierarchical  models. In an alternative approach, the data are first aggregated by taking the mean per subject and condition, before running null hypothesis Bayes factor analyses on the aggregated data. Inferences / Bayes factors based on aggregated data can be biased, when either (i) item variability is present in addition to subject variability, or (ii) when the sphericity assumption (inherent in repeated measures ANOVA) is violated [@schad2022data]. In these cases, aggregated analyses can lead to biased results and should not be used. By contrast, non-aggregated analyses are robust also in these cases and yield accurate Bayes factor estimates.


## Further reading

A detailed explanation on how bridge sampling works can be found in @gronauTutorialBridgeSampling2017, and more details about the bridgesampling package can be found in @gronauBridgesamplingPackageEstimating2017. @wagenmakers2010BayesianHypothesisTesting provides a complete tutorial and the mathematical proof of the Savage--Dickey method; also see @kendall2004 and https://statproofbook.github.io/P/bf-sddr.html. Some limitations of the Savage--Dickey approach under certain conditions are discussed in @heck2019caveat. The package `bayestestr` [@makowski2019bayestestr] can also be used for Bayes factor computations using the Savage--Dickey method. For a Bayes Factor Test calibrated to investigate  replication success, see @verhagenBayesianTestsQuantify2014. A special issue on hierarchical modeling and Bayes factors appears in the journal Computational Brain and Behavior in response to an article by @van2021bayes. @kruschke2018bayesian discuss alternatives to Bayes factors for hypothesis testing. An argument against null hypothesis testing with Bayes Factors appears in this blog post by Andrew Gelman: https://statmodeling.stat.columbia.edu/2019/09/10/i-hate-bayes-factors-when-theyre-used-for-null-hypothesis-significance-testing/.
An argument in favor of null hypothesis testing with Bayes Factor as an approximation (but assuming realistic effects) appears in: https://statmodeling.stat.columbia.edu/2018/03/10/incorporating-bayes-factor-understanding-scientific-information-replication-crisis/.
A visualization of the distinction between Bayes factor and k-fold cross-validation is in a blog post by Fabian Dablander, https://tinyurl.com/47n5cte4. Decision theory, which was only mentioned in passing in this chapter, is discussed in @parmigiani2009decision.
Hypothesis testing in its different flavors is discussed in @robert202250.
Alternative ways to test the accuracy of Bayes factors are discussed in @sekulovskigood.
When planning studies, Bayes-factor based power calculations can be carried out; an example of such power computations in the context of psycholinguistics, which uses the software tools discussed in the present book, is @SampleSizeCBB2021 (also see the references cited there).
