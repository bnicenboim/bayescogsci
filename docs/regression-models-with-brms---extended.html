<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A Regression models with brms - Extended | Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="A Regression models with brms - Extended | Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://bruno.nicenboim.me/bayescogsci/images/temporarycover.jpg" />
  <meta property="og:description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/bnicenboim/bayescogsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Regression models with brms - Extended | Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bruno.nicenboim.me/bayescogsci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel J. Schad, and Shravan Vasishth" />


<meta name="date" content="2025-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-closing.html"/>
<link rel="next" href="advanced-models-with-stan---extended.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<script data-goatcounter="https://bayescogsci.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b>  Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The  law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b>  Discrete random variables: An example using the  binomial distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b>  Continuous random variables: An example using the  normal distribution</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1:  Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (or multivariate) data</a></li>
<li class="chapter" data-level="1.6.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-decomposevcovmatrix"><i class="fa fa-check"></i><b>1.6.4</b> Decomposing a variance-covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-some-useful-r-functions"><i class="fa fa-check"></i><b>1.8</b> Summary of some useful R functions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b>  Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the  posterior using Bayes’ rule: An analytical example</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a  likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a  prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using  Bayes’ rule to compute the  posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The  posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the  posterior through  sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b>  Bayesian Regression Models using Stan:  brms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors:  sensitivity analysis</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b>  Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b>  Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b>  Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b>  Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b>  Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The  log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first  linear regression: Does attentional load affect pupil size?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b>  Likelihood and  priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b>  Log-normal model: Does trial affect response times?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b>  Logistic regression: Does  set size affect  free recall?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b>  Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b>  Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b>  No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b>  Varying intercepts and  varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A  hierarchical log-normal model: The  Stroop effect</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes  log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>6.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>6.1.1</b> Default contrast coding:  Treatment contrasts</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>6.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>6.1.3</b>  Sum contrasts</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>6.1.4</b>  Cell means parameterization and  posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>6.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>6.2.1</b>  Sum contrasts</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>6.2.2</b> The  hypothesis matrix</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>6.2.3</b> Generating contrasts: The  <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>6.3</b> Other types of contrasts: illustration with a factor of four levels</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>6.3.1</b>  Repeated contrasts</a></li>
<li class="chapter" data-level="6.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>6.3.2</b>  Helmert contrasts</a></li>
<li class="chapter" data-level="6.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>6.3.3</b> Contrasts in linear regression analysis: The design or  model matrix</a></li>
<li class="chapter" data-level="6.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>6.3.4</b>  Polynomial contrasts</a></li>
<li class="chapter" data-level="6.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>6.3.5</b> An alternative to contrasts:  Monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>6.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>6.4.1</b>  Centered contrasts</a></li>
<li class="chapter" data-level="6.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>6.4.2</b>  Orthogonal contrasts</a></li>
<li class="chapter" data-level="6.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> The role of the  intercept in  non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>6.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="6.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-5"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
<li class="chapter" data-level="6.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-3"><i class="fa fa-check"></i><b>6.7</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>7</b> Contrast coding with two predictor variables</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>7.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>7.1.1</b>  Nested effects</a></li>
<li class="chapter" data-level="7.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>7.1.2</b>  Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>7.2</b> One factor and one  covariate</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>7.2.1</b> Estimating a  group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>7.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>7.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="7.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="8" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>8</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>8.1</b> Stan syntax</a></li>
<li class="chapter" data-level="8.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>8.2</b> A first simple example with Stan:  Normal likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>8.3</b> Another simple example:  Cloze probability with Stan with the  binomial likelihood</a></li>
<li class="chapter" data-level="8.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>8.4</b>  Regression models in Stan</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>8.4.1</b> A first  linear regression in Stan: Does attentional load affect  pupil size?</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>8.4.2</b>  Interactions in Stan: Does attentional load interact with trial number affecting  pupil size?</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>8.4.3</b>  Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-7"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-5"><i class="fa fa-check"></i><b>8.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>9</b> Hierarchical models and reparameterization </a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>9.1</b> Hierarchical models with Stan</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>9.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>9.1.2</b> Uncorrelated  varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="9.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>9.1.3</b>  Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="9.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>9.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-8"><i class="fa fa-check"></i><b>9.2</b> Summary</a></li>
<li class="chapter" data-level="9.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-6"><i class="fa fa-check"></i><b>9.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>10</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>10.1</b> A change of variables with the reciprocal normal distribution</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>10.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>10.2</b>  Validation of a computed posterior distribution</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>10.2.1</b> The  simulation-based calibration procedure</a></li>
<li class="chapter" data-level="10.2.2" data-path="ch-custom.html"><a href="ch-custom.html#an-example-where-simulation-based-calibration-reveals-a-problem"><i class="fa fa-check"></i><b>10.2.2</b> An example where simulation-based calibration reveals a problem</a></li>
<li class="chapter" data-level="10.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-with-and-limitations-of-simulation-based-calibration"><i class="fa fa-check"></i><b>10.2.3</b> Issues with and limitations of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-the-exponential-distribution-implemented-manually"><i class="fa fa-check"></i><b>10.3</b> Another  custom distribution: The exponential distribution  implemented manually</a></li>
<li class="chapter" data-level="10.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-9"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
<li class="chapter" data-level="10.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-7"><i class="fa fa-check"></i><b>10.5</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="11" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>11</b>  Meta-analysis and  measurement error models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>11.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>11.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>11.2</b>  Measurement-error models</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>11.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-10"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-8"><i class="fa fa-check"></i><b>11.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison</b></span></li>
<li class="chapter" data-level="12" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>12</b> Introduction to model comparison</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-comparison.html"><a href="ch-comparison.html#prior-predictive-vs.-posterior-predictive-model-comparison"><i class="fa fa-check"></i><b>12.1</b> Prior predictive vs. posterior predictive model comparison</a></li>
<li class="chapter" data-level="12.2" data-path="ch-comparison.html"><a href="ch-comparison.html#some-important-points-to-consider-when-comparing-models"><i class="fa fa-check"></i><b>12.2</b> Some important points to consider when comparing models</a></li>
<li class="chapter" data-level="12.3" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-9"><i class="fa fa-check"></i><b>12.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>13</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>13.1</b> Hypothesis testing using the Bayes factor</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>13.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="13.1.2" data-path="ch-bf.html"><a href="ch-bf.html#the-bayes-factor"><i class="fa fa-check"></i><b>13.1.2</b> The Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>13.2</b> Examining the N400 effect with the Bayes factor</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>13.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="13.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>13.2.2</b>  Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>13.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="13.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>13.4</b>  The Bayes factor in Stan</a></li>
<li class="chapter" data-level="13.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>13.5</b> Bayes factors in theory and in practice</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>13.5.1</b> Bayes factors in theory: Stability and  accuracy</a></li>
<li class="chapter" data-level="13.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>13.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="13.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>13.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="ch-bf.html"><a href="ch-bf.html#sample-size-determination-using-bayes-factors"><i class="fa fa-check"></i><b>13.6</b> Sample size determination using Bayes factors</a></li>
<li class="chapter" data-level="13.7" data-path="ch-bf.html"><a href="ch-bf.html#summary-11"><i class="fa fa-check"></i><b>13.7</b> Summary</a></li>
<li class="chapter" data-level="13.8" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-10"><i class="fa fa-check"></i><b>13.8</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>14</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>14.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="14.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>14.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="14.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>14.3</b> Testing the N400 effect using cross-validation</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>14.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="14.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>14.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="14.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>14.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>14.4</b>  Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="14.5" data-path="ch-cv.html"><a href="ch-cv.html#sec-issuesCV"><i class="fa fa-check"></i><b>14.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="14.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>14.6</b> Cross-validation in Stan</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>14.6.1</b>  PSIS-LOO-CV in Stan</a></li>
<li class="chapter" data-level="14.6.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-cv-in-stan"><i class="fa fa-check"></i><b>14.6.2</b>  K-fold-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-12"><i class="fa fa-check"></i><b>14.7</b> Summary</a></li>
<li class="chapter" data-level="14.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-11"><i class="fa fa-check"></i><b>14.8</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>VI Cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="15" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>15</b> Introduction to cognitive modeling</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#what-characterizes-a-computational-cognitive-model"><i class="fa fa-check"></i><b>15.1</b> What characterizes a computational cognitive model?</a></li>
<li class="chapter" data-level="15.2" data-path="ch-cogmod.html"><a href="ch-cogmod.html#some-advantages-of-taking-the-latent-variable-modeling-approach"><i class="fa fa-check"></i><b>15.2</b> Some advantages of taking the latent-variable modeling approach</a></li>
<li class="chapter" data-level="15.3" data-path="ch-cogmod.html"><a href="ch-cogmod.html#types-of-computational-cognitive-model"><i class="fa fa-check"></i><b>15.3</b> Types of computational cognitive model</a></li>
<li class="chapter" data-level="15.4" data-path="ch-cogmod.html"><a href="ch-cogmod.html#summary-13"><i class="fa fa-check"></i><b>15.4</b> Summary</a></li>
<li class="chapter" data-level="15.5" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-12"><i class="fa fa-check"></i><b>15.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>16</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>16.1</b> Modeling  multiple categorical responses</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>16.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="16.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>16.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>16.2</b> Modeling picture naming abilities in aphasia with MPT models</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>16.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="16.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>16.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="16.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>16.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="16.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>16.2.4</b> A  hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-14"><i class="fa fa-check"></i><b>16.3</b> Summary</a></li>
<li class="chapter" data-level="16.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-13"><i class="fa fa-check"></i><b>16.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>17</b> Mixture models</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>17.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>17.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="17.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>17.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="17.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>17.1.3</b> A  multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="17.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>17.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="17.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>17.1.5</b> A  hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-15"><i class="fa fa-check"></i><b>17.2</b> Summary</a></li>
<li class="chapter" data-level="17.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-14"><i class="fa fa-check"></i><b>17.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>18</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>18.1</b> Modeling a lexical decision task</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>18.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>18.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="18.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>18.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="18.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>18.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="18.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>18.1.5</b> Dealing with  contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>18.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="18.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-16"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-closing.html"><a href="ch-closing.html"><i class="fa fa-check"></i><b>19</b> In closing</a></li>
<li class="appendix"><span><b>Online materials</b></span></li>
<li class="chapter" data-level="A" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html"><i class="fa fa-check"></i><b>A</b> Regression models with <code>brms</code> - Extended</a>
<ul>
<li class="chapter" data-level="A.1" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-efficientpriorpd"><i class="fa fa-check"></i><b>A.1</b> An efficient function for generating prior predictive distributions in R</a></li>
<li class="chapter" data-level="A.2" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-truncation"><i class="fa fa-check"></i><b>A.2</b> Truncated distributions</a></li>
<li class="chapter" data-level="A.3" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-intercept"><i class="fa fa-check"></i><b>A.3</b> Intercepts in <code>brms</code></a></li>
<li class="chapter" data-level="A.4" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-lognormal"><i class="fa fa-check"></i><b>A.4</b> Understanding the log-normal likelihood</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#log-normal-distributions-everywhere"><i class="fa fa-check"></i><b>A.4.1</b> Log-normal distributions everywhere</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-priorR"><i class="fa fa-check"></i><b>A.5</b> Prior predictive checks in R</a></li>
<li class="chapter" data-level="A.6" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-exch"><i class="fa fa-check"></i><b>A.6</b> Finitely exchangeable random variables</a></li>
<li class="chapter" data-level="A.7" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-matrixHierachicalModel"><i class="fa fa-check"></i><b>A.7</b> The Matrix Formulation of Hierarchical Models (the Laird-Ware form)</a></li>
<li class="chapter" data-level="A.8" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-cTreatGM"><i class="fa fa-check"></i><b>A.8</b> Treatment contrast with intercept as the grand mean</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html"><i class="fa fa-check"></i><b>B</b> Advanced models with Stan - Extended</a>
<ul>
<li class="chapter" data-level="B.1" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-target"><i class="fa fa-check"></i><b>B.1</b> What does <code>target</code> do in Stan models?</a></li>
<li class="chapter" data-level="B.2" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-tilde"><i class="fa fa-check"></i><b>B.2</b> Explicitly incrementing the log probability function (<code>target</code>) vs. using the sampling or distribution <code>~</code> notation</a></li>
<li class="chapter" data-level="B.3" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-cmdstanr"><i class="fa fa-check"></i><b>B.3</b> An alternative R interface to Stan: <code>cmdstanr</code></a></li>
<li class="chapter" data-level="B.4" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-stancontainers"><i class="fa fa-check"></i><b>B.4</b> Matrix, vector, or array in Stan?</a></li>
<li class="chapter" data-level="B.5" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-noncenterparam"><i class="fa fa-check"></i><b>B.5</b> A simple non-centered parameterization</a></li>
<li class="chapter" data-level="B.6" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-cholesky"><i class="fa fa-check"></i><b>B.6</b> Cholesky factorization for reparameterizing hierarchical models with correlations between adjustments to different parameters</a></li>
<li class="chapter" data-level="B.7" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-sbc"><i class="fa fa-check"></i><b>B.7</b> Different rank visualizations and the <code>SBC</code> package.</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="evidence-synthesis-and-measurements-with-error---extended.html"><a href="evidence-synthesis-and-measurements-with-error---extended.html"><i class="fa fa-check"></i><b>C</b> Evidence synthesis and measurements with error - Extended</a>
<ul>
<li class="chapter" data-level="C.1" data-path="evidence-synthesis-and-measurements-with-error---extended.html"><a href="evidence-synthesis-and-measurements-with-error---extended.html#app-sigmatrue"><i class="fa fa-check"></i><b>C.1</b> What happens if we set <code>sigma = TRUE</code> in <code>resp_se()</code> function in <code>brms</code>?</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html"><i class="fa fa-check"></i><b>D</b> Model comparison - Extended</a>
<ul>
<li class="chapter" data-level="D.1" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-null"><i class="fa fa-check"></i><b>D.1</b> Credible intervals should not be used to reject a null hypothesis</a></li>
<li class="chapter" data-level="D.2" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-likR"><i class="fa fa-check"></i><b>D.2</b> The likelihood ratio vs the Bayes factor</a></li>
<li class="chapter" data-level="D.3" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-integral"><i class="fa fa-check"></i><b>D.3</b> Approximation of the (expected) log predictive density of a model without integration</a></li>
<li class="chapter" data-level="D.4" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-CV-alg"><i class="fa fa-check"></i><b>D.4</b> The cross-validation algorithm for the expected log predictive density of a model</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>E</b> The Art and Science of Prior Elicitation</a>
<ul>
<li class="chapter" data-level="E.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>E.1</b> Eliciting priors from oneself for a self-paced reading study: An example</a>
<ul>
<li class="chapter" data-level="E.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>E.1.1</b> An example: English  relative clauses</a></li>
<li class="chapter" data-level="E.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>E.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="E.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>E.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="E.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>E.1.4</b> Eliciting priors for the  variance components</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>E.2</b>  Eliciting priors from experts</a></li>
<li class="chapter" data-level="E.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>E.3</b> Deriving priors from  meta-analyses</a></li>
<li class="chapter" data-level="E.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>E.4</b> Using previous experiments’  posteriors as priors for a new study</a></li>
<li class="chapter" data-level="E.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-17"><i class="fa fa-check"></i><b>E.5</b> Summary</a></li>
<li class="chapter" data-level="E.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-16"><i class="fa fa-check"></i><b>E.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>F</b> Workflow</a>
<ul>
<li class="chapter" data-level="F.1" data-path="ch-workflow.html"><a href="ch-workflow.html#building-a-model"><i class="fa fa-check"></i><b>F.1</b>  Building a model</a></li>
<li class="chapter" data-level="F.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-to-ask-on-a-model"><i class="fa fa-check"></i><b>F.2</b> Principled questions to ask on a model</a>
<ul>
<li class="chapter" data-level="F.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#checking-whether-assumptions-are-consistent-with-domain-expertise-prior-predictive-checks"><i class="fa fa-check"></i><b>F.2.1</b>  Checking whether assumptions are consistent with  domain expertise: Prior predictive checks</a></li>
<li class="chapter" data-level="F.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#testing-for-correct-posterior-approximations-checks-of-computational-faithfulness"><i class="fa fa-check"></i><b>F.2.2</b>  Testing for correct posterior approximations: Checks of computational faithfulness</a></li>
<li class="chapter" data-level="F.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#sensitivity-of-the-model"><i class="fa fa-check"></i><b>F.2.3</b>  Sensitivity of the model</a></li>
<li class="chapter" data-level="F.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#does-the-model-adequately-capture-the-dataposterior-predictive-checks"><i class="fa fa-check"></i><b>F.2.4</b>  Does the model adequately capture the data?–Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="F.3" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-17"><i class="fa fa-check"></i><b>F.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>G</b> Exercises</a>
<ul>
<li class="chapter" data-level="G.1" data-path="exercises.html"><a href="exercises.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>G.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="G.1.1" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart1"><i class="fa fa-check"></i><b>G.1.1</b> Practice using the <code>pnorm()</code> function–Part 1</a></li>
<li class="chapter" data-level="G.1.2" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart2"><i class="fa fa-check"></i><b>G.1.2</b> Practice using the <code>pnorm()</code> function–Part 2</a></li>
<li class="chapter" data-level="G.1.3" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart3"><i class="fa fa-check"></i><b>G.1.3</b> Practice using the <code>pnorm()</code> function–Part 3</a></li>
<li class="chapter" data-level="G.1.4" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisesqnormPart1"><i class="fa fa-check"></i><b>G.1.4</b> Practice using the <code>qnorm()</code> function–Part 1</a></li>
<li class="chapter" data-level="G.1.5" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisesqnormPart2"><i class="fa fa-check"></i><b>G.1.5</b> Practice using the <code>qnorm()</code> function–Part 2</a></li>
<li class="chapter" data-level="G.1.6" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisessamples1"><i class="fa fa-check"></i><b>G.1.6</b> Practice getting summaries from samples–Part 1</a></li>
<li class="chapter" data-level="G.1.7" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisessamples2"><i class="fa fa-check"></i><b>G.1.7</b> Practice getting summaries from samples–Part 2.</a></li>
<li class="chapter" data-level="G.1.8" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisesvcov1"><i class="fa fa-check"></i><b>G.1.8</b> Practice with a variance-covariance matrix for a bivariate distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.2" data-path="exercises.html"><a href="exercises.html#sec-BDAexercises"><i class="fa fa-check"></i><b>G.2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="G.2.1" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesDerivingBayes"><i class="fa fa-check"></i><b>G.2.1</b> Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="G.2.2" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj1"><i class="fa fa-check"></i><b>G.2.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="G.2.3" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj2"><i class="fa fa-check"></i><b>G.2.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="G.2.4" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj3"><i class="fa fa-check"></i><b>G.2.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="G.2.5" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj4"><i class="fa fa-check"></i><b>G.2.5</b> Conjugate forms 4</a></li>
<li class="chapter" data-level="G.2.6" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesWeightedMean"><i class="fa fa-check"></i><b>G.2.6</b> The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)</a></li>
</ul></li>
<li class="chapter" data-level="G.3" data-path="exercises.html"><a href="exercises.html#ex:compbda"><i class="fa fa-check"></i><b>G.3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="G.3.1" data-path="exercises.html"><a href="exercises.html#exr:simulatedlinearmod"><i class="fa fa-check"></i><b>G.3.1</b> Check for parameter recovery in a linear model using simulated data.</a></li>
<li class="chapter" data-level="G.3.2" data-path="exercises.html"><a href="exercises.html#exr:linearmod"><i class="fa fa-check"></i><b>G.3.2</b> A simple linear model.</a></li>
<li class="chapter" data-level="G.3.3" data-path="exercises.html"><a href="exercises.html#exr:compbda-biasedpost"><i class="fa fa-check"></i><b>G.3.3</b> Revisiting the button-pressing example with different priors.</a></li>
<li class="chapter" data-level="G.3.4" data-path="exercises.html"><a href="exercises.html#exr:ppd"><i class="fa fa-check"></i><b>G.3.4</b> Posterior predictive checks with a log-normal model.</a></li>
<li class="chapter" data-level="G.3.5" data-path="exercises.html"><a href="exercises.html#exr:skew"><i class="fa fa-check"></i><b>G.3.5</b> A skew normal distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.4" data-path="exercises.html"><a href="exercises.html#sec-LMexercises"><i class="fa fa-check"></i><b>G.4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="G.4.1" data-path="exercises.html"><a href="exercises.html#exr:powerposing"><i class="fa fa-check"></i><b>G.4.1</b> A simple linear regression: Power posing and testosterone.</a></li>
<li class="chapter" data-level="G.4.2" data-path="exercises.html"><a href="exercises.html#exr:pupils"><i class="fa fa-check"></i><b>G.4.2</b> Another linear regression model: Revisiting attentional load effect on pupil size.</a></li>
<li class="chapter" data-level="G.4.3" data-path="exercises.html"><a href="exercises.html#exr:lognormalm"><i class="fa fa-check"></i><b>G.4.3</b> Log-normal model: Revisiting the effect of trial on finger tapping times.</a></li>
<li class="chapter" data-level="G.4.4" data-path="exercises.html"><a href="exercises.html#exr:reg-logistic"><i class="fa fa-check"></i><b>G.4.4</b> Logistic regression: Revisiting the effect of set size on free recall.</a></li>
<li class="chapter" data-level="G.4.5" data-path="exercises.html"><a href="exercises.html#exr:red"><i class="fa fa-check"></i><b>G.4.5</b> Red is the sexiest color.</a></li>
</ul></li>
<li class="chapter" data-level="G.5" data-path="exercises.html"><a href="exercises.html#sec-HLMexercises"><i class="fa fa-check"></i><b>G.5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="G.5.1" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-normal"><i class="fa fa-check"></i><b>G.5.1</b> A hierarchical model (normal likelihood) of cognitive load on pupil size.</a></li>
<li class="chapter" data-level="G.5.2" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-logn"><i class="fa fa-check"></i><b>G.5.2</b> Are subject relatives easier to process than object relatives (log-normal likelihood)?</a></li>
<li class="chapter" data-level="G.5.3" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseMandarinRC"><i class="fa fa-check"></i><b>G.5.3</b> Relative clause processing in Mandarin Chinese</a></li>
<li class="chapter" data-level="G.5.4" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseEnglishAgrmt"><i class="fa fa-check"></i><b>G.5.4</b>  Agreement attraction in comprehension</a></li>
<li class="chapter" data-level="G.5.5" data-path="exercises.html"><a href="exercises.html#exr:ab"><i class="fa fa-check"></i><b>G.5.5</b>  Attentional blink (Bernoulli likelihood)</a></li>
<li class="chapter" data-level="G.5.6" data-path="exercises.html"><a href="exercises.html#exr:strooplogis-brms"><i class="fa fa-check"></i><b>G.5.6</b> Is there a Stroop effect in accuracy?</a></li>
<li class="chapter" data-level="G.5.7" data-path="exercises.html"><a href="exercises.html#exr:stroop-dist"><i class="fa fa-check"></i><b>G.5.7</b>  Distributional regression for the Stroop effect.</a></li>
<li class="chapter" data-level="G.5.8" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseGramCE"><i class="fa fa-check"></i><b>G.5.8</b> The  grammaticality illusion</a></li>
</ul></li>
<li class="chapter" data-level="G.6" data-path="exercises.html"><a href="exercises.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>G.6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="G.6.1" data-path="exercises.html"><a href="exercises.html#exr:ContrastsPersian"><i class="fa fa-check"></i><b>G.6.1</b> Contrast coding for a four-condition design</a></li>
<li class="chapter" data-level="G.6.2" data-path="exercises.html"><a href="exercises.html#exr:ContrastsNPIHelmert"><i class="fa fa-check"></i><b>G.6.2</b>  Helmert coding for a six-condition design.</a></li>
<li class="chapter" data-level="G.6.3" data-path="exercises.html"><a href="exercises.html#exr:ContrastsNcomparisons"><i class="fa fa-check"></i><b>G.6.3</b> Number of possible comparisons in a single model.</a></li>
</ul></li>
<li class="chapter" data-level="G.7" data-path="exercises.html"><a href="exercises.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>G.7</b> Contrast coding with two predictor variables</a>
<ul>
<li class="chapter" data-level="G.7.1" data-path="exercises.html"><a href="exercises.html#exr:ContrastsPersianANOVA"><i class="fa fa-check"></i><b>G.7.1</b> ANOVA coding for a four-condition design.</a></li>
<li class="chapter" data-level="G.7.2" data-path="exercises.html"><a href="exercises.html#exr:Contrasts2x2x2Dillon2013"><i class="fa fa-check"></i><b>G.7.2</b> ANOVA and nested comparisons in a <span class="math inline">\(2\times 2\times 2\)</span> design</a></li>
</ul></li>
<li class="chapter" data-level="G.8" data-path="exercises.html"><a href="exercises.html#introduction-to-the-probabilistic-programming-language-stan"><i class="fa fa-check"></i><b>G.8</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="G.8.1" data-path="exercises.html"><a href="exercises.html#exr:first"><i class="fa fa-check"></i><b>G.8.1</b> A very simple model.</a></li>
<li class="chapter" data-level="G.8.2" data-path="exercises.html"><a href="exercises.html#exr:badstan"><i class="fa fa-check"></i><b>G.8.2</b> Incorrect Stan model.</a></li>
<li class="chapter" data-level="G.8.3" data-path="exercises.html"><a href="exercises.html#exr:skewstan"><i class="fa fa-check"></i><b>G.8.3</b> Using Stan documentation.</a></li>
<li class="chapter" data-level="G.8.4" data-path="exercises.html"><a href="exercises.html#exr:linkfunction"><i class="fa fa-check"></i><b>G.8.4</b> The probit link function as an alternative to the logit function.</a></li>
<li class="chapter" data-level="G.8.5" data-path="exercises.html"><a href="exercises.html#exr:logisticstan"><i class="fa fa-check"></i><b>G.8.5</b> Examining the position of the queued word on recall.</a></li>
<li class="chapter" data-level="G.8.6" data-path="exercises.html"><a href="exercises.html#exr:fallacy"><i class="fa fa-check"></i><b>G.8.6</b> The conjunction fallacy.</a></li>
</ul></li>
<li class="chapter" data-level="G.9" data-path="exercises.html"><a href="exercises.html#hierarchical-models-and-reparameterization"><i class="fa fa-check"></i><b>G.9</b> Hierarchical models and reparameterization</a>
<ul>
<li class="chapter" data-level="G.9.1" data-path="exercises.html"><a href="exercises.html#exr:stroop"><i class="fa fa-check"></i><b>G.9.1</b> A log-normal model in Stan.</a></li>
<li class="chapter" data-level="G.9.2" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-logn-stan"><i class="fa fa-check"></i><b>G.9.2</b> A by-subjects and by-items hierarchical model with a log-normal likelihood.</a></li>
<li class="chapter" data-level="G.9.3" data-path="exercises.html"><a href="exercises.html#exr:strooplogis"><i class="fa fa-check"></i><b>G.9.3</b> A hierarchical logistic regression with Stan.</a></li>
<li class="chapter" data-level="G.9.4" data-path="exercises.html"><a href="exercises.html#exr:distr-stan"><i class="fa fa-check"></i><b>G.9.4</b> A distributional regression model of the effect of cloze probability on the N400.</a></li>
</ul></li>
<li class="chapter" data-level="G.10" data-path="exercises.html"><a href="exercises.html#sec-customexercises"><i class="fa fa-check"></i><b>G.10</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="G.10.1" data-path="exercises.html"><a href="exercises.html#exr:shiftedlogn"><i class="fa fa-check"></i><b>G.10.1</b> Fitting a  shifted log-normal distribution.</a></li>
<li class="chapter" data-level="G.10.2" data-path="exercises.html"><a href="exercises.html#exr:wald"><i class="fa fa-check"></i><b>G.10.2</b> Fitting a Wald distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.11" data-path="exercises.html"><a href="exercises.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>G.11</b> Meta-analysis and measurement error models</a>
<ul>
<li class="chapter" data-level="G.11.1" data-path="exercises.html"><a href="exercises.html#exr:REMAMEExtracting"><i class="fa fa-check"></i><b>G.11.1</b> Extracting estimates from published papers</a></li>
<li class="chapter" data-level="G.11.2" data-path="exercises.html"><a href="exercises.html#exr:REMAMEBuerki"><i class="fa fa-check"></i><b>G.11.2</b> A meta-analysis of picture-word interference data</a></li>
<li class="chapter" data-level="G.11.3" data-path="exercises.html"><a href="exercises.html#exr:REMAMELiEnglish"><i class="fa fa-check"></i><b>G.11.3</b> Measurement error model for English VOT data</a></li>
</ul></li>
<li class="chapter" data-level="G.12" data-path="exercises.html"><a href="exercises.html#introduction-to-model-comparison"><i class="fa fa-check"></i><b>G.12</b> Introduction to model comparison</a></li>
<li class="chapter" data-level="G.13" data-path="exercises.html"><a href="exercises.html#bayes-factors"><i class="fa fa-check"></i><b>G.13</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="G.13.1" data-path="exercises.html"><a href="exercises.html#exr:bysubjects"><i class="fa fa-check"></i><b>G.13.1</b> Is there evidence for differences in the effect of cloze probability among the subjects?</a></li>
<li class="chapter" data-level="G.13.2" data-path="exercises.html"><a href="exercises.html#exr:bf-logn"><i class="fa fa-check"></i><b>G.13.2</b> Is there evidence for the claim that English subject relative clauses are easier to process than object relative clauses?</a></li>
<li class="chapter" data-level="G.13.3" data-path="exercises.html"><a href="exercises.html#exr:bf-logistic"><i class="fa fa-check"></i><b>G.13.3</b> In the Grodner and Gibson 2005 data, in question-response accuracies, is there evidence for the claim that sentences with subject relative clauses are easier to comprehend?</a></li>
<li class="chapter" data-level="G.13.4" data-path="exercises.html"><a href="exercises.html#exr:lognstan"><i class="fa fa-check"></i><b>G.13.4</b> Bayes factor and bounded parameters using Stan.</a></li>
</ul></li>
<li class="chapter" data-level="G.14" data-path="exercises.html"><a href="exercises.html#cross-validation"><i class="fa fa-check"></i><b>G.14</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="G.14.1" data-path="exercises.html"><a href="exercises.html#exr:logcv"><i class="fa fa-check"></i><b>G.14.1</b> Predictive accuracy of the linear and the logarithm effect of cloze probability.</a></li>
<li class="chapter" data-level="G.14.2" data-path="exercises.html"><a href="exercises.html#exr:stroopcv"><i class="fa fa-check"></i><b>G.14.2</b> Log-normal model</a></li>
<li class="chapter" data-level="G.14.3" data-path="exercises.html"><a href="exercises.html#exr:logrec"><i class="fa fa-check"></i><b>G.14.3</b> Log-normal vs rec-normal model in Stan</a></li>
</ul></li>
<li class="chapter" data-level="G.15" data-path="exercises.html"><a href="exercises.html#introduction-to-cognitive-modeling"><i class="fa fa-check"></i><b>G.15</b> Introduction to cognitive modeling</a></li>
<li class="chapter" data-level="G.16" data-path="exercises.html"><a href="exercises.html#multinomial-processing-trees"><i class="fa fa-check"></i><b>G.16</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="G.16.1" data-path="exercises.html"><a href="exercises.html#exr:mult"><i class="fa fa-check"></i><b>G.16.1</b> Modeling multiple categorical responses.</a></li>
<li class="chapter" data-level="G.16.2" data-path="exercises.html"><a href="exercises.html#exr:mpt-mnm"><i class="fa fa-check"></i><b>G.16.2</b> An alternative MPT to model the picture recognition task.</a></li>
<li class="chapter" data-level="G.16.3" data-path="exercises.html"><a href="exercises.html#exr:edit-mpt-cat"><i class="fa fa-check"></i><b>G.16.3</b> A simple MPT model that incorporates phonological complexity in the picture recognition task.</a></li>
<li class="chapter" data-level="G.16.4" data-path="exercises.html"><a href="exercises.html#exr:mpt"><i class="fa fa-check"></i><b>G.16.4</b> A more hierarchical MPT.</a></li>
<li class="chapter" data-level="G.16.5" data-path="exercises.html"><a href="exercises.html#exr:mpt-adv"><i class="fa fa-check"></i><b>G.16.5</b> <strong>Advanced</strong>: Multinomial processing trees.</a></li>
</ul></li>
<li class="chapter" data-level="G.17" data-path="exercises.html"><a href="exercises.html#mixture-models"><i class="fa fa-check"></i><b>G.17</b> Mixture models</a>
<ul>
<li class="chapter" data-level="G.17.1" data-path="exercises.html"><a href="exercises.html#exr:pcorrect"><i class="fa fa-check"></i><b>G.17.1</b> Changes in the true point values.</a></li>
<li class="chapter" data-level="G.17.2" data-path="exercises.html"><a href="exercises.html#exr:mixhier"><i class="fa fa-check"></i><b>G.17.2</b> RTs in schizophrenic patients and control.</a></li>
<li class="chapter" data-level="G.17.3" data-path="exercises.html"><a href="exercises.html#exr:mixbias"><i class="fa fa-check"></i><b>G.17.3</b> <strong>Advanced:</strong> Guessing bias in the model.</a></li>
</ul></li>
<li class="chapter" data-level="G.18" data-path="exercises.html"><a href="exercises.html#a-simple-accumulator-model-to-account-for-choice-response-time"><i class="fa fa-check"></i><b>G.18</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="G.18.1" data-path="exercises.html"><a href="exercises.html#exr:recovery"><i class="fa fa-check"></i><b>G.18.1</b> Can we recover the true point values of the parameters of a model when dealing with a contaminant distribution?</a></li>
<li class="chapter" data-level="G.18.2" data-path="exercises.html"><a href="exercises.html#exr:lnracescale"><i class="fa fa-check"></i><b>G.18.2</b> Can the log-normal race model account for fast errors?</a></li>
<li class="chapter" data-level="G.18.3" data-path="exercises.html"><a href="exercises.html#exr:lnldt"><i class="fa fa-check"></i><b>G.18.3</b> Accounting for response time and choice in the lexical decision task using the log-normal race model.</a></li>
</ul></li>
<li class="chapter" data-level="G.19" data-path="exercises.html"><a href="exercises.html#sec-priorsexercises"><i class="fa fa-check"></i><b>G.19</b> The Art and Science of  Prior Elicitation</a>
<ul>
<li class="chapter" data-level="G.19.1" data-path="exercises.html"><a href="exercises.html#exr:PriorsRCs"><i class="fa fa-check"></i><b>G.19.1</b> Develop a plausible informative prior for the difference between object and subject relative clause reading times</a></li>
<li class="chapter" data-level="G.19.2" data-path="exercises.html"><a href="exercises.html#exr:Priorslocalcoherence"><i class="fa fa-check"></i><b>G.19.2</b> Extracting an informative prior from a published paper for a future study</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-models-with-brms---extended" class="section level1 hasAnchor" number="20">
<h1><span class="header-section-number">A</span> Regression models with <code>brms</code> - Extended<a href="regression-models-with-brms---extended.html#regression-models-with-brms---extended" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="app-efficientpriorpd" class="section level2 hasAnchor" number="20.1">
<h2><span class="header-section-number">A.1</span> An efficient function for generating prior predictive distributions in R<a href="regression-models-with-brms---extended.html#app-efficientpriorpd" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As noted in section <a href="ch-compbda.html#sec-priorpred">3.3</a>, generating prior predictive distributions can be computationally slow if done naively in R with a for-loop: Producing <span class="math inline">\(1000\)</span> samples of the prior predictive distribution for our model from section <a href="ch-compbda.html#sec-priorpred">3.3</a> results in <span class="math inline">\(361000\)</span> predicted values, which takes a few seconds to compute. Although this approach works, it is not optimal for more complex models or larger datasets.</p>
<p>To address this, one could use a more efficient function using the <code>map2_dfr()</code> function from the <code>purrr</code> package, which yields approximately a 10-fold increase in speed. Although the distributions should be the same with both functions, the specific numbers in the tables won’t be, due to the randomness in the process of sampling.</p>
<p>The <code>purrr</code> function  <code>map2_dfr()</code> (which works similarly to the base R function <code>lapply()</code> and <code>Map()</code>) essentially runs a for-loop, and builds a data frame with the output. It iterates over the values of two vectors (or lists) simultaneously, here, <code>mu_samples</code> and <code>sigma_samples</code> and, in each iteration, it applies a function to each value of the two vectors, here, <code>mu</code> and <code>sigma</code>. The output of each function is a data frame (or tibble in this case) with <code>N_obs</code> observations which is bound in a larger data frame at the end of the loop. Each of these data frames bound together represents an iteration in the simulation, and we identify the iterations by setting <code>.id = "iter"</code>.</p>
<p>Although this method for generating prior predictive distributions is a bit involved, it has an advantage in comparison to the more straightforward use of <code>predict()</code> (or <code>posterior_predict()</code>, which can also generate prior predictions) together with setting <code>sample_prior = "only"</code> in the <code>brms</code> model (as we will do in section <a href="ch-compbda.html#sec-lognormal">3.7.2</a>). Our method of generating prior predictive distributions does not depend on Stan’s sampler, which means that no matter the number of iterations in our simulation or how uninformative our priors, there will never be any convergence problems.</p>
<div class="sourceCode" id="cb1084"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1084-1"><a href="regression-models-with-brms---extended.html#cb1084-1" aria-hidden="true"></a><span class="kw">library</span>(purrr)</span>
<span id="cb1084-2"><a href="regression-models-with-brms---extended.html#cb1084-2" aria-hidden="true"></a><span class="co"># Define the function:</span></span>
<span id="cb1084-3"><a href="regression-models-with-brms---extended.html#cb1084-3" aria-hidden="true"></a>normal_predictive_distribution &lt;-<span class="st"> </span><span class="cf">function</span>(mu_samples,</span>
<span id="cb1084-4"><a href="regression-models-with-brms---extended.html#cb1084-4" aria-hidden="true"></a>                                           sigma_samples,</span>
<span id="cb1084-5"><a href="regression-models-with-brms---extended.html#cb1084-5" aria-hidden="true"></a>                                           N_obs) {</span>
<span id="cb1084-6"><a href="regression-models-with-brms---extended.html#cb1084-6" aria-hidden="true"></a>  <span class="kw">map2_dfr</span>(mu_samples, sigma_samples, <span class="cf">function</span>(mu, sigma) {</span>
<span id="cb1084-7"><a href="regression-models-with-brms---extended.html#cb1084-7" aria-hidden="true"></a>    <span class="kw">tibble</span>(<span class="dt">trialn =</span> <span class="kw">seq_len</span>(N_obs),</span>
<span id="cb1084-8"><a href="regression-models-with-brms---extended.html#cb1084-8" aria-hidden="true"></a>           <span class="dt">t_pred =</span> <span class="kw">rnorm</span>(N_obs, mu, sigma))</span>
<span id="cb1084-9"><a href="regression-models-with-brms---extended.html#cb1084-9" aria-hidden="true"></a>  }, <span class="dt">.id =</span> <span class="st">&quot;iter&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1084-10"><a href="regression-models-with-brms---extended.html#cb1084-10" aria-hidden="true"></a><span class="st">    </span><span class="co"># .id is always a string and</span></span>
<span id="cb1084-11"><a href="regression-models-with-brms---extended.html#cb1084-11" aria-hidden="true"></a><span class="st">    </span><span class="co"># needs to be converted to a number</span></span>
<span id="cb1084-12"><a href="regression-models-with-brms---extended.html#cb1084-12" aria-hidden="true"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">iter =</span> <span class="kw">as.numeric</span>(iter))</span>
<span id="cb1084-13"><a href="regression-models-with-brms---extended.html#cb1084-13" aria-hidden="true"></a>}</span>
<span id="cb1084-14"><a href="regression-models-with-brms---extended.html#cb1084-14" aria-hidden="true"></a><span class="co"># Test the timing:</span></span>
<span id="cb1084-15"><a href="regression-models-with-brms---extended.html#cb1084-15" aria-hidden="true"></a><span class="kw">tic</span>()</span>
<span id="cb1084-16"><a href="regression-models-with-brms---extended.html#cb1084-16" aria-hidden="true"></a>prior_pred &lt;-</span>
<span id="cb1084-17"><a href="regression-models-with-brms---extended.html#cb1084-17" aria-hidden="true"></a><span class="st">  </span><span class="kw">normal_predictive_distribution</span>(<span class="dt">mu_samples =</span> mu_samples,</span>
<span id="cb1084-18"><a href="regression-models-with-brms---extended.html#cb1084-18" aria-hidden="true"></a>                                 <span class="dt">sigma_samples =</span> sigma_samples,</span>
<span id="cb1084-19"><a href="regression-models-with-brms---extended.html#cb1084-19" aria-hidden="true"></a>                                 <span class="dt">N_obs =</span> N_obs)</span>
<span id="cb1084-20"><a href="regression-models-with-brms---extended.html#cb1084-20" aria-hidden="true"></a><span class="kw">toc</span>()</span></code></pre></div>
<pre><code>## 0.431 sec elapsed</code></pre>
</div>
<div id="app-truncation" class="section level2 hasAnchor" number="20.2">
<h2><span class="header-section-number">A.2</span> Truncated distributions<a href="regression-models-with-brms---extended.html#app-truncation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Any distribution can be truncated. For a continuous distribution, the truncated version of the original distribution will have non-zero probability density values for a continuous subset of the original coverage. To make this more concrete, in the example of section <a href="ch-compbda.html#sec-revisit">3.5</a>, the normal distribution has coverage for values between minus infinity to plus infinity, and our truncated version <span class="math inline">\(\mathit{Normal}_+\)</span> has coverage between zero and plus infinity: all negative values have a density of zero. Let’s see how we can generalize this to be able to understand any truncation of any continuous distribution. (For the discrete case, we can simply replace the integral with a sum, and replace PDF with PMF).</p>
<p>From the axiomatic definitions of probability, we know that the area below a PDF, <span class="math inline">\(f(x)\)</span>, must be equal to one (section <a href="ch-intro.html#introprob">1.1</a>). More formally, this means that the integral of <span class="math inline">\(f\)</span> evaluated as <span class="math inline">\(f(-\infty &lt;X &lt; \infty)\)</span> should be equal to one:</p>
<p><span class="math display">\[\begin{equation}
\int_{-\infty}^{\infty} f(x) dx = 1
\end{equation}\]</span></p>
<p>But if the distribution is truncated, <span class="math inline">\(f\)</span> is going to be evaluated in some subset of its possible values, <span class="math inline">\(f(a &lt;X &lt; b)\)</span>; in the specific case of <span class="math inline">\(\mathit{Normal}_+\)</span>, for example, <span class="math inline">\(a = 0\)</span>, and <span class="math inline">\(b=\infty\)</span>. In the general case, this means that the integral of the PDF evaluated for <span class="math inline">\(a &lt;X &lt; b\)</span> will be lower than one, unless <span class="math inline">\(a=-\infty\)</span> and <span class="math inline">\(b=+\infty\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\int_{a}^{b} f(x) dx &lt; 1
\end{equation}\]</span></p>
<p>We want to ensure that we build a new PDF for the truncated distribution so that even though it has less coverage than the non-truncated version, it still integrates to one. To achieve this, we divide the “unnormalized” PDF by the total area of <span class="math inline">\(f(a &lt;X &lt; b)\)</span> (recall the discussion surrounding Equation <a href="ch-intro.html#eq:factork">(1.1)</a>):</p>
<p><span class="math display">\[\begin{equation}
f_{[a,b]}(x) = \frac{f(x)}{\int_{a}^{b} f(x) dx}
\end{equation}\]</span></p>
<p>The denominator of the previous equation is the difference between the CDF evaluated at <span class="math inline">\(X = b\)</span> and the CDF evaluated at <span class="math inline">\(X =a\)</span>; this can be written as <span class="math inline">\(F(b) - F(a)\)</span>:</p>
<p><span class="math display" id="eq:truncPDF">\[\begin{equation}
f_{[a,b]}(x) = \frac{f(x)}{F(b) - F(a)}
\tag{A.1}
\end{equation}\]</span></p>
<p>For the specific case where <span class="math inline">\(f(x)\)</span> is <span class="math inline">\(Normal(x | 0, \sigma)\)</span> and we want the PDF of <span class="math inline">\(Normal_+(x | 0, \sigma)\)</span>, the bounds will be <span class="math inline">\(a= 0\)</span> and <span class="math inline">\(b =\infty\)</span>.</p>
<p><span class="math display">\[\begin{equation}
Normal_+(x |0, \sigma) = \frac{Normal(x | 0, \sigma)}{1/2}
\end{equation}\]</span></p>
<p>Because <span class="math inline">\(F(X= b =\infty) = 1\)</span> and <span class="math inline">\(F(X = a = 0) = 1/2\)</span>.</p>
<p>You can verify this in R (this is valid for any value of <code>sd</code>).</p>
<div class="sourceCode" id="cb1086"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1086-1"><a href="regression-models-with-brms---extended.html#cb1086-1" aria-hidden="true"></a><span class="kw">dnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">0</span>) <span class="op">*</span><span class="st"> </span><span class="dv">2</span> <span class="op">==</span><span class="st"> </span><span class="kw">dtnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">a =</span> <span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Unless the truncation of the normal distribution is symmetrical, the mean <span class="math inline">\(\mu\)</span> of the truncated normal does not coincide with the mean of the parent (untruncated) normal distribution; call this mean of the  parent distribution <span class="math inline">\(\hat{\mu}\)</span>. For any type of truncation, the standard deviation of the truncated distribution <span class="math inline">\(\sigma\)</span> does not coincide with the standard deviation of the parent distribution; call this latter standard deviation <span class="math inline">\(\hat\sigma\)</span>. Confusingly enough, the family of truncated functions <code>*tnorm()</code> keeps the names of the arguments of the family of functions <code>*norm()</code>: <code>mean</code> and <code>sd</code>. So, when defining a truncated normal distribution like <code>dtnorm(mean = 300, sd = 100, a = 0, b = Inf)</code>, the <code>mean</code> and <code>sd</code> refer to the mean <span class="math inline">\(\hat{\mu}\)</span> and standard deviation <span class="math inline">\(\hat\sigma\)</span> of the untruncated parent distribution.</p>
<p>Sometimes one needs to model observed data as coming from a truncated normal distribution. An example would be a vector of observed standard deviations; perhaps one wants to use these estimates to work out a truncated normal prior. In order to derive such an empirically motivated prior, we have to work out what mean and standard deviation we need to use in a truncated normal distribution. We could compute the mean and standard deviation from the observed vector of standard deviations, and then use the procedure shown below to work out the mean and standard deviation that we would need to put into the truncated normal distribution. This approach is used in online chapter <a href="ch-priors.html#ch-priors">E</a>, section <a href="ch-priors.html#sec-varcomppriors">E.1.4</a> for working out a prior based on standard deviation estimates from existing data.</p>
<p>The mean and standard deviation of the parent distribution of a truncated normal (<span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span>) with boundaries <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, given the mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> of the truncated normal, are computed as follows <span class="citation">(Johnson, Kotz, and Balakrishnan <a href="#ref-johnson1995continuous" role="doc-biblioref">1995</a>)</span>. <span class="math inline">\(\phi(X)\)</span> is the PDF of the standard normal (i.e., <span class="math inline">\(\mathit{Normal}(\mu=0, \sigma=1)\)</span>) evaluated at <span class="math inline">\(X\)</span>, and <span class="math inline">\(\Phi(X)\)</span> is the CDF of the standard normal evaluated at <span class="math inline">\(X\)</span>.</p>
<p>First, define two terms <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> for convenience:</p>
<p><span class="math display">\[\begin{align}
\alpha =(a-\hat\mu )/\hat\sigma &amp;&amp; \beta =(b-\hat\mu )/\hat\sigma
\end{align}\]</span></p>
<p>Then, the mean <span class="math inline">\(\mu\)</span> of the truncated distribution can be computed as follows based on the parameters of the parent distribution:</p>
<p><span class="math display" id="eq:meantrunc">\[\begin{equation}
\mu  = \hat\mu - \hat\sigma {\frac {\phi (\beta )-\phi (\alpha )}{\Phi (\beta )-\Phi (\alpha )}} 
\tag{A.2}
\end{equation}\]</span></p>
<p>The variance <span class="math inline">\(\sigma^2\)</span> of the truncated distribution is:</p>
<p><span class="math display" id="eq:vartrunc">\[\begin{equation}
\sigma^2 = \hat\sigma^2 \times \left( 1 -  \frac{\beta \phi (\alpha )-\alpha \phi (\beta )}{\Phi (\beta )-\Phi (\alpha )}  - 
\left(\frac {\phi (\alpha )-\phi (\beta )}{\Phi (\beta )-\Phi (\alpha )}\right)^2
\right)
\tag{A.3}
\end{equation}\]</span></p>
<p>Equations <a href="regression-models-with-brms---extended.html#eq:meantrunc">(A.2)</a> and <a href="regression-models-with-brms---extended.html#eq:vartrunc">(A.3)</a> have two variables, so if one is given the values for the truncated distribution <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, one can solve (using algebra) for the mean and standard deviation of the untruncated distribution, <span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span>.</p>
<p>For example, suppose that <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=500\)</span>, and that the mean and standard deviation of the untruncated parent distribution is <span class="math inline">\(\hat\mu=300\)</span> and <span class="math inline">\(\hat\sigma=200\)</span>. We can simulate such a situation and estimate the mean and standard deviation of the truncated distribution:</p>
<div class="sourceCode" id="cb1088"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1088-1"><a href="regression-models-with-brms---extended.html#cb1088-1" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">rtnorm</span>(<span class="dv">10000000</span>, <span class="dt">mean =</span> <span class="dv">300</span>, <span class="dt">sd =</span> <span class="dv">200</span>, <span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">b =</span> <span class="dv">500</span>)</span>
<span id="cb1088-2"><a href="regression-models-with-brms---extended.html#cb1088-2" aria-hidden="true"></a><span class="co">## the mean and sd of the truncated distributions</span></span>
<span id="cb1088-3"><a href="regression-models-with-brms---extended.html#cb1088-3" aria-hidden="true"></a><span class="co">## using simulation:</span></span>
<span id="cb1088-4"><a href="regression-models-with-brms---extended.html#cb1088-4" aria-hidden="true"></a><span class="kw">mean</span>(x)</span></code></pre></div>
<pre><code>## [1] 271</code></pre>
<div class="sourceCode" id="cb1090"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1090-1"><a href="regression-models-with-brms---extended.html#cb1090-1" aria-hidden="true"></a><span class="kw">sd</span>(x)</span></code></pre></div>
<pre><code>## [1] 129</code></pre>
<p>These simulated values are identical to the values computed using equations <a href="regression-models-with-brms---extended.html#eq:meantrunc">(A.2)</a> and <a href="regression-models-with-brms---extended.html#eq:vartrunc">(A.3)</a>:</p>
<div class="sourceCode" id="cb1092"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1092-1"><a href="regression-models-with-brms---extended.html#cb1092-1" aria-hidden="true"></a>a &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb1092-2"><a href="regression-models-with-brms---extended.html#cb1092-2" aria-hidden="true"></a>b &lt;-<span class="st"> </span><span class="dv">500</span></span>
<span id="cb1092-3"><a href="regression-models-with-brms---extended.html#cb1092-3" aria-hidden="true"></a>bar_x &lt;-<span class="st"> </span><span class="dv">300</span></span>
<span id="cb1092-4"><a href="regression-models-with-brms---extended.html#cb1092-4" aria-hidden="true"></a>bar_sigma &lt;-<span class="st"> </span><span class="dv">200</span></span>
<span id="cb1092-5"><a href="regression-models-with-brms---extended.html#cb1092-5" aria-hidden="true"></a>alpha &lt;-<span class="st"> </span>(a <span class="op">-</span><span class="st"> </span>bar_x) <span class="op">/</span><span class="st"> </span>bar_sigma</span>
<span id="cb1092-6"><a href="regression-models-with-brms---extended.html#cb1092-6" aria-hidden="true"></a>beta &lt;-<span class="st"> </span>(b <span class="op">-</span><span class="st"> </span>bar_x) <span class="op">/</span><span class="st"> </span>bar_sigma</span>
<span id="cb1092-7"><a href="regression-models-with-brms---extended.html#cb1092-7" aria-hidden="true"></a>term1 &lt;-<span class="st"> </span>((<span class="kw">dnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">dnorm</span>(alpha)) <span class="op">/</span></span>
<span id="cb1092-8"><a href="regression-models-with-brms---extended.html#cb1092-8" aria-hidden="true"></a><span class="st">            </span>(<span class="kw">pnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(alpha)))</span>
<span id="cb1092-9"><a href="regression-models-with-brms---extended.html#cb1092-9" aria-hidden="true"></a>term2 &lt;-<span class="st"> </span>((beta <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(beta) <span class="op">-</span><span class="st"> </span>alpha <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(alpha)) <span class="op">/</span></span>
<span id="cb1092-10"><a href="regression-models-with-brms---extended.html#cb1092-10" aria-hidden="true"></a><span class="st">            </span>(<span class="kw">pnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(alpha)))</span>
<span id="cb1092-11"><a href="regression-models-with-brms---extended.html#cb1092-11" aria-hidden="true"></a><span class="co">## the mean and sd of the truncated distribution</span></span>
<span id="cb1092-12"><a href="regression-models-with-brms---extended.html#cb1092-12" aria-hidden="true"></a><span class="co">## computed analytically:</span></span>
<span id="cb1092-13"><a href="regression-models-with-brms---extended.html#cb1092-13" aria-hidden="true"></a>(mu &lt;-<span class="st"> </span>bar_x <span class="op">-</span><span class="st"> </span>bar_sigma <span class="op">*</span><span class="st"> </span>term1)</span></code></pre></div>
<pre><code>## [1] 271</code></pre>
<div class="sourceCode" id="cb1094"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1094-1"><a href="regression-models-with-brms---extended.html#cb1094-1" aria-hidden="true"></a>(sigma &lt;-<span class="st"> </span><span class="kw">sqrt</span>(bar_sigma<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>term2 <span class="op">-</span><span class="st"> </span>term1<span class="op">^</span><span class="dv">2</span>)))</span></code></pre></div>
<pre><code>## [1] 129</code></pre>
<p>The equations for the mean and variance of the truncated distribution (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>) can also be used to work out the mean and variance of the parent untruncated distribution (<span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span>), if one has estimates for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> (from data).</p>
<p>Suppose that we have observed data with mean <span class="math inline">\(\mu = 271\)</span> and <span class="math inline">\(\sigma=129\)</span>. We want to assume that the data are coming from a truncated normal which has lower bound <span class="math inline">\(0\)</span> and upper bound <span class="math inline">\(500\)</span>. What are the mean and standard deviation of the parent distribution, <span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span>?</p>
<p>To answer this question, first rewrite the equations as follows:</p>
<p><span class="math display" id="eq:meantruncrewritten">\[\begin{equation}
\mu  - \hat\mu + \hat\sigma {\frac {\phi (\beta )-\phi (\alpha )}{\Phi (\beta )-\Phi (\alpha )}} = 0
\tag{A.4}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:vartruncrewritten">\[\begin{equation}
\sigma^2 - \hat\sigma^2 \times \left( 1 -  \frac{\beta \phi (\alpha )-\alpha \phi (\beta )}{\Phi (\beta )-\Phi (\alpha )}  - 
\left(\frac {\phi (\alpha )-\phi (\beta )}{\Phi (\beta )-\Phi (\alpha )}\right)^2
\right) = 0
\tag{A.5}
\end{equation}\]</span></p>
<p>Next, solve for <span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span> given the observed mean and the standard deviation of the truncated distribution, and that one knows the boundaries (<span class="math inline">\(a\)</span>, and <span class="math inline">\(b\)</span>).</p>
<p>Define the  system of equations according to the specifications of  <code>multiroot()</code> from the package  <code>rootSolve</code>: <code>x</code> for the unknowns (<span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span>), and <code>parms</code> for the known parameters: <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and the mean and standard deviation of the truncated normal.</p>
<div class="sourceCode" id="cb1096"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1096-1"><a href="regression-models-with-brms---extended.html#cb1096-1" aria-hidden="true"></a>eq_system &lt;-<span class="st"> </span><span class="cf">function</span>(x, parms) {</span>
<span id="cb1096-2"><a href="regression-models-with-brms---extended.html#cb1096-2" aria-hidden="true"></a>  mu_hat &lt;-<span class="st"> </span>x[<span class="dv">1</span>]</span>
<span id="cb1096-3"><a href="regression-models-with-brms---extended.html#cb1096-3" aria-hidden="true"></a>  sigma_hat &lt;-<span class="st"> </span>x[<span class="dv">2</span>]</span>
<span id="cb1096-4"><a href="regression-models-with-brms---extended.html#cb1096-4" aria-hidden="true"></a>  alpha &lt;-<span class="st"> </span>(parms[<span class="st">&quot;a&quot;</span>] <span class="op">-</span><span class="st"> </span>mu_hat) <span class="op">/</span><span class="st"> </span>sigma_hat</span>
<span id="cb1096-5"><a href="regression-models-with-brms---extended.html#cb1096-5" aria-hidden="true"></a>  beta &lt;-<span class="st"> </span>(parms[<span class="st">&quot;b&quot;</span>] <span class="op">-</span><span class="st"> </span>mu_hat) <span class="op">/</span><span class="st"> </span>sigma_hat</span>
<span id="cb1096-6"><a href="regression-models-with-brms---extended.html#cb1096-6" aria-hidden="true"></a>  <span class="kw">c</span>(<span class="dt">F1 =</span> parms[<span class="st">&quot;mu&quot;</span>] <span class="op">-</span><span class="st"> </span>mu_hat <span class="op">+</span><span class="st"> </span>sigma_hat <span class="op">*</span></span>
<span id="cb1096-7"><a href="regression-models-with-brms---extended.html#cb1096-7" aria-hidden="true"></a><span class="st">    </span>(<span class="kw">dnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">dnorm</span>(alpha)) <span class="op">/</span><span class="st"> </span>(<span class="kw">pnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(alpha)),</span>
<span id="cb1096-8"><a href="regression-models-with-brms---extended.html#cb1096-8" aria-hidden="true"></a>    <span class="dt">F2 =</span> parms[<span class="st">&quot;sigma&quot;</span>] <span class="op">-</span><span class="st"> </span>sigma_hat <span class="op">*</span></span>
<span id="cb1096-9"><a href="regression-models-with-brms---extended.html#cb1096-9" aria-hidden="true"></a><span class="st">     </span><span class="kw">sqrt</span>((<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>((beta) <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(beta) <span class="op">-</span><span class="st"> </span>(alpha) <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(alpha)) <span class="op">/</span></span>
<span id="cb1096-10"><a href="regression-models-with-brms---extended.html#cb1096-10" aria-hidden="true"></a><span class="st">   </span>(<span class="kw">pnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(alpha)) <span class="op">-</span><span class="st"> </span>((<span class="kw">dnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">dnorm</span>(alpha)) <span class="op">/</span></span>
<span id="cb1096-11"><a href="regression-models-with-brms---extended.html#cb1096-11" aria-hidden="true"></a><span class="st">             </span>(<span class="kw">pnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(alpha)))<span class="op">^</span><span class="dv">2</span>)))</span>
<span id="cb1096-12"><a href="regression-models-with-brms---extended.html#cb1096-12" aria-hidden="true"></a>}</span></code></pre></div>
<p>Solving the two equations using <code>multiroot()</code> from the package <code>rootSolve</code> gives us the mean and standard deviation <span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span> of the parent normal distribution. (Notice that <code>x</code> is a required parameter of the previous function so that it works with <code>multiroot()</code>, however, outside of the function the variable <code>x</code> is a vector containing the samples of the truncated normal distribution generated with <code>rtnorm()</code>).</p>
<div class="sourceCode" id="cb1097"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1097-1"><a href="regression-models-with-brms---extended.html#cb1097-1" aria-hidden="true"></a>soln &lt;-<span class="st"> </span><span class="kw">multiroot</span>(<span class="dt">f =</span> eq_system,</span>
<span id="cb1097-2"><a href="regression-models-with-brms---extended.html#cb1097-2" aria-hidden="true"></a>                  <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb1097-3"><a href="regression-models-with-brms---extended.html#cb1097-3" aria-hidden="true"></a>                  <span class="dt">parms =</span> <span class="kw">c</span>(<span class="dt">a =</span> <span class="dv">0</span>,</span>
<span id="cb1097-4"><a href="regression-models-with-brms---extended.html#cb1097-4" aria-hidden="true"></a>                            <span class="dt">b =</span> <span class="dv">500</span>,</span>
<span id="cb1097-5"><a href="regression-models-with-brms---extended.html#cb1097-5" aria-hidden="true"></a>                            <span class="dt">mu =</span> <span class="kw">mean</span>(x),</span>
<span id="cb1097-6"><a href="regression-models-with-brms---extended.html#cb1097-6" aria-hidden="true"></a>                            <span class="dt">sigma =</span> <span class="kw">sd</span>(x)))</span>
<span id="cb1097-7"><a href="regression-models-with-brms---extended.html#cb1097-7" aria-hidden="true"></a>soln<span class="op">$</span>root</span></code></pre></div>
<pre><code>## [1] 300 200</code></pre>
<p>The function <code>compute_meansd_parent()</code> encapsulates the previous procedure and it is provided in the <code>bcogsci</code> package.</p>
</div>
<div id="app-intercept" class="section level2 hasAnchor" number="20.3">
<h2><span class="header-section-number">A.3</span> Intercepts in <code>brms</code><a href="regression-models-with-brms---extended.html#app-intercept" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we set up a prior for the intercept in <code>brms</code>, we actually set a prior for an intercept assuming that all the predictors are centered. This means that when predictors are not centered (and only then), there is a mismatch between the interpretation of the intercept as returned in the output of <code>brms</code> and the interpretation of the intercept with respect to its prior specification. In this case, only the intercept in the output corresponds to the formula in the <code>brms</code> call, that is, the intercept in the output corresponds to the non-centered model. However, as we show below, when the intercept is much larger than the effects that we are considering in the formula (what we generally call <span class="math inline">\(\beta\)</span>), this discrepancy hardly matters.</p>
<p>The reason for this mismatch when our predictors are uncentered is that <code>brms</code> increases  sampling efficiency by automatically centering all the predictors internally (that is the population-level design matrix <span class="math inline">\(X\)</span> is internally centered around its column means when <code>brms</code> fits a model). This did not matter in our previous examples because we centered our predictor (or we had no predictor), but it might matter if we want to have uncentered predictors. In the design we are discussing, a non-centered predictor of load will mean that the intercept, <span class="math inline">\(\alpha\)</span>, has a straightforward interpretation: the <span class="math inline">\(\alpha\)</span> is the mean pupil size when there is no attention load. This is in contrast with the centered version presented before, where the intercept <span class="math inline">\(\alpha\)</span> represents the pupil size for the average load of <code>2.44</code> (<code>c_load</code> is equal to 0). The difference between the non-centered model (below) and the centered version presented before is depicted in Figure <a href="regression-models-with-brms---extended.html#fig:centered-non-centered">A.1</a>.</p>
<p>Suppose that we are quite sure that the prior values for the no load condition (i.e., load is non-centered) fall between <span class="math inline">\(400\)</span> and <span class="math inline">\(1200\)</span> ms. In that case, the following prior could be set for <span class="math inline">\(\alpha\)</span>: <span class="math inline">\(\mathit{Normal}(800,200)\)</span>. In this case, the model becomes:</p>
<div class="sourceCode" id="cb1099"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1099-1"><a href="regression-models-with-brms---extended.html#cb1099-1" aria-hidden="true"></a>prior_nc &lt;-</span>
<span id="cb1099-2"><a href="regression-models-with-brms---extended.html#cb1099-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">800</span>, <span class="dv">200</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> Intercept),</span>
<span id="cb1099-3"><a href="regression-models-with-brms---extended.html#cb1099-3" aria-hidden="true"></a>    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb1099-4"><a href="regression-models-with-brms---extended.html#cb1099-4" aria-hidden="true"></a>    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> load))</span>
<span id="cb1099-5"><a href="regression-models-with-brms---extended.html#cb1099-5" aria-hidden="true"></a></span>
<span id="cb1099-6"><a href="regression-models-with-brms---extended.html#cb1099-6" aria-hidden="true"></a>fit_pupil_non_centered &lt;-<span class="st"> </span><span class="kw">brm</span>(p_size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Intercept <span class="op">+</span><span class="st"> </span>load,</span>
<span id="cb1099-7"><a href="regression-models-with-brms---extended.html#cb1099-7" aria-hidden="true"></a>                              <span class="dt">data =</span> df_pupil,</span>
<span id="cb1099-8"><a href="regression-models-with-brms---extended.html#cb1099-8" aria-hidden="true"></a>                              <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb1099-9"><a href="regression-models-with-brms---extended.html#cb1099-9" aria-hidden="true"></a>                              <span class="dt">prior =</span> prior_nc)</span></code></pre></div>
<p>When the predictor is non-centered as shown above, the regular centered intercept is removed by adding <code>0</code> to the formula, and by replacing the intercept with the “actual” intercept we want to set priors on with <code>Intercept</code>. The word <code>Intercept</code> is a reserved word; we cannot name any predictor with this name. This new parameter is also of class <code>b</code>, so its prior needs to be defined accordingly. Once we use <code>0 + Intercept + ..</code>, the intercept is not calculated with predictors that are automatically centered any more.</p>
<p>The output below shows that, as expected, although the posterior for the intercept has changed noticeably, the posterior for the effect of load remains virtually unchanged.</p>
<div class="sourceCode" id="cb1100"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1100-1"><a href="regression-models-with-brms---extended.html#cb1100-1" aria-hidden="true"></a><span class="kw">posterior_summary</span>(fit_pupil_non_centered,</span>
<span id="cb1100-2"><a href="regression-models-with-brms---extended.html#cb1100-2" aria-hidden="true"></a>                  <span class="dt">variable =</span> <span class="kw">c</span>(<span class="st">&quot;b_Intercept&quot;</span>, <span class="st">&quot;b_load&quot;</span>))</span></code></pre></div>
<pre><code>##             Estimate Est.Error   Q2.5 Q97.5
## b_Intercept    624.0      34.4 558.54 693.7
## b_load          32.3      11.6   8.58  54.6</code></pre>
<p>Notice the following potential pitfall. A model like the one below will fit a non-centered load predictor, but will assign a prior of <span class="math inline">\(\mathit{Normal}(800,200)\)</span> to the intercept of a model that assumes a centered predictor, <span class="math inline">\(\alpha_{centered}\)</span>, and not the current intercept, <span class="math inline">\(\alpha\)</span>.</p>
<div class="sourceCode" id="cb1102"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1102-1"><a href="regression-models-with-brms---extended.html#cb1102-1" aria-hidden="true"></a>prior_nc &lt;-</span>
<span id="cb1102-2"><a href="regression-models-with-brms---extended.html#cb1102-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">800</span>, <span class="dv">200</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb1102-3"><a href="regression-models-with-brms---extended.html#cb1102-3" aria-hidden="true"></a>    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb1102-4"><a href="regression-models-with-brms---extended.html#cb1102-4" aria-hidden="true"></a>    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> load))</span>
<span id="cb1102-5"><a href="regression-models-with-brms---extended.html#cb1102-5" aria-hidden="true"></a>fit_pupil_wrong &lt;-<span class="st"> </span><span class="kw">brm</span>(p_size <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>load,</span>
<span id="cb1102-6"><a href="regression-models-with-brms---extended.html#cb1102-6" aria-hidden="true"></a>                       <span class="dt">data =</span> df_pupil,</span>
<span id="cb1102-7"><a href="regression-models-with-brms---extended.html#cb1102-7" aria-hidden="true"></a>                       <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb1102-8"><a href="regression-models-with-brms---extended.html#cb1102-8" aria-hidden="true"></a>                       <span class="dt">prior =</span> prior_nc)</span></code></pre></div>
<p>What does it mean to set a prior to <span class="math inline">\(\alpha_{centered}\)</span> in a model that doesn’t include <span class="math inline">\(\alpha_{centered}\)</span>?</p>
<p>The fitted (expected) values of the non-centered model and the centered one are identical, that is, the values of the response distribution without the residual error are identical for both models:</p>
<p><span class="math display" id="eq:fitted">\[\begin{equation}
\alpha + load_n \cdot \beta = \alpha_{centered} + (load_n - mean(load)) \cdot \beta 
\tag{A.6}
\end{equation}\]</span></p>
<p>The left side of Equation <a href="regression-models-with-brms---extended.html#eq:fitted">(A.6)</a> refers to the expected values based on our current non-centered model, and the right side refers to the expected values based on the centered model. We can re-arrange terms to understand what the effect is of a prior on <span class="math inline">\(\alpha_{centered}\)</span> in our model that doesn’t include <span class="math inline">\(\alpha_{centered}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha + load_n \cdot \beta &amp;= \alpha_{centered} + load_n\cdot \beta - mean(load) \cdot \beta\\
\alpha  &amp;= \alpha_{centered}  - mean(load) \cdot \beta\\
\alpha + mean(load) \cdot \beta  &amp;= \alpha_{centered}  
\end{aligned}
\end{equation}\]</span></p>
<p>That means that in the non-centered model, we are actually setting our prior to <span class="math inline">\(\alpha + mean(load) \cdot \beta\)</span>.
When <span class="math inline">\(\beta\)</span> is very small (or the means of our predictors are very small because they might be “almost” centered), and the prior for <span class="math inline">\(\alpha\)</span> is very wide, we might hardly notice the difference between setting a prior to <span class="math inline">\(\alpha_{centered}\)</span> or to our actual <span class="math inline">\(\alpha\)</span> in a non-centered model (especially if the likelihood dominates anyway). But it is important to pay attention to what the parameters represent that we are setting priors on.</p>
<p>To sum up, <code>brms</code> automatically centers all predictors for posterior estimation, and the prior of the intercept is applied to the centered version of the model during model fitting. However, when the predictors specified in the formula are not centered, then <code>brms</code> uses the equations shown before to return in the output the posterior of the intercept for the non-centered predictors.<a href="#fn70" class="footnote-ref" id="fnref70"><sup>70</sup></a></p>
<p>In our example analyses with <code>brms</code> in this book, we will always center our predictors.</p>

<div class="figure"><span style="display:block;" id="fig:centered-non-centered"></span>
<img src="bayescogsci_files/figure-html/centered-non-centered-1.svg" alt="Regression lines for the non-centered and centered linear regressions. The intercept (\(\alpha\)) represented by a circle is positioned differently depending on the centering, whereas the slope (\(\beta\)) represented by a vertical dashed line has the same magnitude in both models." width="672" />
<p class="caption">
FIGURE A.1: Regression lines for the non-centered and centered linear regressions. The intercept (<span class="math inline">\(\alpha\)</span>) represented by a circle is positioned differently depending on the centering, whereas the slope (<span class="math inline">\(\beta\)</span>) represented by a vertical dashed line has the same magnitude in both models.
</p>
</div>
</div>
<div id="app-lognormal" class="section level2 hasAnchor" number="20.4">
<h2><span class="header-section-number">A.4</span> Understanding the log-normal likelihood<a href="regression-models-with-brms---extended.html#app-lognormal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It is important to understand what we are assuming with a log-normal likelihood. Formally, if a random variable <span class="math inline">\(Z\)</span> is normally distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the transformed random variable <span class="math inline">\(Y = \exp(Z)\)</span> is log-normally distributed and has density:</p>
<p><span class="math display">\[\begin{equation}
\mathit{LogNormal}(y|\mu,\sigma)=f(z)= \frac{1}{\sqrt{2\pi \sigma^2}y} \exp \left(-\frac{(\log(y)-\mu)^2}{2\sigma^2} \right)
\end{equation}\]</span></p>
<p>As explained in section <a href="ch-compbda.html#sec-lnfirst">3.7.1</a>, the model from Equation <a href="ch-reg.html#eq:rtloglik">(4.1)</a> is equivalent to the following:</p>
<p><span class="math display" id="eq:aX">\[\begin{equation}
\log(t_n) \sim \mathit{Normal}(\alpha + c\_trial_n \cdot \beta,\sigma)\\
\tag{A.7}
\end{equation}\]</span></p>
<p>The family of normal distributions is closed under linear transformations: that is, if <span class="math inline">\(X\)</span> is normally distributed with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, then (for any real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>), <span class="math inline">\(a X + b\)</span> is also normally distributed, with mean <span class="math inline">\(a \mu +b\)</span> (and standard deviation <span class="math inline">\(\sqrt{a^2\sigma^2}=|a|\sigma\)</span>).</p>
<p>This means that, assuming <span class="math inline">\(Z \sim \mathit{Normal}(\alpha, \sigma)\)</span>, Equation <a href="regression-models-with-brms---extended.html#eq:aX">(A.7)</a> can be re-written as follows:</p>
<p><span class="math display" id="eq:rtlogliknoncen">\[\begin{equation}
\log(rt_n) = Z + c\_trial_n \cdot \beta
\tag{A.8}
\end{equation}\]</span></p>
<p>Exponentiate both sides, and use the property of exponents that <span class="math inline">\(\exp(x+y)\)</span> is equal to <span class="math inline">\(\exp(x) \cdot \exp(y)\)</span>; set <span class="math inline">\(Y =\exp(Z)\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
rt_n &amp;= \exp \big(Z  + c\_trial_n \cdot \beta\big) \\
rt_n &amp;= \exp(Z )   \cdot \exp\big(c\_trial_n \cdot \beta\big) \\
rt_n &amp;= Y \cdot \exp\big(c\_trial_n \cdot \beta\big) 
\end{aligned}
\end{equation}\]</span></p>
<p>The last equation has two terms being multiplied, the first one, <span class="math inline">\(Y\)</span>, is telling us that we are assuming that finger tapping times are log-normally distributed with a median of <span class="math inline">\(\exp(\alpha)\)</span>, the second term, <span class="math inline">\(\exp(c\_trial_n \cdot \beta)\)</span> is telling us that the effect of trial number is multiplicative and grows or decays exponentially with the trial number. This has two important consequences:</p>
<ol style="list-style-type: decimal">
<li><p>Different values of the intercept, <span class="math inline">\(\alpha\)</span>, given the same <span class="math inline">\(\beta\)</span>, will affect the difference in finger tapping or response times for two adjacent trials (compare this with what happens with an additive model, such as when a normal likelihood is used); see Figure <a href="regression-models-with-brms---extended.html#fig:logexp">A.2</a>. This is because, unlike in the additive case, the intercept doesn’t cancel out:</p>
<ul>
<li><p>Additive case:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
&amp; (\alpha + trial_n \cdot \beta) - (\alpha + trial_{n-1} \cdot \beta) = \\
&amp;=\alpha -\alpha + ( trial_n - trial_{n-1} ) \cdot \beta\\
&amp;= ( trial_n - trial_{n-1} ) \cdot \beta
\end{aligned}
\end{equation}\]</span></p></li>
<li><p>Multiplicative case:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
   &amp;\exp(\alpha) \cdot \exp(trial_n \cdot \beta) -\exp(\alpha) \cdot \exp(trial_{n-1} \cdot \beta) =\\ 
   &amp;= \exp(\alpha) \big(\exp(trial_n  \cdot \beta)  - \exp(trial_{n-1}\cdot \beta) \big)\\
   &amp;\neq \big(\exp(trial_n)  - \exp(trial_{n-1})  \big) \cdot \exp(\beta) 
\end{aligned}
   \end{equation}\]</span></p></li>
</ul></li>
<li><p>As the trial number increases, the same value of <span class="math inline">\(\beta\)</span> will have a very different impact on the original scale of the dependent variable: Any (fixed) negative value for <span class="math inline">\(\beta\)</span> will lead to  exponential decay and any (fixed) positive value will lead to  exponential growth; see Figure <a href="regression-models-with-brms---extended.html#fig:expgd">A.3</a>.</p></li>
</ol>

<div class="figure"><span style="display:block;" id="fig:logexp"></span>
<img src="bayescogsci_files/figure-html/logexp-1.svg" alt="The fitted values of the difference in response time between two adjacent trials, when \(\beta=0.01\) and \(\alpha\) lies between 0.1 and 15. The graph shows how changes in the intercept lead to changes in the difference in response times between trials, even if \(\beta\) is fixed." width="672" />
<p class="caption">
FIGURE A.2: The fitted values of the difference in response time between two adjacent trials, when <span class="math inline">\(\beta=0.01\)</span> and <span class="math inline">\(\alpha\)</span> lies between 0.1 and 15. The graph shows how changes in the intercept lead to changes in the difference in response times between trials, even if <span class="math inline">\(\beta\)</span> is fixed.
</p>
</div>

<div class="figure"><span style="display:block;" id="fig:expgd"></span>
<img src="bayescogsci_files/figure-html/expgd-1.svg" alt="The fitted values of the dependent variable (response times in ms) as a function of trial number, when (A) \(\beta = -0.01\), exponential decay, and when (B) \(\beta =0.01\), exponential growth." width="672" />
<p class="caption">
FIGURE A.3: The fitted values of the dependent variable (response times in ms) as a function of trial number, when (A) <span class="math inline">\(\beta = -0.01\)</span>, exponential decay, and when (B) <span class="math inline">\(\beta =0.01\)</span>, exponential growth.
</p>
</div>
<p>Does exponential growth or decay make sense in this particular example? We need to consider that if they do make sense, they will be an approximation valid for a specific range of values, at some point we will expect a ceiling or a floor effect: response times cannot truly be 0 milliseconds, or take several minutes. However, in our specific model, exponential growth or decay <em>by trial</em> is probably a bad approximation: We will predict that our subject will take extremely long (if <span class="math inline">\(\beta &gt;0\)</span>) or extremely short (if <span class="math inline">\(\beta &lt;0\)</span>) time in pressing the space bar in a relatively low number of trials. This doesn’t mean that the likelihood is wrong by itself, but it does mean that at least we need to put a cap on the growth or decay of our experimental manipulation. We can do this if the exponential growth or decay is a function of, for example, log-transformed trial numbers:</p>
<p><span class="math display">\[\begin{equation}
t_n \sim \mathit{LogNormal}(\alpha + c\_\log\_trial_n \cdot \beta,\sigma)\\
\end{equation}\]</span></p>

<div class="figure"><span style="display:block;" id="fig:expgd2"></span>
<img src="bayescogsci_files/figure-html/expgd2-1.svg" alt="Fitted value of the dependent variable (times in ms) as function of the natural logarithm of the trial number, when (A) \(\beta=-0.01\), exponential decay, and when (B) \(\beta =.01\), exponential growth." width="672"  />
<p class="caption">
FIGURE A.4: Fitted value of the dependent variable (times in ms) as function of the natural logarithm of the trial number, when (A) <span class="math inline">\(\beta=-0.01\)</span>, exponential decay, and when (B) <span class="math inline">\(\beta =.01\)</span>, exponential growth.
</p>
</div>
<div id="log-normal-distributions-everywhere" class="section level3 hasAnchor" number="20.4.1">
<h3><span class="header-section-number">A.4.1</span> Log-normal distributions everywhere<a href="regression-models-with-brms---extended.html#log-normal-distributions-everywhere" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The normal distribution is most often assumed to describe the random variation that occurs in the data from many scientific disciplines. However, most measurements actually show skewed distributions. <span class="citation">Limpert, Stahel, and Abbt (<a href="#ref-limpertLognormalDistributionsSciences2001" role="doc-biblioref">2001</a>)</span> discuss the log-normal distribution in scientific disciplines and how diverse type of data, from lengths of latent periods of infectious diseases to distribution of mineral resources in the Earth’s crust, including even body height–the quintessential example of a normal distribution–closely fit the log-normal distribution.</p>
<p><span class="citation">Limpert, Stahel, and Abbt (<a href="#ref-limpertLognormalDistributionsSciences2001" role="doc-biblioref">2001</a>)</span> point out that because a random variable that results from multiplying many independent variables has an approximate log-normal distribution, the most basic indicator of the importance of the log-normal distribution may be very general: Chemistry and physics are fundamental in life, and the prevailing operation in the laws of these disciplines is multiplication rather than addition.</p>
<p>Furthermore, at many physiological and anatomical levels in the brain, the distribution of numerous parameters is in fact strongly skewed with a heavy tail, suggesting that skewed (typically log-normal) distributions are fundamental to structural and functional brain organization. This might be explained given that the majority of interactions in highly interconnected systems, especially in biological systems, are multiplicative and synergistic rather than additive <span class="citation">(Buzsáki and Mizuseki <a href="#ref-buzsakiLogdynamicBrainHow2014" role="doc-biblioref">2014</a>)</span>.</p>
<p>Does the log-normal distribution make sense for  response times?
It has been long noticed that the log-normal distribution often provides a good fit to response times
distributions <span class="citation">(Brée <a href="#ref-breeDistributionProblemsolvingTimes1975" role="doc-biblioref">1975</a>; Ulrich and Miller <a href="#ref-ulrichEffectsTruncationReaction1994" role="doc-biblioref">1994</a>)</span>.
One advantage of assuming log-normally distributed response times (but, in fact, this is true for many skewed distributions) is that it entails that the standard deviation of the response time distribution will increase with the mean, as has been observed in empirical distributions of response times <span class="citation">(Wagenmakers, Grasman, and Molenaar <a href="#ref-wagenmakersRelationMeanVariance2005" role="doc-biblioref">2005</a>)</span>. Interestingly, it turns out that log-normal response times are also easily generated by certain process models. <span class="citation">Ulrich and Miller (<a href="#ref-ulrichInformationProcessingModels1993" role="doc-biblioref">1993</a>)</span> show, for example, that models in which response times are determined by a series of processes cascading activation from an input level to an output level (usually passing through a number of intervening processing levels along the way) can generate log-normally distributed response times.</p>
</div>
</div>
<div id="app-priorR" class="section level2 hasAnchor" number="20.5">
<h2><span class="header-section-number">A.5</span> Prior predictive checks in R<a href="regression-models-with-brms---extended.html#app-priorR" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The following function is an edited version of the earlier <code>normal_predictive_distribution</code> from the online section <a href="regression-models-with-brms---extended.html#app-efficientpriorpd">A.1</a>, which was used in section <a href="ch-compbda.html#sec-priorpred">3.3</a>; it has been edited to make it compatible with logistic regression and dependent on set size.</p>
<p>As we did before, our custom function uses the <code>purrr</code> function <code>map2_dfr()</code>, which runs an efficient for-loop, iterating over two vectors (here <code>alpha_samples</code> and <code>beta_samples</code>), and builds a data frame with the output.</p>
<div class="sourceCode" id="cb1103"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1103-1"><a href="regression-models-with-brms---extended.html#cb1103-1" aria-hidden="true"></a>logistic_model_pred &lt;-<span class="st"> </span><span class="cf">function</span>(alpha_samples,</span>
<span id="cb1103-2"><a href="regression-models-with-brms---extended.html#cb1103-2" aria-hidden="true"></a>                                beta_samples,</span>
<span id="cb1103-3"><a href="regression-models-with-brms---extended.html#cb1103-3" aria-hidden="true"></a>                                set_size,</span>
<span id="cb1103-4"><a href="regression-models-with-brms---extended.html#cb1103-4" aria-hidden="true"></a>                                N_obs) {</span>
<span id="cb1103-5"><a href="regression-models-with-brms---extended.html#cb1103-5" aria-hidden="true"></a>  <span class="kw">map2_dfr</span>(alpha_samples, beta_samples,</span>
<span id="cb1103-6"><a href="regression-models-with-brms---extended.html#cb1103-6" aria-hidden="true"></a>           <span class="cf">function</span>(alpha, beta) {</span>
<span id="cb1103-7"><a href="regression-models-with-brms---extended.html#cb1103-7" aria-hidden="true"></a>             <span class="kw">tibble</span>(<span class="dt">set_size =</span> set_size,</span>
<span id="cb1103-8"><a href="regression-models-with-brms---extended.html#cb1103-8" aria-hidden="true"></a>                    <span class="co"># center size:</span></span>
<span id="cb1103-9"><a href="regression-models-with-brms---extended.html#cb1103-9" aria-hidden="true"></a>                    <span class="dt">c_set_size =</span> set_size <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(set_size),</span>
<span id="cb1103-10"><a href="regression-models-with-brms---extended.html#cb1103-10" aria-hidden="true"></a>                    <span class="co"># change the likelihood:</span></span>
<span id="cb1103-11"><a href="regression-models-with-brms---extended.html#cb1103-11" aria-hidden="true"></a>                    <span class="co"># Notice the use of a link function</span></span>
<span id="cb1103-12"><a href="regression-models-with-brms---extended.html#cb1103-12" aria-hidden="true"></a>                    <span class="co"># for alpha and beta</span></span>
<span id="cb1103-13"><a href="regression-models-with-brms---extended.html#cb1103-13" aria-hidden="true"></a>                    <span class="dt">theta =</span> <span class="kw">plogis</span>(alpha <span class="op">+</span><span class="st"> </span>c_set_size <span class="op">*</span><span class="st"> </span>beta),</span>
<span id="cb1103-14"><a href="regression-models-with-brms---extended.html#cb1103-14" aria-hidden="true"></a>                    <span class="dt">correct_pred =</span> <span class="kw">rbern</span>(N_obs, <span class="dt">prob =</span> theta))</span>
<span id="cb1103-15"><a href="regression-models-with-brms---extended.html#cb1103-15" aria-hidden="true"></a>           },</span>
<span id="cb1103-16"><a href="regression-models-with-brms---extended.html#cb1103-16" aria-hidden="true"></a>           <span class="dt">.id =</span> <span class="st">&quot;iter&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1103-17"><a href="regression-models-with-brms---extended.html#cb1103-17" aria-hidden="true"></a><span class="st">    </span><span class="co"># .id is always a string and needs</span></span>
<span id="cb1103-18"><a href="regression-models-with-brms---extended.html#cb1103-18" aria-hidden="true"></a><span class="st">    </span><span class="co"># to be converted to a number</span></span>
<span id="cb1103-19"><a href="regression-models-with-brms---extended.html#cb1103-19" aria-hidden="true"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">iter =</span> <span class="kw">as.numeric</span>(iter))</span>
<span id="cb1103-20"><a href="regression-models-with-brms---extended.html#cb1103-20" aria-hidden="true"></a>}</span></code></pre></div>
<p>Let’s assume 800 observations with 200 observation for each set size:</p>
<div class="sourceCode" id="cb1104"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1104-1"><a href="regression-models-with-brms---extended.html#cb1104-1" aria-hidden="true"></a>N_obs &lt;-<span class="st"> </span><span class="dv">800</span></span>
<span id="cb1104-2"><a href="regression-models-with-brms---extended.html#cb1104-2" aria-hidden="true"></a>set_size &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>), <span class="dv">200</span>)</span></code></pre></div>
<p>Now, iterate over plausible standard deviations of <span class="math inline">\(\beta\)</span> with the <code>purrr</code> function <code>map_dfr()</code>, which iterates over one vector (here <code>sds_beta</code>), and also builds a data frame with the output.</p>
<div class="sourceCode" id="cb1105"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1105-1"><a href="regression-models-with-brms---extended.html#cb1105-1" aria-hidden="true"></a>alpha_samples &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="fl">1.5</span>)</span>
<span id="cb1105-2"><a href="regression-models-with-brms---extended.html#cb1105-2" aria-hidden="true"></a>sds_beta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>)</span>
<span id="cb1105-3"><a href="regression-models-with-brms---extended.html#cb1105-3" aria-hidden="true"></a>prior_pred &lt;-<span class="st"> </span><span class="kw">map_dfr</span>(sds_beta, <span class="cf">function</span>(sd) {</span>
<span id="cb1105-4"><a href="regression-models-with-brms---extended.html#cb1105-4" aria-hidden="true"></a>  beta_samples &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, sd)</span>
<span id="cb1105-5"><a href="regression-models-with-brms---extended.html#cb1105-5" aria-hidden="true"></a>  <span class="kw">logistic_model_pred</span>(<span class="dt">alpha_samples =</span> alpha_samples,</span>
<span id="cb1105-6"><a href="regression-models-with-brms---extended.html#cb1105-6" aria-hidden="true"></a>                      <span class="dt">beta_samples =</span> beta_samples,</span>
<span id="cb1105-7"><a href="regression-models-with-brms---extended.html#cb1105-7" aria-hidden="true"></a>                      <span class="dt">set_size =</span> set_size,</span>
<span id="cb1105-8"><a href="regression-models-with-brms---extended.html#cb1105-8" aria-hidden="true"></a>                      <span class="dt">N_obs =</span> N_obs) <span class="op">%&gt;%</span></span>
<span id="cb1105-9"><a href="regression-models-with-brms---extended.html#cb1105-9" aria-hidden="true"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">prior_beta_sd =</span> sd)</span>
<span id="cb1105-10"><a href="regression-models-with-brms---extended.html#cb1105-10" aria-hidden="true"></a>})</span></code></pre></div>
<p>Calculate the accuracy for each one of the priors we want to examine, for each iteration, and for each set size.</p>
<div class="sourceCode" id="cb1106"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1106-1"><a href="regression-models-with-brms---extended.html#cb1106-1" aria-hidden="true"></a>mean_accuracy &lt;-</span>
<span id="cb1106-2"><a href="regression-models-with-brms---extended.html#cb1106-2" aria-hidden="true"></a><span class="st">  </span>prior_pred <span class="op">%&gt;%</span></span>
<span id="cb1106-3"><a href="regression-models-with-brms---extended.html#cb1106-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">group_by</span>(prior_beta_sd, iter, set_size) <span class="op">%&gt;%</span></span>
<span id="cb1106-4"><a href="regression-models-with-brms---extended.html#cb1106-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(correct_pred)) <span class="op">%&gt;%</span></span>
<span id="cb1106-5"><a href="regression-models-with-brms---extended.html#cb1106-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prior =</span> <span class="kw">paste0</span>(<span class="st">&quot;Normal(0, &quot;</span>, prior_beta_sd, <span class="st">&quot;)&quot;</span>))</span></code></pre></div>
<p>The plot of the accuracy is shown in Figure <a href="ch-reg.html#fig:priors4beta">4.13</a> of the book, and repeated here in Figure <a href="regression-models-with-brms---extended.html#fig:priors4betacode">A.5</a>.</p>

<div class="sourceCode" id="cb1107"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1107-1"><a href="regression-models-with-brms---extended.html#cb1107-1" aria-hidden="true"></a>mean_accuracy <span class="op">%&gt;%</span></span>
<span id="cb1107-2"><a href="regression-models-with-brms---extended.html#cb1107-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(accuracy)) <span class="op">+</span></span>
<span id="cb1107-3"><a href="regression-models-with-brms---extended.html#cb1107-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span></span>
<span id="cb1107-4"><a href="regression-models-with-brms---extended.html#cb1107-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">facet_grid</span>(set_size <span class="op">~</span><span class="st"> </span>prior) <span class="op">+</span></span>
<span id="cb1107-5"><a href="regression-models-with-brms---extended.html#cb1107-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">.5</span>, <span class="dv">1</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:priors4betacode"></span>
<img src="bayescogsci_files/figure-html/priors4betacode-1.svg" alt="The prior predictive distributions of mean accuracy of the model defined in section 4.3, for different set sizes and different priors for \(\beta\)." width="672" />
<p class="caption">
FIGURE A.5: The prior predictive distributions of mean accuracy of the model defined in section <a href="ch-reg.html#sec-logistic">4.3</a>, for different set sizes and different priors for <span class="math inline">\(\beta\)</span>.
</p>
</div>
<p>It’s sometimes more useful to look at the predicted differences in accuracy between set sizes. We calculate them as follows, and plot them in Figure <a href="ch-reg.html#fig:priors4beta2">4.14</a> of the book, repeated here in Figure <a href="regression-models-with-brms---extended.html#fig:priors4beta2code">A.6</a>.</p>
<div class="sourceCode" id="cb1108"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1108-1"><a href="regression-models-with-brms---extended.html#cb1108-1" aria-hidden="true"></a>diff_accuracy &lt;-<span class="st"> </span>mean_accuracy <span class="op">%&gt;%</span></span>
<span id="cb1108-2"><a href="regression-models-with-brms---extended.html#cb1108-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">arrange</span>(set_size) <span class="op">%&gt;%</span></span>
<span id="cb1108-3"><a href="regression-models-with-brms---extended.html#cb1108-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">group_by</span>(iter, prior_beta_sd) <span class="op">%&gt;%</span></span>
<span id="cb1108-4"><a href="regression-models-with-brms---extended.html#cb1108-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diff_accuracy =</span> accuracy <span class="op">-</span><span class="st"> </span><span class="kw">lag</span>(accuracy)) <span class="op">%&gt;%</span></span>
<span id="cb1108-5"><a href="regression-models-with-brms---extended.html#cb1108-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diffsize =</span> <span class="kw">paste</span>(set_size, <span class="st">&quot;-&quot;</span>, <span class="kw">lag</span>(set_size))) <span class="op">%&gt;%</span></span>
<span id="cb1108-6"><a href="regression-models-with-brms---extended.html#cb1108-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">filter</span>(set_size <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span>)</span></code></pre></div>

<div class="sourceCode" id="cb1109"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1109-1"><a href="regression-models-with-brms---extended.html#cb1109-1" aria-hidden="true"></a>diff_accuracy <span class="op">%&gt;%</span></span>
<span id="cb1109-2"><a href="regression-models-with-brms---extended.html#cb1109-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(diff_accuracy)) <span class="op">+</span></span>
<span id="cb1109-3"><a href="regression-models-with-brms---extended.html#cb1109-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span></span>
<span id="cb1109-4"><a href="regression-models-with-brms---extended.html#cb1109-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">facet_grid</span>(diffsize <span class="op">~</span><span class="st"> </span>prior) <span class="op">+</span></span>
<span id="cb1109-5"><a href="regression-models-with-brms---extended.html#cb1109-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="op">-</span>.<span class="dv">5</span>, <span class="dv">0</span>, <span class="fl">.5</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:priors4beta2code"></span>
<img src="bayescogsci_files/figure-html/priors4beta2code-1.svg" alt="The prior predictive distributions of differences in mean accuracy between set sizes of the model defined in section 4.3 for different priors for \(\beta\)." width="672" />
<p class="caption">
FIGURE A.6: The prior predictive distributions of differences in mean accuracy between set sizes of the model defined in section <a href="ch-reg.html#sec-logistic">4.3</a> for different priors for <span class="math inline">\(\beta\)</span>.
</p>
</div>
</div>
<div id="app-exch" class="section level2 hasAnchor" number="20.6">
<h2><span class="header-section-number">A.6</span> Finitely exchangeable random variables<a href="regression-models-with-brms---extended.html#app-exch" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Formally, we say that the random variables <span class="math inline">\(Y_1,\dots,Y_N\)</span> are finitely exchangeable if, for any set of particular outcomes of an experiment <span class="math inline">\(y_1,\dots,y_N\)</span>, the probability <span class="math inline">\(p(y_1,\dots,y_N)\)</span> that we assign to these outcomes is unaffected by permuting the labels given to the variables. In other words, for any permutation <span class="math inline">\(\pi(n)\)</span>, where <span class="math inline">\(n=1,\dots,N\)</span> (<span class="math inline">\(\pi\)</span> is a function that takes as input the positive integer <span class="math inline">\(n\)</span> and returns another positive integer; e.g., the function takes a subject indexed as 1, and returns index 3), we can reasonably assume that <span class="math inline">\(p(y_1,\dots,y_N)=p(y_{\pi(1)},\dots,y_{\pi(N)})\)</span>. A simple example is a coin tossed twice. Suppose the first coin toss is <span class="math inline">\(Y_1=1\)</span>, a heads, and the second coin toss is <span class="math inline">\(Y_2=0\)</span>, a tails. If we are willing to assume that the probability of getting one heads is unaffected by whether it appears in the first or the second toss, i.e., <span class="math inline">\(p(Y_1=1,Y_2=0)=p(Y_1=0,Y_2=1)\)</span>, then we assume that the indices are exchangeable.</p>
<p>Some important connections and differences between exchangeability and the frequentist concept of  independent and identically distributed (iid):</p>
<ul>
<li><p><strong>If the data are  exchangeable, they are not necessarily iid</strong>. For example, suppose you have a box with one black ball and two red balls in it. Your task is to repeatedly draw (without replacement) a ball at random. Suppose that in your first draw, you draw one ball and get the black ball. The probability of getting a black ball in the next two draws is now <span class="math inline">\(0\)</span>. However, if in your first draw you had retrieved a red ball, then there is a non-zero probability of drawing a black ball in the next two draws. The outcome in the first draw affects the probability of subsequent draws–they are not independent. But the sequence of random variables is exchangeable. To see this, consider the following: If a red ball is drawn, count it as a <span class="math inline">\(0\)</span>, and if a black ball is drawn, then count it as <span class="math inline">\(1\)</span>. Then, the three possible outcomes and the probabilities are</p>
<ul>
<li><span class="math inline">\(1,0,0\)</span>; <span class="math inline">\(P(X_1=1,X_2=0,X_3=0) = \frac{1}{3} \times 1 \times 1=\frac{1}{3}\)</span></li>
<li><span class="math inline">\(0,1,0\)</span> <span class="math inline">\(P(X_1=0,X_2=1,X_3=0) = \frac{2}{3} \times \frac{1}{2} \times 1=\frac{1}{3}\)</span></li>
<li><span class="math inline">\(0,0,1\)</span> <span class="math inline">\(P(X_1=0,X_2=0,X_3=1) = \frac{2}{3} \times \frac{1}{2} \times 1=\frac{1}{3}\)</span></li>
</ul>
<p>The random variables <span class="math inline">\(X_1,X_2,X_3\)</span> can be permuted and the joint probability distribution (technically, the PMF) is the same in each case.</p></li>
<li><p><strong>If the data are exchangeable, then they are identically distributed</strong>. For example, in the box containing one black ball and two red balls, suppose we count the draw of a black ball as a <span class="math inline">\(1\)</span>, and the draw of a red ball as a <span class="math inline">\(0\)</span>. Then the probability <span class="math inline">\(P(X_1=1)=\frac{1}{3}\)</span> and <span class="math inline">\(P(X_1=0)=\frac{2}{3}\)</span>; this is also true for <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span>. That is, these random variables are identically distributed.</p></li>
<li><p><strong>If the data are iid in the standard frequentist sense, then they are exchangeable</strong>. For example, suppose you have <span class="math inline">\(i=1,\dots,n\)</span> instances of a random variable <span class="math inline">\(X\)</span> whose PDF is <span class="math inline">\(f(x)\)</span>. Suppose also that <span class="math inline">\(X_i\)</span> are iid. The joint PDF (this can be discrete or continuous, i.e., a PMF or PDF) is</p>
<p><span class="math display">\[\begin{equation}
 f_{X_1,\dots,X_n}(x_1,\dots,x_n) = f(x_1) \cdot \dots \cdot f(x_n)
 \end{equation}\]</span></p>
<p>Because the terms on the right-hand side can be permuted, the labels can be permuted on any of the <span class="math inline">\(x_i\)</span>. This means that <span class="math inline">\(X_1,\dots,X_n\)</span> are exchangeable.</p></li>
</ul>
</div>
<div id="app-matrixHierachicalModel" class="section level2 hasAnchor" number="20.7">
<h2><span class="header-section-number">A.7</span> The Matrix Formulation of Hierarchical Models (the Laird-Ware form)<a href="regression-models-with-brms---extended.html#app-matrixHierachicalModel" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the book, we generally write linear models as follows; where <span class="math inline">\(n\)</span> refers to the row id in the data frame.</p>
<p><span class="math display">\[\begin{equation}
y_n \sim \mathit{Normal}(\alpha + \beta\cdot x_n)
\end{equation}\]</span></p>
<p>This simple linear model can be re-written as follows:</p>
<p><span class="math display">\[\begin{equation}
y_n = \alpha + \beta\cdot x_n + \varepsilon_n
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\varepsilon_n \sim \mathit{Normal}(0, \sigma)\)</span>.</p>
<p>The model does not change if <span class="math inline">\(\alpha\)</span> is multiplied by <span class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[\begin{equation}
y_n = \alpha\cdot 1 + \beta\cdot x_n + \varepsilon_n
\end{equation}\]</span></p>
<p>The above is actually <span class="math inline">\(n\)</span> linear equations, and can be written compactly in matrix form:</p>
<p><span class="math display">\[\begin{equation}
{\begin{pmatrix}
    y_1\\
    y_2\\
    \vdots \\
    y_n\\
   \end{pmatrix}}
 =
 {\begin{pmatrix}
    1 &amp; x_1\\
    1 &amp; x_2\\
    \vdots &amp; \vdots \\
    1 &amp; x_n\\
   \end{pmatrix}}
{\begin{pmatrix}
    \alpha\\
    \beta \\
   \end{pmatrix}}
+
 {\begin{pmatrix}
    \varepsilon_1 \\
    \varepsilon_2 \\
    \vdots \\
     \varepsilon_n \\
   \end{pmatrix}}
\end{equation}\]</span></p>
<p>Consider this matrix in the above equation:</p>
<p><span class="math display">\[\begin{equation}
{\begin{pmatrix}
    1 &amp; x_1\\
    1 &amp; x_2\\
    \vdots &amp; \vdots \\
    1 &amp; x_n\\
   \end{pmatrix}}
\end{equation}\]</span></p>
<p>This matrix is called the model matrix or the design matrix; we will encounter it again in the contrast coding chapters, where it plays a crucial role. If we write the dependent variable <span class="math inline">\(y\)</span> as a <span class="math inline">\(n\times 1\)</span> vector, the above matrix as the matrix <span class="math inline">\(X\)</span> (which has dimensions <span class="math inline">\(n \times 2\)</span>), the intercept and slope parameters as a <span class="math inline">\(2\times 1\)</span> matrix <span class="math inline">\(\zeta\)</span>, and the residual errors as an <span class="math inline">\(n\times 1\)</span> matrix, we can write the linear model very compactly:</p>
<p><span class="math display">\[\begin{equation}
y =  X \zeta + \varepsilon
\end{equation}\]</span></p>
<p>The above matrix formulation of the linear model extends to the hierarchical model very straightforwardly. For example,
consider the by-subjects and by-items correlated varying intercept varying slopes model <span class="math inline">\(M_{sih}\)</span> that we saw in section <a href="ch-hierarchical.html#sec-sih">5.2.5</a>. This model has the following likelihood:</p>
<p><span class="math display">\[\begin{multline}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + w_{item[n],1} \\
  + c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2}), \sigma)
  \end{multline}\]</span></p>
<p>The terms in the location parameter in the normal likelihood can be re-written in matrix form, just like the linear model above. To see this, consider the fact that the location term</p>
<p><span class="math display">\[\begin{equation}
\alpha + u_{subj[n],1} + w_{item[n],1} + c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2})
\end{equation}\]</span></p>
<p>can be re-written as</p>
<p><span class="math display">\[\begin{multline}
\alpha\cdot 1 + u_{subj[n],1}\cdot 1 + w_{item[n],1}\cdot 1 +\\
\beta \cdot c\_cloze_n + u_{subj[n],2}\cdot c\_cloze_n+ w_{item[n],2}\cdot c\_cloze_n
\end{multline}\]</span></p>
<p>The above equation can in turn be written in matrix form as follows. The symbol <span class="math inline">\(\odot\)</span> is the Hadamard product: this is cell-wise multiplication rather than matrix multiplication.<a href="#fn71" class="footnote-ref" id="fnref71"><sup>71</sup></a></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
&amp;
{\begin{pmatrix}
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}
{\begin{pmatrix}
    \alpha\\
    \beta \\
   \end{pmatrix}}
+ \\
&amp; {\begin{pmatrix}
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}
   \odot
   {\begin{pmatrix}
    u_{subj[1],1}  &amp;  u_{subj[1],2} \\
    u_{subj[2],1} &amp; u_{subj[2],2} \\
     \vdots &amp; \vdots\\
     u_{subj[n],1} &amp; u_{subj[n],2}\\
   \end{pmatrix}}
   +\\
&amp; {\begin{pmatrix}
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}
   \odot
   {\begin{pmatrix}
    w_{item[1],1} &amp; w_{item[1],2}\\
    w_{item[2],1} &amp; w_{item[2],2} \\
    \vdots &amp; \vdots \\
    w_{item[n],1}  &amp; w_{item[n],2} \\
   \end{pmatrix}}
 \end{aligned}
\end{equation}\]</span></p>
<p>In this hierarchical model, there are three model matrices:</p>
<ul>
<li>the model matrix associated with the intercept <span class="math inline">\(\alpha\)</span> and the slope <span class="math inline">\(\beta\)</span>; below, we call this the matrix <span class="math inline">\(X\)</span>.</li>
<li>the model matrix associated with the by-subject varying intercepts and slopes; call this the matrix <span class="math inline">\(Z_u\)</span>.</li>
<li>the model matrix associated with the by-item varying intercepts and slopes; call this the matrix <span class="math inline">\(Z_w\)</span>.</li>
</ul>
<p>The model can now be written very compactly in matrix form by writing these three matrices as follows:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
X = &amp; {\begin{pmatrix}
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}\\
   Z_u = &amp;
   {\begin{pmatrix}
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}} \\
   Z_w = &amp;
   {\begin{pmatrix}
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}} \\
\end{aligned}
\end{equation}\]</span></p>
<p>The location part of the model <span class="math inline">\(M_{sih}\)</span> can now be written very compactly:</p>
<p><span class="math display">\[\begin{equation}
X \zeta
+
Z_u \odot z_u
+
Z_w \odot z_w
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\zeta\)</span> is a <span class="math inline">\(2\times 1\)</span> matrix containing the intercept <span class="math inline">\(\alpha\)</span> and the slope <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(z_u\)</span> and <span class="math inline">\(z_w\)</span> are the intercept and slope adjustments by subject and by item:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
z_u = &amp;
 {\begin{pmatrix}
    u_{subj[1],1}  &amp;  u_{subj[1],2} \\
    u_{subj[2],1} &amp; u_{subj[2],2} \\
     \vdots &amp; \vdots\\
     u_{subj[n],1} &amp; u_{subj[n],2}\\
   \end{pmatrix}}\\
 z_w = &amp;
    {\begin{pmatrix}
    w_{item[1],1} &amp; w_{item[1],2}\\
    w_{item[2],1} &amp; w_{item[2],2} \\
    \vdots &amp; \vdots \\
    w_{item[n],1}  &amp; w_{item[n],2} \\
   \end{pmatrix}}\\
\end{aligned}
\end{equation}\]</span></p>
<p>In summary, the hierarchical model has a very general matrix formulation <span class="citation">(cf. Laird and Ware <a href="#ref-laird1982random" role="doc-biblioref">1982</a>)</span>:</p>
<p><span class="math display">\[\begin{equation}
 signal = X \zeta
+
Z_u \odot z_u
+
Z_w \odot z_w +
\varepsilon
 \end{equation}\]</span></p>
<p>The practical relevance of this matrix formulation is that we can define hierarchical models very compactly and efficiently in Stan by expressing the model in terms of the model matrices <span class="citation">(Sorensen, Hohenstein, and Vasishth <a href="#ref-SorensenVasishthTutorial" role="doc-biblioref">2016</a>)</span>. As an aside, notice that in the above example, <span class="math inline">\(X=Z_u=Z_w\)</span>; but in principle one could have different model matrices for the fixed vs. random effects.</p>
</div>
<div id="app-cTreatGM" class="section level2 hasAnchor" number="20.8">
<h2><span class="header-section-number">A.8</span> Treatment contrast with intercept as the grand mean<a href="regression-models-with-brms---extended.html#app-cTreatGM" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In chapter <a href="ch-contr.html#ch-contr">6</a>, we have introduced the treatment contrast, where each contrast compares one condition to a baseline condition. We have discussed that the intercept in the treatment contrast estimates the condition mean for the  baseline condition. There are some applications where this behavior may seem sub-optimal. This can be the case in experimental designs with multiple factors, where we may want to use  centered contrasts (this is discussed in chapter <a href="ch-coding2x2.html#ch-coding2x2">7</a>). Moreover, the contrast coding of the population-level (or fixed) effects also defines what the group-level (or random) effects assess. If the intercept assesses the grand mean–rather than the baseline condition–in hierarchical models, then the group-level intercepts reflect the grand mean variance, rather than the variance in the baseline condition.</p>
<p>It is possible to design a treatment contrast where the intercept reflects the grand mean (assuming a balanced design; otherwise it’s the unweighted grand mean). We implement this using the <code>hypr</code> package. The trick is to add the intercept explicitly as a comparison of the average of all four condition means:</p>
<div class="sourceCode" id="cb1110"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1110-1"><a href="regression-models-with-brms---extended.html#cb1110-1" aria-hidden="true"></a>HcTrGM &lt;-<span class="st"> </span><span class="kw">hypr</span>(<span class="dt">b0 =</span> <span class="op">~</span><span class="st"> </span>(F1 <span class="op">+</span><span class="st"> </span>F2 <span class="op">+</span><span class="st"> </span>F3 <span class="op">+</span><span class="st"> </span>F4) <span class="op">/</span><span class="st"> </span><span class="dv">4</span>,</span>
<span id="cb1110-2"><a href="regression-models-with-brms---extended.html#cb1110-2" aria-hidden="true"></a>               <span class="dt">b1 =</span> F2 <span class="op">~</span><span class="st"> </span>F1,</span>
<span id="cb1110-3"><a href="regression-models-with-brms---extended.html#cb1110-3" aria-hidden="true"></a>               <span class="dt">b2 =</span> F3 <span class="op">~</span><span class="st"> </span>F1,</span>
<span id="cb1110-4"><a href="regression-models-with-brms---extended.html#cb1110-4" aria-hidden="true"></a>               <span class="dt">b3 =</span> F4 <span class="op">~</span><span class="st"> </span>F1,</span>
<span id="cb1110-5"><a href="regression-models-with-brms---extended.html#cb1110-5" aria-hidden="true"></a>               <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;F1&quot;</span>, <span class="st">&quot;F2&quot;</span>, <span class="st">&quot;F3&quot;</span>, <span class="st">&quot;F4&quot;</span>))</span>
<span id="cb1110-6"><a href="regression-models-with-brms---extended.html#cb1110-6" aria-hidden="true"></a>HcTrGM</span></code></pre></div>
<pre><code>## hypr object containing 4 null hypotheses:
## H0.b0: 0 = (F1 + F2 + F3 + F4)/4  (Intercept)
## H0.b1: 0 = F2 - F1
## H0.b2: 0 = F3 - F1
## H0.b3: 0 = F4 - F1
## 
## Call:
## hypr(b0 = ~1/4 * F1 + 1/4 * F2 + 1/4 * F3 + 1/4 * F4, b1 = ~F2 - 
##     F1, b2 = ~F3 - F1, b3 = ~F4 - F1, levels = c(&quot;F1&quot;, &quot;F2&quot;, 
## &quot;F3&quot;, &quot;F4&quot;))
## 
## Hypothesis matrix (transposed):
##    b0  b1  b2  b3 
## F1 1/4  -1  -1  -1
## F2 1/4   1   0   0
## F3 1/4   0   1   0
## F4 1/4   0   0   1
## 
## Contrast matrix:
##    b0   b1   b2   b3  
## F1    1 -1/4 -1/4 -1/4
## F2    1  3/4 -1/4 -1/4
## F3    1 -1/4  3/4 -1/4
## F4    1 -1/4 -1/4  3/4</code></pre>
<p>The hypothesis matrix now explicitly codes the intercept as the first column, where all hypothesis weights are equal and sum up to one. This is coding the intercept. The other hypothesis weights are as expected for the treatment contrast. The contrast matrix now looks very different compared to the standard treatment contrast. Next, we fit a model with this adapted treatment contrast. The function  <code>contr.hypothesis</code> automatically removes the intercept that is encoded in <code>HcTrGM</code>, since this is automatically added by <code>brms</code>.</p>
<div class="sourceCode" id="cb1112"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1112-1"><a href="regression-models-with-brms---extended.html#cb1112-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcTrGM)</span>
<span id="cb1112-2"><a href="regression-models-with-brms---extended.html#cb1112-2" aria-hidden="true"></a>fit_TrGM &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span>F,</span>
<span id="cb1112-3"><a href="regression-models-with-brms---extended.html#cb1112-3" aria-hidden="true"></a>                <span class="dt">data =</span> df_contrasts3,</span>
<span id="cb1112-4"><a href="regression-models-with-brms---extended.html#cb1112-4" aria-hidden="true"></a>                <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb1112-5"><a href="regression-models-with-brms---extended.html#cb1112-5" aria-hidden="true"></a>                <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb1112-6"><a href="regression-models-with-brms---extended.html#cb1112-6" aria-hidden="true"></a>                          <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb1112-7"><a href="regression-models-with-brms---extended.html#cb1112-7" aria-hidden="true"></a>                          <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb1113"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1113-1"><a href="regression-models-with-brms---extended.html#cb1113-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_TrGM)</span></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5 Q97.5
## Intercept    20.01      2.38  15.07  24.7
## Fb1           9.48      6.75  -4.47  22.3
## Fb2          -0.42      6.75 -14.04  12.8
## Fb3          29.25      6.81  15.41  42.4</code></pre>
<p>The results show that the coefficients reflect comparisons of each condition <span class="math inline">\(F2\)</span>, F3, and F4 to the baseline condition <span class="math inline">\(F1\)</span>. The intercept now captures the grand mean across all four conditions of <span class="math inline">\(20\)</span>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references hanging-indent">
<div id="ref-breeDistributionProblemsolvingTimes1975">
<p>Brée, David S. 1975. “The Distribution of Problem-Solving Times: An Examination of the Stages Model.” <em>British Journal of Mathematical and Statistical Psychology</em> 28 (2): 177–200. <a href="https://doi.org/10/cnx3q7">https://doi.org/10/cnx3q7</a>.</p>
</div>
<div id="ref-buzsakiLogdynamicBrainHow2014">
<p>Buzsáki, György, and Kenji Mizuseki. 2014. “The Log-Dynamic Brain: How Skewed Distributions Affect Network Operations.” <em>Nature Reviews Neuroscience</em> 15 (4): 264–78. <a href="https://doi.org/10.1038/nrn3687">https://doi.org/10.1038/nrn3687</a>.</p>
</div>
<div id="ref-johnson1995continuous">
<p>Johnson, Norman L., Samuel Kotz, and Narayanaswamy Balakrishnan. 1995. <em>Continuous Univariate Distributions, Volume 2</em>. Vol. 289. John Wiley; Sons.</p>
</div>
<div id="ref-laird1982random">
<p>Laird, Nan M., and James H. Ware. 1982. “Random-Effects Models for Longitudinal Data.” <em>Biometrics</em>, 963–74. <a href="https://doi.org/https://doi.org/10.2307/2529876">https://doi.org/https://doi.org/10.2307/2529876</a>.</p>
</div>
<div id="ref-limpertLognormalDistributionsSciences2001">
<p>Limpert, Eckhard, Werner A. Stahel, and Markus Abbt. 2001. “Log-Normal Distributions Across the Sciences: Keys and Clues.” <em>BioScience</em> 51 (5): 341. <a href="https://doi.org/10.1641/0006-3568(2001)051%5B0341:LNDATS%5D2.0.CO;2">https://doi.org/10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2</a>.</p>
</div>
<div id="ref-SorensenVasishthTutorial">
<p>Sorensen, Tanner, Sven Hohenstein, and Shravan Vasishth. 2016. “Bayesian Linear Mixed Models Using Stan: A Tutorial for Psychologists, Linguists, and Cognitive Scientists.” <em>Quantitative Methods for Psychology</em> 12 (3): 175–200.</p>
</div>
<div id="ref-ulrichInformationProcessingModels1993">
<p>Ulrich, Rolf, and Jeff Miller. 1993. “Information Processing Models Generating Lognormally Distributed Reaction Times.” <em>Journal of Mathematical Psychology</em> 37 (4): 513–25. <a href="https://doi.org/10.1006/jmps.1993.1032">https://doi.org/10.1006/jmps.1993.1032</a>.</p>
</div>
<div id="ref-ulrichEffectsTruncationReaction1994">
<p>Ulrich, Rolf, and Jeff Miller. 1994. “Effects of Truncation on Reaction Time Analysis.” <em>Journal of Experimental Psychology: General</em> 123 (1): 34–80. <a href="https://doi.org/10/b8tsnh">https://doi.org/10/b8tsnh</a>.</p>
</div>
<div id="ref-wagenmakersRelationMeanVariance2005">
<p>Wagenmakers, Eric-Jan, Raoul P. P. P. Grasman, and Peter C. M. Molenaar. 2005. “On the Relation Between the Mean and the Variance of a Diffusion Model Response Time Distribution.” <em>Journal of Mathematical Psychology</em> 49 (3): 195–204. <a href="https://doi.org/10.1016/j.jmp.2005.02.003">https://doi.org/10.1016/j.jmp.2005.02.003</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="70">
<li id="fn70"><p>These transformations are visible when checking the generated Stan code using <code>make_stancode()</code>.<a href="regression-models-with-brms---extended.html#fnref70" class="footnote-back">↩︎</a></p></li>
<li id="fn71"><p>This means that if we have two matrices <span class="math inline">\(A_{i,j}\)</span> and <span class="math inline">\(B_{i,j}\)</span>, the Hadamard product produces a matrix <span class="math inline">\(C_{i,j}\)</span> that is the result of multiplying each cell <span class="math inline">\(A_{i,j}\)</span> with <span class="math inline">\(B_{i,j}\)</span>, for all row and column ids <span class="math inline">\(i,j\)</span> respectively.<a href="regression-models-with-brms---extended.html#fnref71" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-closing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advanced-models-with-stan---extended.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
