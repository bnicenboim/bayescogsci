<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Contrast coding | Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Contrast coding | Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://bruno.nicenboim.me/bayescogsci/images/temporarycover.jpg" />
  <meta property="og:description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/bnicenboim/bayescogsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Contrast coding | Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bruno.nicenboim.me/bayescogsci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel J. Schad, and Shravan Vasishth" />


<meta name="date" content="2025-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-hierarchical.html"/>
<link rel="next" href="ch-coding2x2.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<script data-goatcounter="https://bayescogsci.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b>  Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The  law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b>  Discrete random variables: An example using the  binomial distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b>  Continuous random variables: An example using the  normal distribution</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1:  Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (or multivariate) data</a></li>
<li class="chapter" data-level="1.6.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-decomposevcovmatrix"><i class="fa fa-check"></i><b>1.6.4</b> Decomposing a variance-covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-some-useful-r-functions"><i class="fa fa-check"></i><b>1.8</b> Summary of some useful R functions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b>  Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the  posterior using Bayes’ rule: An analytical example</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a  likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a  prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using  Bayes’ rule to compute the  posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The  posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the  posterior through  sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b>  Bayesian Regression Models using Stan:  brms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors:  sensitivity analysis</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b>  Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b>  Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b>  Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b>  Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b>  Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The  log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first  linear regression: Does attentional load affect pupil size?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b>  Likelihood and  priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b>  Log-normal model: Does trial affect response times?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b>  Logistic regression: Does  set size affect  free recall?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b>  Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b>  Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b>  No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b>  Varying intercepts and  varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A  hierarchical log-normal model: The  Stroop effect</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes  log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>6.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>6.1.1</b> Default contrast coding:  Treatment contrasts</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>6.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>6.1.3</b>  Sum contrasts</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>6.1.4</b>  Cell means parameterization and  posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>6.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>6.2.1</b>  Sum contrasts</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>6.2.2</b> The  hypothesis matrix</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>6.2.3</b> Generating contrasts: The  <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>6.3</b> Other types of contrasts: illustration with a factor of four levels</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>6.3.1</b>  Repeated contrasts</a></li>
<li class="chapter" data-level="6.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>6.3.2</b>  Helmert contrasts</a></li>
<li class="chapter" data-level="6.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>6.3.3</b> Contrasts in linear regression analysis: The design or  model matrix</a></li>
<li class="chapter" data-level="6.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>6.3.4</b>  Polynomial contrasts</a></li>
<li class="chapter" data-level="6.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>6.3.5</b> An alternative to contrasts:  Monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>6.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>6.4.1</b>  Centered contrasts</a></li>
<li class="chapter" data-level="6.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>6.4.2</b>  Orthogonal contrasts</a></li>
<li class="chapter" data-level="6.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> The role of the  intercept in  non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>6.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="6.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-5"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
<li class="chapter" data-level="6.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-3"><i class="fa fa-check"></i><b>6.7</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>7</b> Contrast coding with two predictor variables</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>7.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>7.1.1</b>  Nested effects</a></li>
<li class="chapter" data-level="7.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>7.1.2</b>  Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>7.2</b> One factor and one  covariate</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>7.2.1</b> Estimating a  group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>7.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>7.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="7.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="8" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>8</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>8.1</b> Stan syntax</a></li>
<li class="chapter" data-level="8.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>8.2</b> A first simple example with Stan:  Normal likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>8.3</b> Another simple example:  Cloze probability with Stan with the  binomial likelihood</a></li>
<li class="chapter" data-level="8.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>8.4</b>  Regression models in Stan</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>8.4.1</b> A first  linear regression in Stan: Does attentional load affect  pupil size?</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>8.4.2</b>  Interactions in Stan: Does attentional load interact with trial number affecting  pupil size?</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>8.4.3</b>  Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-7"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-5"><i class="fa fa-check"></i><b>8.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>9</b> Hierarchical models and reparameterization </a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>9.1</b> Hierarchical models with Stan</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>9.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>9.1.2</b> Uncorrelated  varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="9.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>9.1.3</b>  Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="9.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>9.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-8"><i class="fa fa-check"></i><b>9.2</b> Summary</a></li>
<li class="chapter" data-level="9.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-6"><i class="fa fa-check"></i><b>9.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>10</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>10.1</b> A change of variables with the reciprocal normal distribution</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>10.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>10.2</b>  Validation of a computed posterior distribution</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>10.2.1</b> The  simulation-based calibration procedure</a></li>
<li class="chapter" data-level="10.2.2" data-path="ch-custom.html"><a href="ch-custom.html#an-example-where-simulation-based-calibration-reveals-a-problem"><i class="fa fa-check"></i><b>10.2.2</b> An example where simulation-based calibration reveals a problem</a></li>
<li class="chapter" data-level="10.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-with-and-limitations-of-simulation-based-calibration"><i class="fa fa-check"></i><b>10.2.3</b> Issues with and limitations of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-the-exponential-distribution-implemented-manually"><i class="fa fa-check"></i><b>10.3</b> Another  custom distribution: The exponential distribution  implemented manually</a></li>
<li class="chapter" data-level="10.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-9"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
<li class="chapter" data-level="10.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-7"><i class="fa fa-check"></i><b>10.5</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="11" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>11</b>  Meta-analysis and  measurement error models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>11.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>11.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>11.2</b>  Measurement-error models</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>11.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-10"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-8"><i class="fa fa-check"></i><b>11.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison</b></span></li>
<li class="chapter" data-level="12" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>12</b> Introduction to model comparison</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-comparison.html"><a href="ch-comparison.html#prior-predictive-vs.-posterior-predictive-model-comparison"><i class="fa fa-check"></i><b>12.1</b> Prior predictive vs. posterior predictive model comparison</a></li>
<li class="chapter" data-level="12.2" data-path="ch-comparison.html"><a href="ch-comparison.html#some-important-points-to-consider-when-comparing-models"><i class="fa fa-check"></i><b>12.2</b> Some important points to consider when comparing models</a></li>
<li class="chapter" data-level="12.3" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-9"><i class="fa fa-check"></i><b>12.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>13</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>13.1</b> Hypothesis testing using the Bayes factor</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>13.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="13.1.2" data-path="ch-bf.html"><a href="ch-bf.html#the-bayes-factor"><i class="fa fa-check"></i><b>13.1.2</b> The Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>13.2</b> Examining the N400 effect with the Bayes factor</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>13.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="13.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>13.2.2</b>  Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>13.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="13.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>13.4</b>  The Bayes factor in Stan</a></li>
<li class="chapter" data-level="13.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>13.5</b> Bayes factors in theory and in practice</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>13.5.1</b> Bayes factors in theory: Stability and  accuracy</a></li>
<li class="chapter" data-level="13.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>13.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="13.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>13.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="ch-bf.html"><a href="ch-bf.html#sample-size-determination-using-bayes-factors"><i class="fa fa-check"></i><b>13.6</b> Sample size determination using Bayes factors</a></li>
<li class="chapter" data-level="13.7" data-path="ch-bf.html"><a href="ch-bf.html#summary-11"><i class="fa fa-check"></i><b>13.7</b> Summary</a></li>
<li class="chapter" data-level="13.8" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-10"><i class="fa fa-check"></i><b>13.8</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>14</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>14.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="14.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>14.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="14.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>14.3</b> Testing the N400 effect using cross-validation</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>14.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="14.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>14.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="14.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>14.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>14.4</b>  Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="14.5" data-path="ch-cv.html"><a href="ch-cv.html#sec-issuesCV"><i class="fa fa-check"></i><b>14.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="14.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>14.6</b> Cross-validation in Stan</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>14.6.1</b>  PSIS-LOO-CV in Stan</a></li>
<li class="chapter" data-level="14.6.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-cv-in-stan"><i class="fa fa-check"></i><b>14.6.2</b>  K-fold-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-12"><i class="fa fa-check"></i><b>14.7</b> Summary</a></li>
<li class="chapter" data-level="14.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-11"><i class="fa fa-check"></i><b>14.8</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>VI Cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="15" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>15</b> Introduction to cognitive modeling</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#what-characterizes-a-computational-cognitive-model"><i class="fa fa-check"></i><b>15.1</b> What characterizes a computational cognitive model?</a></li>
<li class="chapter" data-level="15.2" data-path="ch-cogmod.html"><a href="ch-cogmod.html#some-advantages-of-taking-the-latent-variable-modeling-approach"><i class="fa fa-check"></i><b>15.2</b> Some advantages of taking the latent-variable modeling approach</a></li>
<li class="chapter" data-level="15.3" data-path="ch-cogmod.html"><a href="ch-cogmod.html#types-of-computational-cognitive-model"><i class="fa fa-check"></i><b>15.3</b> Types of computational cognitive model</a></li>
<li class="chapter" data-level="15.4" data-path="ch-cogmod.html"><a href="ch-cogmod.html#summary-13"><i class="fa fa-check"></i><b>15.4</b> Summary</a></li>
<li class="chapter" data-level="15.5" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-12"><i class="fa fa-check"></i><b>15.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>16</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>16.1</b> Modeling  multiple categorical responses</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>16.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="16.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>16.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>16.2</b> Modeling picture naming abilities in aphasia with MPT models</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>16.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="16.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>16.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="16.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>16.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="16.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>16.2.4</b> A  hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-14"><i class="fa fa-check"></i><b>16.3</b> Summary</a></li>
<li class="chapter" data-level="16.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-13"><i class="fa fa-check"></i><b>16.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>17</b> Mixture models</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>17.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>17.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="17.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>17.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="17.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>17.1.3</b> A  multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="17.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>17.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="17.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>17.1.5</b> A  hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-15"><i class="fa fa-check"></i><b>17.2</b> Summary</a></li>
<li class="chapter" data-level="17.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-14"><i class="fa fa-check"></i><b>17.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>18</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>18.1</b> Modeling a lexical decision task</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>18.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>18.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="18.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>18.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="18.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>18.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="18.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>18.1.5</b> Dealing with  contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>18.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="18.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-16"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-closing.html"><a href="ch-closing.html"><i class="fa fa-check"></i><b>19</b> In closing</a></li>
<li class="appendix"><span><b>Online materials</b></span></li>
<li class="chapter" data-level="A" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html"><i class="fa fa-check"></i><b>A</b> Regression models with <code>brms</code> - Extended</a>
<ul>
<li class="chapter" data-level="A.1" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-efficientpriorpd"><i class="fa fa-check"></i><b>A.1</b> An efficient function for generating prior predictive distributions in R</a></li>
<li class="chapter" data-level="A.2" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-truncation"><i class="fa fa-check"></i><b>A.2</b> Truncated distributions</a></li>
<li class="chapter" data-level="A.3" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-intercept"><i class="fa fa-check"></i><b>A.3</b> Intercepts in <code>brms</code></a></li>
<li class="chapter" data-level="A.4" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-lognormal"><i class="fa fa-check"></i><b>A.4</b> Understanding the log-normal likelihood</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#log-normal-distributions-everywhere"><i class="fa fa-check"></i><b>A.4.1</b> Log-normal distributions everywhere</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-priorR"><i class="fa fa-check"></i><b>A.5</b> Prior predictive checks in R</a></li>
<li class="chapter" data-level="A.6" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-exch"><i class="fa fa-check"></i><b>A.6</b> Finitely exchangeable random variables</a></li>
<li class="chapter" data-level="A.7" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-matrixHierachicalModel"><i class="fa fa-check"></i><b>A.7</b> The Matrix Formulation of Hierarchical Models (the Laird-Ware form)</a></li>
<li class="chapter" data-level="A.8" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-cTreatGM"><i class="fa fa-check"></i><b>A.8</b> Treatment contrast with intercept as the grand mean</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html"><i class="fa fa-check"></i><b>B</b> Advanced models with Stan - Extended</a>
<ul>
<li class="chapter" data-level="B.1" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-target"><i class="fa fa-check"></i><b>B.1</b> What does <code>target</code> do in Stan models?</a></li>
<li class="chapter" data-level="B.2" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-tilde"><i class="fa fa-check"></i><b>B.2</b> Explicitly incrementing the log probability function (<code>target</code>) vs. using the sampling or distribution <code>~</code> notation</a></li>
<li class="chapter" data-level="B.3" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-cmdstanr"><i class="fa fa-check"></i><b>B.3</b> An alternative R interface to Stan: <code>cmdstanr</code></a></li>
<li class="chapter" data-level="B.4" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-stancontainers"><i class="fa fa-check"></i><b>B.4</b> Matrix, vector, or array in Stan?</a></li>
<li class="chapter" data-level="B.5" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-noncenterparam"><i class="fa fa-check"></i><b>B.5</b> A simple non-centered parameterization</a></li>
<li class="chapter" data-level="B.6" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-cholesky"><i class="fa fa-check"></i><b>B.6</b> Cholesky factorization for reparameterizing hierarchical models with correlations between adjustments to different parameters</a></li>
<li class="chapter" data-level="B.7" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-sbc"><i class="fa fa-check"></i><b>B.7</b> Different rank visualizations and the <code>SBC</code> package.</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="evidence-synthesis-and-measurements-with-error---extended.html"><a href="evidence-synthesis-and-measurements-with-error---extended.html"><i class="fa fa-check"></i><b>C</b> Evidence synthesis and measurements with error - Extended</a>
<ul>
<li class="chapter" data-level="C.1" data-path="evidence-synthesis-and-measurements-with-error---extended.html"><a href="evidence-synthesis-and-measurements-with-error---extended.html#app-sigmatrue"><i class="fa fa-check"></i><b>C.1</b> What happens if we set <code>sigma = TRUE</code> in <code>resp_se()</code> function in <code>brms</code>?</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html"><i class="fa fa-check"></i><b>D</b> Model comparison - Extended</a>
<ul>
<li class="chapter" data-level="D.1" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-null"><i class="fa fa-check"></i><b>D.1</b> Credible intervals should not be used to reject a null hypothesis</a></li>
<li class="chapter" data-level="D.2" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-likR"><i class="fa fa-check"></i><b>D.2</b> The likelihood ratio vs the Bayes factor</a></li>
<li class="chapter" data-level="D.3" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-integral"><i class="fa fa-check"></i><b>D.3</b> Approximation of the (expected) log predictive density of a model without integration</a></li>
<li class="chapter" data-level="D.4" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-CV-alg"><i class="fa fa-check"></i><b>D.4</b> The cross-validation algorithm for the expected log predictive density of a model</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>E</b> The Art and Science of Prior Elicitation</a>
<ul>
<li class="chapter" data-level="E.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>E.1</b> Eliciting priors from oneself for a self-paced reading study: An example</a>
<ul>
<li class="chapter" data-level="E.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>E.1.1</b> An example: English  relative clauses</a></li>
<li class="chapter" data-level="E.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>E.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="E.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>E.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="E.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>E.1.4</b> Eliciting priors for the  variance components</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>E.2</b>  Eliciting priors from experts</a></li>
<li class="chapter" data-level="E.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>E.3</b> Deriving priors from  meta-analyses</a></li>
<li class="chapter" data-level="E.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>E.4</b> Using previous experiments’  posteriors as priors for a new study</a></li>
<li class="chapter" data-level="E.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-17"><i class="fa fa-check"></i><b>E.5</b> Summary</a></li>
<li class="chapter" data-level="E.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-16"><i class="fa fa-check"></i><b>E.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>F</b> Workflow</a>
<ul>
<li class="chapter" data-level="F.1" data-path="ch-workflow.html"><a href="ch-workflow.html#building-a-model"><i class="fa fa-check"></i><b>F.1</b>  Building a model</a></li>
<li class="chapter" data-level="F.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-to-ask-on-a-model"><i class="fa fa-check"></i><b>F.2</b> Principled questions to ask on a model</a>
<ul>
<li class="chapter" data-level="F.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#checking-whether-assumptions-are-consistent-with-domain-expertise-prior-predictive-checks"><i class="fa fa-check"></i><b>F.2.1</b>  Checking whether assumptions are consistent with  domain expertise: Prior predictive checks</a></li>
<li class="chapter" data-level="F.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#testing-for-correct-posterior-approximations-checks-of-computational-faithfulness"><i class="fa fa-check"></i><b>F.2.2</b>  Testing for correct posterior approximations: Checks of computational faithfulness</a></li>
<li class="chapter" data-level="F.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#sensitivity-of-the-model"><i class="fa fa-check"></i><b>F.2.3</b>  Sensitivity of the model</a></li>
<li class="chapter" data-level="F.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#does-the-model-adequately-capture-the-dataposterior-predictive-checks"><i class="fa fa-check"></i><b>F.2.4</b>  Does the model adequately capture the data?–Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="F.3" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-17"><i class="fa fa-check"></i><b>F.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>G</b> Exercises</a>
<ul>
<li class="chapter" data-level="G.1" data-path="exercises.html"><a href="exercises.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>G.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="G.1.1" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart1"><i class="fa fa-check"></i><b>G.1.1</b> Practice using the <code>pnorm()</code> function–Part 1</a></li>
<li class="chapter" data-level="G.1.2" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart2"><i class="fa fa-check"></i><b>G.1.2</b> Practice using the <code>pnorm()</code> function–Part 2</a></li>
<li class="chapter" data-level="G.1.3" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart3"><i class="fa fa-check"></i><b>G.1.3</b> Practice using the <code>pnorm()</code> function–Part 3</a></li>
<li class="chapter" data-level="G.1.4" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisesqnormPart1"><i class="fa fa-check"></i><b>G.1.4</b> Practice using the <code>qnorm()</code> function–Part 1</a></li>
<li class="chapter" data-level="G.1.5" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisesqnormPart2"><i class="fa fa-check"></i><b>G.1.5</b> Practice using the <code>qnorm()</code> function–Part 2</a></li>
<li class="chapter" data-level="G.1.6" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisessamples1"><i class="fa fa-check"></i><b>G.1.6</b> Practice getting summaries from samples–Part 1</a></li>
<li class="chapter" data-level="G.1.7" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisessamples2"><i class="fa fa-check"></i><b>G.1.7</b> Practice getting summaries from samples–Part 2.</a></li>
<li class="chapter" data-level="G.1.8" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisesvcov1"><i class="fa fa-check"></i><b>G.1.8</b> Practice with a variance-covariance matrix for a bivariate distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.2" data-path="exercises.html"><a href="exercises.html#sec-BDAexercises"><i class="fa fa-check"></i><b>G.2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="G.2.1" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesDerivingBayes"><i class="fa fa-check"></i><b>G.2.1</b> Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="G.2.2" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj1"><i class="fa fa-check"></i><b>G.2.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="G.2.3" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj2"><i class="fa fa-check"></i><b>G.2.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="G.2.4" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj3"><i class="fa fa-check"></i><b>G.2.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="G.2.5" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj4"><i class="fa fa-check"></i><b>G.2.5</b> Conjugate forms 4</a></li>
<li class="chapter" data-level="G.2.6" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesWeightedMean"><i class="fa fa-check"></i><b>G.2.6</b> The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)</a></li>
</ul></li>
<li class="chapter" data-level="G.3" data-path="exercises.html"><a href="exercises.html#ex:compbda"><i class="fa fa-check"></i><b>G.3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="G.3.1" data-path="exercises.html"><a href="exercises.html#exr:simulatedlinearmod"><i class="fa fa-check"></i><b>G.3.1</b> Check for parameter recovery in a linear model using simulated data.</a></li>
<li class="chapter" data-level="G.3.2" data-path="exercises.html"><a href="exercises.html#exr:linearmod"><i class="fa fa-check"></i><b>G.3.2</b> A simple linear model.</a></li>
<li class="chapter" data-level="G.3.3" data-path="exercises.html"><a href="exercises.html#exr:compbda-biasedpost"><i class="fa fa-check"></i><b>G.3.3</b> Revisiting the button-pressing example with different priors.</a></li>
<li class="chapter" data-level="G.3.4" data-path="exercises.html"><a href="exercises.html#exr:ppd"><i class="fa fa-check"></i><b>G.3.4</b> Posterior predictive checks with a log-normal model.</a></li>
<li class="chapter" data-level="G.3.5" data-path="exercises.html"><a href="exercises.html#exr:skew"><i class="fa fa-check"></i><b>G.3.5</b> A skew normal distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.4" data-path="exercises.html"><a href="exercises.html#sec-LMexercises"><i class="fa fa-check"></i><b>G.4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="G.4.1" data-path="exercises.html"><a href="exercises.html#exr:powerposing"><i class="fa fa-check"></i><b>G.4.1</b> A simple linear regression: Power posing and testosterone.</a></li>
<li class="chapter" data-level="G.4.2" data-path="exercises.html"><a href="exercises.html#exr:pupils"><i class="fa fa-check"></i><b>G.4.2</b> Another linear regression model: Revisiting attentional load effect on pupil size.</a></li>
<li class="chapter" data-level="G.4.3" data-path="exercises.html"><a href="exercises.html#exr:lognormalm"><i class="fa fa-check"></i><b>G.4.3</b> Log-normal model: Revisiting the effect of trial on finger tapping times.</a></li>
<li class="chapter" data-level="G.4.4" data-path="exercises.html"><a href="exercises.html#exr:reg-logistic"><i class="fa fa-check"></i><b>G.4.4</b> Logistic regression: Revisiting the effect of set size on free recall.</a></li>
<li class="chapter" data-level="G.4.5" data-path="exercises.html"><a href="exercises.html#exr:red"><i class="fa fa-check"></i><b>G.4.5</b> Red is the sexiest color.</a></li>
</ul></li>
<li class="chapter" data-level="G.5" data-path="exercises.html"><a href="exercises.html#sec-HLMexercises"><i class="fa fa-check"></i><b>G.5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="G.5.1" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-normal"><i class="fa fa-check"></i><b>G.5.1</b> A hierarchical model (normal likelihood) of cognitive load on pupil size.</a></li>
<li class="chapter" data-level="G.5.2" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-logn"><i class="fa fa-check"></i><b>G.5.2</b> Are subject relatives easier to process than object relatives (log-normal likelihood)?</a></li>
<li class="chapter" data-level="G.5.3" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseMandarinRC"><i class="fa fa-check"></i><b>G.5.3</b> Relative clause processing in Mandarin Chinese</a></li>
<li class="chapter" data-level="G.5.4" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseEnglishAgrmt"><i class="fa fa-check"></i><b>G.5.4</b>  Agreement attraction in comprehension</a></li>
<li class="chapter" data-level="G.5.5" data-path="exercises.html"><a href="exercises.html#exr:ab"><i class="fa fa-check"></i><b>G.5.5</b>  Attentional blink (Bernoulli likelihood)</a></li>
<li class="chapter" data-level="G.5.6" data-path="exercises.html"><a href="exercises.html#exr:strooplogis-brms"><i class="fa fa-check"></i><b>G.5.6</b> Is there a Stroop effect in accuracy?</a></li>
<li class="chapter" data-level="G.5.7" data-path="exercises.html"><a href="exercises.html#exr:stroop-dist"><i class="fa fa-check"></i><b>G.5.7</b>  Distributional regression for the Stroop effect.</a></li>
<li class="chapter" data-level="G.5.8" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseGramCE"><i class="fa fa-check"></i><b>G.5.8</b> The  grammaticality illusion</a></li>
</ul></li>
<li class="chapter" data-level="G.6" data-path="exercises.html"><a href="exercises.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>G.6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="G.6.1" data-path="exercises.html"><a href="exercises.html#exr:ContrastsPersian"><i class="fa fa-check"></i><b>G.6.1</b> Contrast coding for a four-condition design</a></li>
<li class="chapter" data-level="G.6.2" data-path="exercises.html"><a href="exercises.html#exr:ContrastsNPIHelmert"><i class="fa fa-check"></i><b>G.6.2</b>  Helmert coding for a six-condition design.</a></li>
<li class="chapter" data-level="G.6.3" data-path="exercises.html"><a href="exercises.html#exr:ContrastsNcomparisons"><i class="fa fa-check"></i><b>G.6.3</b> Number of possible comparisons in a single model.</a></li>
</ul></li>
<li class="chapter" data-level="G.7" data-path="exercises.html"><a href="exercises.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>G.7</b> Contrast coding with two predictor variables</a>
<ul>
<li class="chapter" data-level="G.7.1" data-path="exercises.html"><a href="exercises.html#exr:ContrastsPersianANOVA"><i class="fa fa-check"></i><b>G.7.1</b> ANOVA coding for a four-condition design.</a></li>
<li class="chapter" data-level="G.7.2" data-path="exercises.html"><a href="exercises.html#exr:Contrasts2x2x2Dillon2013"><i class="fa fa-check"></i><b>G.7.2</b> ANOVA and nested comparisons in a <span class="math inline">\(2\times 2\times 2\)</span> design</a></li>
</ul></li>
<li class="chapter" data-level="G.8" data-path="exercises.html"><a href="exercises.html#introduction-to-the-probabilistic-programming-language-stan"><i class="fa fa-check"></i><b>G.8</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="G.8.1" data-path="exercises.html"><a href="exercises.html#exr:first"><i class="fa fa-check"></i><b>G.8.1</b> A very simple model.</a></li>
<li class="chapter" data-level="G.8.2" data-path="exercises.html"><a href="exercises.html#exr:badstan"><i class="fa fa-check"></i><b>G.8.2</b> Incorrect Stan model.</a></li>
<li class="chapter" data-level="G.8.3" data-path="exercises.html"><a href="exercises.html#exr:skewstan"><i class="fa fa-check"></i><b>G.8.3</b> Using Stan documentation.</a></li>
<li class="chapter" data-level="G.8.4" data-path="exercises.html"><a href="exercises.html#exr:linkfunction"><i class="fa fa-check"></i><b>G.8.4</b> The probit link function as an alternative to the logit function.</a></li>
<li class="chapter" data-level="G.8.5" data-path="exercises.html"><a href="exercises.html#exr:logisticstan"><i class="fa fa-check"></i><b>G.8.5</b> Examining the position of the queued word on recall.</a></li>
<li class="chapter" data-level="G.8.6" data-path="exercises.html"><a href="exercises.html#exr:fallacy"><i class="fa fa-check"></i><b>G.8.6</b> The conjunction fallacy.</a></li>
</ul></li>
<li class="chapter" data-level="G.9" data-path="exercises.html"><a href="exercises.html#hierarchical-models-and-reparameterization"><i class="fa fa-check"></i><b>G.9</b> Hierarchical models and reparameterization</a>
<ul>
<li class="chapter" data-level="G.9.1" data-path="exercises.html"><a href="exercises.html#exr:stroop"><i class="fa fa-check"></i><b>G.9.1</b> A log-normal model in Stan.</a></li>
<li class="chapter" data-level="G.9.2" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-logn-stan"><i class="fa fa-check"></i><b>G.9.2</b> A by-subjects and by-items hierarchical model with a log-normal likelihood.</a></li>
<li class="chapter" data-level="G.9.3" data-path="exercises.html"><a href="exercises.html#exr:strooplogis"><i class="fa fa-check"></i><b>G.9.3</b> A hierarchical logistic regression with Stan.</a></li>
<li class="chapter" data-level="G.9.4" data-path="exercises.html"><a href="exercises.html#exr:distr-stan"><i class="fa fa-check"></i><b>G.9.4</b> A distributional regression model of the effect of cloze probability on the N400.</a></li>
</ul></li>
<li class="chapter" data-level="G.10" data-path="exercises.html"><a href="exercises.html#sec-customexercises"><i class="fa fa-check"></i><b>G.10</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="G.10.1" data-path="exercises.html"><a href="exercises.html#exr:shiftedlogn"><i class="fa fa-check"></i><b>G.10.1</b> Fitting a  shifted log-normal distribution.</a></li>
<li class="chapter" data-level="G.10.2" data-path="exercises.html"><a href="exercises.html#exr:wald"><i class="fa fa-check"></i><b>G.10.2</b> Fitting a Wald distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.11" data-path="exercises.html"><a href="exercises.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>G.11</b> Meta-analysis and measurement error models</a>
<ul>
<li class="chapter" data-level="G.11.1" data-path="exercises.html"><a href="exercises.html#exr:REMAMEExtracting"><i class="fa fa-check"></i><b>G.11.1</b> Extracting estimates from published papers</a></li>
<li class="chapter" data-level="G.11.2" data-path="exercises.html"><a href="exercises.html#exr:REMAMEBuerki"><i class="fa fa-check"></i><b>G.11.2</b> A meta-analysis of picture-word interference data</a></li>
<li class="chapter" data-level="G.11.3" data-path="exercises.html"><a href="exercises.html#exr:REMAMELiEnglish"><i class="fa fa-check"></i><b>G.11.3</b> Measurement error model for English VOT data</a></li>
</ul></li>
<li class="chapter" data-level="G.12" data-path="exercises.html"><a href="exercises.html#introduction-to-model-comparison"><i class="fa fa-check"></i><b>G.12</b> Introduction to model comparison</a></li>
<li class="chapter" data-level="G.13" data-path="exercises.html"><a href="exercises.html#bayes-factors"><i class="fa fa-check"></i><b>G.13</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="G.13.1" data-path="exercises.html"><a href="exercises.html#exr:bysubjects"><i class="fa fa-check"></i><b>G.13.1</b> Is there evidence for differences in the effect of cloze probability among the subjects?</a></li>
<li class="chapter" data-level="G.13.2" data-path="exercises.html"><a href="exercises.html#exr:bf-logn"><i class="fa fa-check"></i><b>G.13.2</b> Is there evidence for the claim that English subject relative clauses are easier to process than object relative clauses?</a></li>
<li class="chapter" data-level="G.13.3" data-path="exercises.html"><a href="exercises.html#exr:bf-logistic"><i class="fa fa-check"></i><b>G.13.3</b> In the Grodner and Gibson 2005 data, in question-response accuracies, is there evidence for the claim that sentences with subject relative clauses are easier to comprehend?</a></li>
<li class="chapter" data-level="G.13.4" data-path="exercises.html"><a href="exercises.html#exr:lognstan"><i class="fa fa-check"></i><b>G.13.4</b> Bayes factor and bounded parameters using Stan.</a></li>
</ul></li>
<li class="chapter" data-level="G.14" data-path="exercises.html"><a href="exercises.html#cross-validation"><i class="fa fa-check"></i><b>G.14</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="G.14.1" data-path="exercises.html"><a href="exercises.html#exr:logcv"><i class="fa fa-check"></i><b>G.14.1</b> Predictive accuracy of the linear and the logarithm effect of cloze probability.</a></li>
<li class="chapter" data-level="G.14.2" data-path="exercises.html"><a href="exercises.html#exr:stroopcv"><i class="fa fa-check"></i><b>G.14.2</b> Log-normal model</a></li>
<li class="chapter" data-level="G.14.3" data-path="exercises.html"><a href="exercises.html#exr:logrec"><i class="fa fa-check"></i><b>G.14.3</b> Log-normal vs rec-normal model in Stan</a></li>
</ul></li>
<li class="chapter" data-level="G.15" data-path="exercises.html"><a href="exercises.html#introduction-to-cognitive-modeling"><i class="fa fa-check"></i><b>G.15</b> Introduction to cognitive modeling</a></li>
<li class="chapter" data-level="G.16" data-path="exercises.html"><a href="exercises.html#multinomial-processing-trees"><i class="fa fa-check"></i><b>G.16</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="G.16.1" data-path="exercises.html"><a href="exercises.html#exr:mult"><i class="fa fa-check"></i><b>G.16.1</b> Modeling multiple categorical responses.</a></li>
<li class="chapter" data-level="G.16.2" data-path="exercises.html"><a href="exercises.html#exr:mpt-mnm"><i class="fa fa-check"></i><b>G.16.2</b> An alternative MPT to model the picture recognition task.</a></li>
<li class="chapter" data-level="G.16.3" data-path="exercises.html"><a href="exercises.html#exr:edit-mpt-cat"><i class="fa fa-check"></i><b>G.16.3</b> A simple MPT model that incorporates phonological complexity in the picture recognition task.</a></li>
<li class="chapter" data-level="G.16.4" data-path="exercises.html"><a href="exercises.html#exr:mpt"><i class="fa fa-check"></i><b>G.16.4</b> A more hierarchical MPT.</a></li>
<li class="chapter" data-level="G.16.5" data-path="exercises.html"><a href="exercises.html#exr:mpt-adv"><i class="fa fa-check"></i><b>G.16.5</b> <strong>Advanced</strong>: Multinomial processing trees.</a></li>
</ul></li>
<li class="chapter" data-level="G.17" data-path="exercises.html"><a href="exercises.html#mixture-models"><i class="fa fa-check"></i><b>G.17</b> Mixture models</a>
<ul>
<li class="chapter" data-level="G.17.1" data-path="exercises.html"><a href="exercises.html#exr:pcorrect"><i class="fa fa-check"></i><b>G.17.1</b> Changes in the true point values.</a></li>
<li class="chapter" data-level="G.17.2" data-path="exercises.html"><a href="exercises.html#exr:mixhier"><i class="fa fa-check"></i><b>G.17.2</b> RTs in schizophrenic patients and control.</a></li>
<li class="chapter" data-level="G.17.3" data-path="exercises.html"><a href="exercises.html#exr:mixbias"><i class="fa fa-check"></i><b>G.17.3</b> <strong>Advanced:</strong> Guessing bias in the model.</a></li>
</ul></li>
<li class="chapter" data-level="G.18" data-path="exercises.html"><a href="exercises.html#a-simple-accumulator-model-to-account-for-choice-response-time"><i class="fa fa-check"></i><b>G.18</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="G.18.1" data-path="exercises.html"><a href="exercises.html#exr:recovery"><i class="fa fa-check"></i><b>G.18.1</b> Can we recover the true point values of the parameters of a model when dealing with a contaminant distribution?</a></li>
<li class="chapter" data-level="G.18.2" data-path="exercises.html"><a href="exercises.html#exr:lnracescale"><i class="fa fa-check"></i><b>G.18.2</b> Can the log-normal race model account for fast errors?</a></li>
<li class="chapter" data-level="G.18.3" data-path="exercises.html"><a href="exercises.html#exr:lnldt"><i class="fa fa-check"></i><b>G.18.3</b> Accounting for response time and choice in the lexical decision task using the log-normal race model.</a></li>
</ul></li>
<li class="chapter" data-level="G.19" data-path="exercises.html"><a href="exercises.html#sec-priorsexercises"><i class="fa fa-check"></i><b>G.19</b> The Art and Science of  Prior Elicitation</a>
<ul>
<li class="chapter" data-level="G.19.1" data-path="exercises.html"><a href="exercises.html#exr:PriorsRCs"><i class="fa fa-check"></i><b>G.19.1</b> Develop a plausible informative prior for the difference between object and subject relative clause reading times</a></li>
<li class="chapter" data-level="G.19.2" data-path="exercises.html"><a href="exercises.html#exr:Priorslocalcoherence"><i class="fa fa-check"></i><b>G.19.2</b> Extracting an informative prior from a published paper for a future study</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-contr" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Contrast coding<a href="ch-contr.html#ch-contr" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Whenever one uses a categorical variable as a predictor in a Bayesian regression model, for example when estimating the difference in a dependent variable among two or three experimental conditions, it becomes necessary to code the discrete factor levels into numeric predictor variables. This coding is termed <em>contrast coding</em>. For example, in the previous chapter (section <a href="ch-hierarchical.html#sec-stroop">5.3</a>), we coded two experimental conditions as <span class="math inline">\(-1\)</span> and <span class="math inline">\(+1\)</span>; this is called a sum contrast. Contrasts are the values that we assign to predictor variables to encode specific comparisons between factor levels and to create predictor terms to estimate these comparisons in any type of regression, including Bayesian regressions.</p>
<p>Contrast coding in Bayesian models works more or less the same way as in frequentist models, and the same principles and tools can be used in both cases.
As one important difference, the prior can matter in Bayesian models <span class="citation">(especially when working with Bayes factors: Dablander et al. <a href="#ref-dablander2022puzzle" role="doc-biblioref">2022</a>; Rouder et al. <a href="#ref-rouder2012default" role="doc-biblioref">2012</a>)</span>; the prior needs to be adapted to the scaling of contrasts. For example, given two conditions in an experiment, if we code a sum contrast (discussed below) with <span class="math inline">\(\pm 1\)</span> coding, the parameter representing the difference between the two conditions will be half as large as the observed difference in the data, compared to the case when we code the contrasts with <span class="math inline">\(\pm 0.5\)</span> coding. In the latter case, the parameter of interest does represent the observed difference in means between the conditions. The two contrast coding options (<span class="math inline">\(\pm 1\)</span> and <span class="math inline">\(\pm 0.5\)</span>) obviously imply different prior specifications for the parameter of interest.</p>
<p>This chapter will introduce contrast coding in the context of Bayesian models. The descriptions are in large parts taken from <span class="citation">Schad et al. (<a href="#ref-schadHowCapitalizePriori2020" role="doc-biblioref">2020</a>)</span> (which is published under a CC-BY 4.0 license: <a href="https://creativecommons.org/licenses/by/4.0/" class="uri">https://creativecommons.org/licenses/by/4.0/</a>) and adapted for the current context.</p>
<p>Consider a situation where we want to estimate differences in a dependent variable between three levels of a factor. An example could be differences in response times between three levels of word class (noun, verb, adjective). We might be interested in whether word class influences response times. In frequentist statistics, one way to approach this question would be to run an ANOVA and compute an omnibus <span class="math inline">\(F\)</span>-test for whether word class explains response times.<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a> However, if based on such omnibus approaches we find support for an influence of word class on response times, it remains unclear where this effect actually comes from, i.e., whether it originated from the nouns, verbs, or adjectives. This is problematic for inference because scientists typically have specific expectations about which groups differ from each other. In this chapter, we will show how to estimate specific comparisons directly in a Bayesian linear model. This gives the researcher a lot of control over Bayesian analyses. Specifically, we show how  planned comparisons between specific conditions  (groups) or clusters of conditions, are implemented as contrasts. This is a very effective way to align expectations with the statistical model. In Bayesian models, any specific comparisons can also be computed after the model is fit. Nevertheless, coding a priori expectations into contrasts for model fitting will make it much more straightforward to estimate certain comparisons between experimental conditions, and will allow us to perform Bayesian model comparisons using Bayes factors to provide evidence for or against very specific hypotheses.</p>
<p>For this and the next chapter, although knowledge of the matrix formulation of the linear model is not necessary, for a deeper understanding of contrast coding some exposure to the matrix formulation is desirable. We discuss the matrix formulation in the online section <a href="regression-models-with-brms---extended.html#app-matrixHierachicalModel">A.7</a>.</p>
<div id="basic-concepts-illustrated-using-a-two-level-factor" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Basic concepts illustrated using a two-level factor<a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We first consider the simplest case: suppose we want to compare the means of a dependent variable (DV) such as response times between two groups of subjects. A simulated data set is available in the package <code>bcogsci</code> as the data set <code>df_contrasts1</code>. The simulations assumed longer response times in condition <span class="math inline">\(F1\)</span> (<span class="math inline">\(\mu_1 = 0.8\)</span> sec) than <span class="math inline">\(F2\)</span> (<span class="math inline">\(\mu_2 = 0.4\)</span> sec). The data from the <span class="math inline">\(10\)</span> simulated subjects are aggregated and summary statistics are computed for the two groups.</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb286-1"><a href="ch-contr.html#cb286-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_contrasts1&quot;</span>)</span>
<span id="cb286-2"><a href="ch-contr.html#cb286-2" aria-hidden="true"></a>df_contrasts1</span></code></pre></div>
<pre><code>## # A tibble: 10 × 3
##   F        DV    id
##   &lt;fct&gt; &lt;dbl&gt; &lt;int&gt;
## 1 F1    0.636     1
## 2 F1    0.841     2
## 3 F1    0.555     3
## # ℹ 7 more rows</code></pre>
<pre><code>## [1] 0.6</code></pre>
<table>
<caption><span id="tab:cTab1Means">TABLE 6.1: </span>Summary statistics per condition for the simulated data.</caption>
<thead>
<tr class="header">
<th align="left">Factor</th>
<th align="right">N data</th>
<th align="right">Est. means</th>
<th align="right">Std. dev.</th>
<th align="right">Std. errors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">F1</td>
<td align="right"><span class="math inline">\(5\)</span></td>
<td align="right"><span class="math inline">\(0.8\)</span></td>
<td align="right"><span class="math inline">\(0.2\)</span></td>
<td align="right"><span class="math inline">\(0.1\)</span></td>
</tr>
<tr class="even">
<td align="left">F2</td>
<td align="right"><span class="math inline">\(5\)</span></td>
<td align="right"><span class="math inline">\(0.4\)</span></td>
<td align="right"><span class="math inline">\(0.2\)</span></td>
<td align="right"><span class="math inline">\(0.1\)</span></td>
</tr>
</tbody>
</table>

<div class="figure"><span style="display:block;" id="fig:cFig1Means"></span>
<img src="bayescogsci_files/figure-html/cFig1Means-1.svg" alt="Means and standard errors of the simulated dependent variable (e.g., response times in seconds) in two conditions \(F1\) and \(F2\)." width="384" />
<p class="caption">
FIGURE 6.1: Means and standard errors of the simulated dependent variable (e.g., response times in seconds) in two conditions <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>.
</p>
</div>
<p>The results, displayed in Figure <a href="ch-contr.html#fig:cFig1Means">6.1</a> and in Table <a href="ch-contr.html#tab:cTab1Means">6.1</a>, show that the assumed true condition means are exactly realized with the simulated data. The numbers are exact because the  <code>mvrnorm()</code> function used here (see <code>?df_contrasts1</code>) ensures that the data are generated so that the sample mean yields the true means for each level. In real data sets, of course, the sample means will vary from experiment to experiment.</p>
<p>A simple Bayesian linear model of <code>DV</code> on <span class="math inline">\(F\)</span> yields a straightforward estimate of the difference between the group means. We use relatively uninformative priors. The estimates for the population-level effects are presented below using the function <code>fixef()</code>:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb289-1"><a href="ch-contr.html#cb289-1" aria-hidden="true"></a>fit_F &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span>F,</span>
<span id="cb289-2"><a href="ch-contr.html#cb289-2" aria-hidden="true"></a>             <span class="dt">data =</span> df_contrasts1,</span>
<span id="cb289-3"><a href="ch-contr.html#cb289-3" aria-hidden="true"></a>             <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb289-4"><a href="ch-contr.html#cb289-4" aria-hidden="true"></a>             <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb289-5"><a href="ch-contr.html#cb289-5" aria-hidden="true"></a>                       <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb289-6"><a href="ch-contr.html#cb289-6" aria-hidden="true"></a>                       <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb290"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb290-1"><a href="ch-contr.html#cb290-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_F)</span></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept     0.79      0.11  0.57  1.01
## FF2          -0.39      0.16 -0.70 -0.07</code></pre>
<p>Comparing the means for each condition with the coefficients (<code>Estimates</code>) reveals that (i) the intercept (<span class="math inline">\(0.8\)</span>) is the mean for condition <span class="math inline">\(F1\)</span>, <span class="math inline">\(\hat\mu_1\)</span>; and (ii) the slope (<code>FF2</code>: <span class="math inline">\(-0.4\)</span>) is the difference between the estimated means for the two groups, <span class="math inline">\(\hat\mu_2 - \hat\mu_1\)</span> <span class="citation">(Bolker <a href="#ref-Bolker2018" role="doc-biblioref">2018</a>)</span>:</p>
<p><span class="math display" id="eq:betac">\[\begin{equation}
\begin{array}{lcl}
\text{Intercept} = &amp; \hat{\mu}_1 &amp; = \text{estimated mean for } F1 \\
\text{Slope (FF2)} = &amp; \hat{\mu}_2 - \hat{\mu}_1 &amp; = \text{estim. mean for } F2 - \text{estim. mean for }F1
\end{array}
\tag{6.1}
\end{equation}\]</span></p>
<p>The new information is the <span class="math inline">\(95\)</span>% credible interval for the difference between the two groups.</p>
<div id="treatmentcontrasts" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Default contrast coding:  Treatment contrasts<a href="ch-contr.html#treatmentcontrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How does the function <code>brm()</code> arrive at these particular values for the intercept and slope? That is, why does the intercept assess the mean of condition <span class="math inline">\(F1\)</span> and how do we know the slope measures the difference in means between <span class="math inline">\(F2\)</span>-<span class="math inline">\(F1\)</span>? This result is a consequence of the  default contrast coding of the factor <span class="math inline">\(F\)</span>. R assigns treatment contrasts to factors and orders their levels alphabetically. The alphabetically first factor level (here: <span class="math inline">\(F1\)</span>) is coded in R by default as <span class="math inline">\(0\)</span> and the second level (here: <span class="math inline">\(F2\)</span>) is coded as <span class="math inline">\(1\)</span>.<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a> This becomes clear when we inspect the current contrast attribute of the factor using the  <code>contrasts()</code> command:</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb292-1"><a href="ch-contr.html#cb292-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts1<span class="op">$</span>F)</span></code></pre></div>
<pre><code>##    F2
## F1  0
## F2  1</code></pre>
<p>Why does this contrast coding yield these particular regression coefficients? Let’s take a look at the  regression equation.
Let <span class="math inline">\(\alpha\)</span> represent the intercept, and <span class="math inline">\(\beta_1\)</span> the slope. Then, the simple regression above expresses the belief that the expected response time <span class="math inline">\(\hat{y}\)</span> (or <span class="math inline">\(E[Y]\)</span>) is a linear function of the factor <span class="math inline">\(F\)</span>.</p>
<p><span class="math display">\[\begin{equation}
E[Y] = \alpha + \beta_1 x
\label{eq:lm1}
\end{equation}\]</span></p>
<p>So, if <span class="math inline">\(x = 0\)</span> (condition <span class="math inline">\(F1\)</span>), the expectation is <span class="math inline">\(\alpha + \beta_1 \cdot 0 = \alpha\)</span>; and if <span class="math inline">\(x = 1\)</span> (condition <span class="math inline">\(F2\)</span>), the expectation is <span class="math inline">\(\alpha + \beta_1 \cdot 1 = \alpha + \beta_1\)</span>.</p>
<p>Expressing the above in terms of the estimated coefficients:</p>
<p><span class="math display">\[\begin{equation}
\begin{array}{lccll}
\text{estim. value for }F1 = &amp; \hat{\mu}_1 = &amp; \hat{\alpha} = &amp; \text{Intercept} \\
\text{estim. value for }F2 = &amp; \hat{\mu}_2 = &amp; \hat{\alpha} + \hat{\beta}_1 = &amp; \text{Intercept} + \text{Slope (FF2)}
\end{array}
\label{eq:predVal}
\end{equation}\]</span></p>
<p>It is useful to think of such unstandardized regression coefficients as  difference scores; they express the increase in the dependent variable <span class="math inline">\(y\)</span> associated with a change in the independent variable <span class="math inline">\(x\)</span> by one unit, such as going from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span> in this example. The difference between condition means is <span class="math inline">\(0.4 - 0.8 = -0.4\)</span>, which is the estimated regression coefficient <span class="math inline">\(\hat{\beta}_1\)</span>. The sign of the slope is negative because we have chosen to subtract the larger mean <span class="math inline">\(F1\)</span> score from the smaller mean <span class="math inline">\(F2\)</span> score.</p>
</div>
<div id="inverseMatrix" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Defining comparisons<a href="ch-contr.html#inverseMatrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The analysis of the regression equation demonstrates that in the treatment contrast the intercept assesses the average response in the baseline condition, whereas the slope estimates the difference between condition means. However, these are just verbal descriptions of what each coefficient assesses. Is it also possible to formally write down what each coefficient assesses?</p>
<p>From the perspective of parameter estimation, the slope represents the effect of main interest, so we consider this first. The treatment contrast specifies that the slope <span class="math inline">\(\beta_1\)</span> estimates the difference in means between the two levels of the factor <span class="math inline">\(F\)</span>. This can formally be written as:</p>
<p><span class="math display">\[\begin{equation}
\beta_1 = \mu_{F2} - \mu_{F1}
\end{equation}\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[\begin{equation}
\beta_1 = - 1 \cdot \mu_{F1} + 1 \cdot \mu_{F2}
\end{equation}\]</span></p>
<p>The <span class="math inline">\(\pm 1\)</span> weights in the parameter estimation directly express which means are compared by the treatment contrast.</p>
<p>The intercept in the treatment contrast estimates a quantity that is usually of little interest: it estimates the mean in condition <span class="math inline">\(F1\)</span>.
Formally, the parameter <span class="math inline">\(\alpha\)</span> estimates the following quantity:</p>
<p><span class="math display">\[\begin{equation}
\alpha = \mu_{F1}
\end{equation}\]</span></p>
<p>
or equivalently:</p>
<p><span class="math display">\[\begin{equation}
\alpha = 1 \cdot \mu_{F1} + 0 \cdot \mu_{F2} .
\end{equation}\]</span></p>
<p>
The fact that the intercept term formally estimates the mean of condition <span class="math inline">\(F1\)</span> is in line with our previous derivation (see equation <a href="ch-contr.html#eq:betac">(6.1)</a>).</p>
<p>In R, factor levels are ordered alphabetically and by default the first level is used as the baseline in treatment contrasts. Obviously, this default mapping will depend on the levels’ alphabetical ordering. If a different  baseline condition is desired, it is possible to re-order the levels. Here is one way of re-ordering the levels:</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb294-1"><a href="ch-contr.html#cb294-1" aria-hidden="true"></a>df_contrasts1<span class="op">$</span>Fb &lt;-<span class="st"> </span><span class="kw">factor</span>(df_contrasts1<span class="op">$</span>F, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;F2&quot;</span>, <span class="st">&quot;F1&quot;</span>))</span>
<span id="cb294-2"><a href="ch-contr.html#cb294-2" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts1<span class="op">$</span>Fb)</span></code></pre></div>
<pre><code>##    F1
## F2  0
## F1  1</code></pre>
<p>
This re-ordering did not change any data associated with the factor, only one of its attributes. With this new contrast attribute, a simple Bayesian model yields the following result.</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb296-1"><a href="ch-contr.html#cb296-1" aria-hidden="true"></a>fit_Fb &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span>Fb,</span>
<span id="cb296-2"><a href="ch-contr.html#cb296-2" aria-hidden="true"></a>              <span class="dt">data =</span> df_contrasts1,</span>
<span id="cb296-3"><a href="ch-contr.html#cb296-3" aria-hidden="true"></a>              <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb296-4"><a href="ch-contr.html#cb296-4" aria-hidden="true"></a>              <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb296-5"><a href="ch-contr.html#cb296-5" aria-hidden="true"></a>                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb296-6"><a href="ch-contr.html#cb296-6" aria-hidden="true"></a>                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb297"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb297-1"><a href="ch-contr.html#cb297-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_Fb)</span></code></pre></div>
<pre><code>##           Estimate Est.Error Q2.5 Q97.5
## Intercept     0.40      0.11 0.19  0.63
## FbF1          0.39      0.15 0.09  0.68</code></pre>
<p>The model now estimates different quantities. The intercept now codes the mean of condition <span class="math inline">\(F2\)</span>, and the slope measures the difference in means between <span class="math inline">\(F1\)</span> minus <span class="math inline">\(F2\)</span>. This represents an alternative coding of the treatment contrast.</p>
<p>These model posteriors do not allow us to claim that we have evidence for the hypothesis that the effect of factor <span class="math inline">\(F\)</span> is different from zero. If the research focus is on such hypothesis testing, Bayesian hypothesis tests can be carried out using Bayes factors, by comparing a model containing a contrast of interest with a model lacking this contrast. We will discuss details of Bayesian hypothesis testing based on Bayes factors in chapter <a href="ch-bf.html#ch-bf">13</a>. For now, our focus in obtaining estimates of the relevant comparison(s) we are interested in when we have two or more conditions.</p>
</div>
<div id="effectcoding" class="section level3 hasAnchor" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span>  Sum contrasts<a href="ch-contr.html#effectcoding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Treatment contrasts are only one of many options. It is also possible to use sum contrasts, which code one of the conditions as <span class="math inline">\(-1\)</span> and the other as <span class="math inline">\(+1\)</span>, effectively <em>centering</em> the effects at the  grand mean (GM, i.e., the mean of the two group means). Here, we rescale the contrast to values of <span class="math inline">\(-0.5\)</span> and <span class="math inline">\(+0.5\)</span>, which makes the estimated treatment effect the same as for treatment coding and are easier to interpret.</p>
<p>To define this contrast in a linear regression, one way is to use the <code>contrasts</code> function (another way is to define a column containing <span class="math inline">\(+0.5\)</span> and <span class="math inline">\(-0.5\)</span> for the corresponding levels of the factor).</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb299-1"><a href="ch-contr.html#cb299-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts1<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">+0.5</span>)</span>
<span id="cb299-2"><a href="ch-contr.html#cb299-2" aria-hidden="true"></a>fit_mSum &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span>F,</span>
<span id="cb299-3"><a href="ch-contr.html#cb299-3" aria-hidden="true"></a>                <span class="dt">data =</span> df_contrasts1,</span>
<span id="cb299-4"><a href="ch-contr.html#cb299-4" aria-hidden="true"></a>                <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb299-5"><a href="ch-contr.html#cb299-5" aria-hidden="true"></a>                <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb299-6"><a href="ch-contr.html#cb299-6" aria-hidden="true"></a>                          <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb299-7"><a href="ch-contr.html#cb299-7" aria-hidden="true"></a>                          <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb300"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb300-1"><a href="ch-contr.html#cb300-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_mSum)</span></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept     0.60      0.08  0.43  0.76
## F1           -0.39      0.16 -0.70 -0.07</code></pre>
<p>Here, the slope (<span class="math inline">\(F1\)</span>) again codes the difference of the groups associated with the first and second factor levels. It has the same value as in the treatment contrast.
One important difference from the treatment contrast is that the intercept now represents the estimate of the average of condition means for <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>, that is, the grand mean. For the scaled sum contrast:</p>
<p><span class="math display">\[\begin{equation}
\begin{array}{lcl}
\text{Intercept} = &amp; (\hat{\mu}_1 + \hat{\mu}_2)/2 &amp; = \text{estimated mean of }F1 \text{ and }F2 \\
\text{Slope (F1)} = &amp; \hat{\mu}_2 - \hat{\mu}_1 &amp; = \text{estim. mean of }F2 - \text{estim. mean for} F1
\end{array}
\label{eq:beta2}
\end{equation}\]</span></p>
<p>Why does the intercept assess the grand mean and why does the slope estimate the group difference? This is the result of rescaling the sum contrast. The first factor level (<span class="math inline">\(F1\)</span>) was coded as <span class="math inline">\(-0.5\)</span>, and the second factor level (<span class="math inline">\(F2\)</span>) as <span class="math inline">\(+0.5\)</span>:</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb302-1"><a href="ch-contr.html#cb302-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts1<span class="op">$</span>F)</span></code></pre></div>
<pre><code>##    [,1]
## F1 -0.5
## F2  0.5</code></pre>
<p>Look again at the regression equation to better understand what computations are performed. Again, <span class="math inline">\(\alpha\)</span> represents the intercept, <span class="math inline">\(\beta_1\)</span> represents the slope, and the predictor variable <span class="math inline">\(x\)</span> represents the factor <span class="math inline">\(F\)</span>. The regression equation is written as:</p>
<p><span class="math display">\[\begin{equation}
E[Y] = \alpha + \beta_1 x
\label{eq:lm2}
\end{equation}\]</span></p>
<p>The group of <span class="math inline">\(F1\)</span> subjects is then coded as <span class="math inline">\(-0.5\)</span>, and the response time for the group of <span class="math inline">\(F1\)</span> subjects is <span class="math inline">\(\alpha + \beta_1 \cdot x_1 = 0.6 + (-0.4) \cdot (-0.5) = 0.8\)</span>. By contrast, the <span class="math inline">\(F2\)</span> group is coded as <span class="math inline">\(+0.5\)</span>. By implication, the mean of the <span class="math inline">\(F2\)</span> group must be <span class="math inline">\(\alpha + \beta_1 \cdot x_1 = 0.6 + (-0.4) \cdot 0.5 = 0.4\)</span>.
Expressed in terms of the estimated coefficients:</p>
<p><span class="math display">\[\begin{equation}
\begin{array}{lccll}
\text{estim. value for }F1 = &amp; \hat{\mu}_1 = &amp; \hat{\alpha} - 0.5 \cdot \hat{\beta}_1 = &amp; \text{Intercept} - 0.5 \cdot \text{Slope (F1)}\\
\text{estim. value for }F2 = &amp; \hat{\mu}_2 = &amp; \hat{\alpha} + 0.5 \cdot \hat{\beta}_1 = &amp; \text{Intercept} + 0.5 \cdot \text{Slope (F1)}
\end{array}
\label{eq:predVal2}
\end{equation}\]</span></p>
<p>The  unstandardized regression coefficient is a  difference score: Taking a step of one unit on the predictor variable <span class="math inline">\(x\)</span>, e.g., from <span class="math inline">\(-0.5\)</span> to <span class="math inline">\(+0.5\)</span>, reflecting a step from condition <span class="math inline">\(F1\)</span> to <span class="math inline">\(F2\)</span>, changes the dependent variable from <span class="math inline">\(0.8\)</span> (for condition <span class="math inline">\(F1\)</span>) to <span class="math inline">\(0.4\)</span> (condition <span class="math inline">\(F2\)</span>). This reflects a difference of <span class="math inline">\(0.4 - 0.8 = -0.4\)</span>; this is again the estimated regression coefficient <span class="math inline">\(\hat{\beta}_1\)</span>.
Moreover, as mentioned above, the intercept now assesses the grand mean, i.e., the unweighted mean of the means for conditions <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>: it is in the middle between condition means for <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>.</p>
<p>So far we gave verbal statements about what is estimated by the intercept and the slope in the case of the scaled sum contrast. It is possible to write these statements as formal parameter estimates.
In scaled sum contrasts, the slope parameter <span class="math inline">\(\beta_1\)</span> assesses the following quantity:</p>
<p><span class="math display">\[\begin{equation}
\beta_1 = -1 \cdot \mu_{F1} + 1 \cdot \mu_{F2}
\end{equation}\]</span></p>
<p>
This estimates the same quantity as the slope in the treatment contrast.
The intercept now assesses a different quantity: the average of the two conditions <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\alpha = 1/2 \cdot \mu_{F1} + 1/2 \cdot \mu_{F2} = \frac{\mu_{F1} + \mu_{F2}}{2}
\end{equation}\]</span></p>
<p>
In balanced data, i.e., in data sets where there are no missing data points, the average of the two conditions <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span> is the  grand mean. In unbalanced data sets, where there are missing values, this average is the  weighted grand mean.
To illustrate this point, consider an example with fully balanced data and two equal group sizes of <span class="math inline">\(5\)</span> subjects for each group <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>. Here, the grand mean is also the mean across all subjects. Next, consider a highly simplified unbalanced data set, where in condition <span class="math inline">\(F1\)</span> two observations of the dependent variable are available with values of <span class="math inline">\(2\)</span> and <span class="math inline">\(3\)</span>, and where in condition <span class="math inline">\(F2\)</span> only one observation of the dependent variable is available with a value of <span class="math inline">\(4\)</span>. In this data set, the mean across all subjects is <span class="math inline">\(\frac{2 + 3 + 4}{3} = \frac{9}{3} = 3\)</span>. However, the (weighted) grand mean as assessed in the intercept in a model using sum contrasts for factor <span class="math inline">\(F\)</span> would first compute the mean for each group separately (i.e., <span class="math inline">\(\frac{2 + 3}{2} = 2.5\)</span>, and <span class="math inline">\(4\)</span>), and then compute the mean across conditions <span class="math inline">\(\frac{2.5 + 4}{2} = \frac{6.5}{2} = 3.25\)</span>. The grand mean of <span class="math inline">\(3.25\)</span> is different from the mean <span class="math inline">\(3\)</span> that was computed using all the subjects’ data all at once.</p>
<p>To summarize, treatment contrasts and sum contrasts are two possible ways to parameterize the difference between two groups; they generally estimate different quantities. Treatment contrasts compare one or more means against a baseline condition, whereas sum contrasts compare a condition’s mean to the grand mean (which in the two-group case also implies estimating the difference between the two group means). One question that comes up here is: how does one know what quantities are estimated by a given set of contrasts? (In the context of Bayes factors, the question would be: what hypothesis test does the contrast coding encode?) This question will be discussed in detail below for the general case of any arbitrary contrast coding.</p>
</div>
<div id="sec-cellMeans" class="section level3 hasAnchor" number="6.1.4">
<h3><span class="header-section-number">6.1.4</span>  Cell means parameterization and  posterior comparisons<a href="ch-contr.html#sec-cellMeans" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One alternative option is to use what is called the cell means parameterization (this coding is also called  “one-hot encoding” in the context of machine learning). In this approach, one does not estimate an intercept term, and then differences between factor levels. Instead, each free parameter is used to simply estimate the mean of one of the factor levels. As a consequence, no comparisons between condition means are estimated, but simply the mean of each experimental condition is estimated. Cell means parameterization is specified by explicitly removing the intercept term (which is added automatically in <code>brms</code>) by adding a <span class="math inline">\(-1\)</span> in the regression formula:</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb304-1"><a href="ch-contr.html#cb304-1" aria-hidden="true"></a>fit_mCM &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span>F,</span>
<span id="cb304-2"><a href="ch-contr.html#cb304-2" aria-hidden="true"></a>               <span class="dt">data =</span> df_contrasts1,</span>
<span id="cb304-3"><a href="ch-contr.html#cb304-3" aria-hidden="true"></a>               <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb304-4"><a href="ch-contr.html#cb304-4" aria-hidden="true"></a>               <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb304-5"><a href="ch-contr.html#cb304-5" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb305"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb305-1"><a href="ch-contr.html#cb305-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_mCM)</span></code></pre></div>
<pre><code>##     Estimate Est.Error Q2.5 Q97.5
## FF1     0.79      0.11 0.57  1.01
## FF2     0.40      0.11 0.18  0.63</code></pre>
<p>Now, the regression coefficients (see the column labeled <code>Estimate</code>) estimate the mean of the first factor level (<span class="math inline">\(0.8\)</span>) and the mean of the second factor level (<span class="math inline">\(0.4\)</span>). This cell means parameterization usually does not allow us to make inferences about the hypotheses of interest using Bayes factors, as these hypotheses usually relate to differences between conditions rather than to whether each condition differs from zero.</p>
<p>The cell means parameterization provides a good example demonstrating an advantage of Bayesian data analysis: In Bayesian models, it is possible to use the posterior samples to compute new estimates that were not directly contained in the fitted model. To implement this, we first extract the posterior samples from the <code>brm()</code> model object:</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb307-1"><a href="ch-contr.html#cb307-1" aria-hidden="true"></a>df_postSamp &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_mCM)</span></code></pre></div>
<p>In a second step, we can then compute comparisons from these posterior samples. For example, we can compute the difference between conditions <span class="math inline">\(F2\)</span> and <span class="math inline">\(F1\)</span>. To do so, we simply take the posterior samples for each condition, and compute their difference.</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb308-1"><a href="ch-contr.html#cb308-1" aria-hidden="true"></a>df_postSamp  &lt;-<span class="st"> </span>df_postSamp <span class="op">%&gt;%</span></span>
<span id="cb308-2"><a href="ch-contr.html#cb308-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">b_dif =</span> b_FF2 <span class="op">-</span><span class="st"> </span>b_FF1)</span></code></pre></div>
<p>This provides a posterior sample of the difference between conditions. It is possible to investigate this posterior sample by looking at its mean and 95% credible interval:</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb309-1"><a href="ch-contr.html#cb309-1" aria-hidden="true"></a><span class="kw">c</span>(<span class="dt">Estimate =</span> <span class="kw">mean</span>(df_postSamp<span class="op">$</span>b_dif),</span>
<span id="cb309-2"><a href="ch-contr.html#cb309-2" aria-hidden="true"></a>  <span class="kw">quantile</span>(df_postSamp<span class="op">$</span>b_dif, <span class="dt">p =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</span></code></pre></div>
<pre><code>## Estimate     2.5%    97.5% 
##  -0.3945  -0.7044  -0.0833</code></pre>
<p>The above summary provides the same estimate (roughly <span class="math inline">\(-0.4\)</span>) that we obtained previously when using the treatment contrast or the scaled sum contrast.
(Note that if the priors are sufficiently non-informative, the results will be similar. However, given that the priors differ in comparison to a model estimating the difference directly, the posterior may also differ between models.)</p>
<p>Thus, Bayesian models provide a lot of flexibility in computing new comparisons post-hoc from the posterior samples and in obtaining their posterior distributions. However, what these posterior computations do not provide directly are inferences on null hypotheses. That is, just by looking at the credible intervals, we cannot make inferences about whether a null hypothesis can be rejected; an explicit hypothesis test is needed to answer such a question (see chapter <a href="ch-bf.html#ch-bf">13</a>).</p>
</div>
</div>
<div id="the-hypothesis-matrix-illustrated-with-a-three-level-factor" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> The hypothesis matrix illustrated with a three-level factor<a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider an example with the three word classes nouns, verbs, and adjectives. We load simulated data from a lexical decision task with response times as dependent variable. The research question is: do response times differ as a function of the between-subject factor word class with three levels: nouns, verbs, and adjectives? Here, just to illustrate the case of a three-level factor, we make the arbitrary assumption that nouns have longest response times and adjectives the shortest response times. Word class is specified as a between-subject factor. In cognitive science experiments, word class will usually vary within subjects and between items. Because the within- or between-subjects status of an effect is independent of its contrast coding, we assume the manipulation to be between subjects for ease of exposition. The concepts presented here extend to repeated measures designs that are often analyzed using hierarchical Bayesian (linear mixed) models.</p>
<p>Load and display the simulated data.</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb311-1"><a href="ch-contr.html#cb311-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_contrasts2&quot;</span>)</span>
<span id="cb311-2"><a href="ch-contr.html#cb311-2" aria-hidden="true"></a><span class="kw">head</span>(df_contrasts2)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 3
##   F        DV    id
##   &lt;fct&gt; &lt;int&gt; &lt;int&gt;
## 1 nouns   476     1
## 2 nouns   517     2
## 3 nouns   491     3
## # ℹ 3 more rows</code></pre>
<table>
<caption><span id="tab:cTab2Means">TABLE 6.2: </span>Summary statistics per condition for the simulated data.</caption>
<thead>
<tr class="header">
<th align="left">Factor</th>
<th align="right">N data</th>
<th align="right">Est. means</th>
<th align="right">Std. dev.</th>
<th align="right">Std. errors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">adjectives</td>
<td align="right"><span class="math inline">\(4\)</span></td>
<td align="right"><span class="math inline">\(400.2\)</span></td>
<td align="right"><span class="math inline">\(19.9\)</span></td>
<td align="right"><span class="math inline">\(9.9\)</span></td>
</tr>
<tr class="even">
<td align="left">nouns</td>
<td align="right"><span class="math inline">\(4\)</span></td>
<td align="right"><span class="math inline">\(500.0\)</span></td>
<td align="right"><span class="math inline">\(20.0\)</span></td>
<td align="right"><span class="math inline">\(10.0\)</span></td>
</tr>
<tr class="odd">
<td align="left">verbs</td>
<td align="right"><span class="math inline">\(4\)</span></td>
<td align="right"><span class="math inline">\(450.2\)</span></td>
<td align="right"><span class="math inline">\(20.0\)</span></td>
<td align="right"><span class="math inline">\(10.0\)</span></td>
</tr>
</tbody>
</table>
<p>As shown in Table <a href="ch-contr.html#tab:cTab2Means">6.2</a>, the estimated means reflect our assumptions about the true means in the data simulation: Response times are longest for nouns and shortest for adjectives.
In the following sections, we use this data set to illustrate sum contrasts. Furthermore, we will use an additional data set to illustrate repeated, Helmert, polynomial, and custom contrasts. In practice, usually only one set of contrasts is selected when the expected pattern of means is formulated during the design of the experiment.</p>
<div id="sumcontrasts" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span>  Sum contrasts<a href="ch-contr.html#sumcontrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We begin with sum contrasts. Suppose that the expectation is that nouns are responded to slower and adjectives are responded to faster than the grand mean response time. Then, the research question could be: By how much do nouns differ from the grand mean and by how much do adjectives differ from the grand mean? And are the responses slower or faster than the grand mean? We want to estimate the following two quantities:</p>
<p><span class="math display">\[\begin{equation}
\beta_1 = \mu_1 - \frac{\mu_1+\mu_2+\mu_3}{3} = \mu_1 - GM
\end{equation}\]</span></p>
<p>
and</p>
<p><span class="math display">\[\begin{equation}
\beta_2 = \mu_2 - \frac{\mu_1+\mu_2+\mu_3}{3} = \mu_2 - GM
\end{equation}\]</span></p>
<p><span class="math inline">\(\beta_1\)</span> can also be written as:</p>
<p><span class="math display">\[\begin{equation} \label{h01}
\beta_1 = \frac{2}{3} \mu_1 - \frac{1}{3}\mu_2 - \frac{1}{3}\mu_3
\end{equation}\]</span></p>
<p>
Here, the weights <span class="math inline">\(2/3, -1/3, -1/3\)</span> are informative about how to combine the condition means to estimate the linear model coefficient.</p>
<p><span class="math inline">\(\beta_2\)</span> is also rewritten as:</p>
<p><span class="math display">\[\begin{align}\label{h02}
\beta_2 = &amp; \mu_2 - \frac{\mu_1+\mu_2+\mu_3}{3} \\
\Leftrightarrow \beta_2 = &amp; -\frac{1}{3}\mu_1 + \frac{2}{3} \mu_2 - \frac{1}{3} \mu_3
\end{align}\]</span></p>
<p>
Here, the weights are <span class="math inline">\(-1/3, 2/3, -1/3\)</span>, and they again indicate how to combine the condition means for estimating the regression coefficient.</p>
</div>
<div id="the-hypothesis-matrix" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> The  hypothesis matrix<a href="ch-contr.html#the-hypothesis-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The  weights of the condition means are not only useful for defining parameter estimates. They also provide the starting step in a very powerful method which allows the researcher to generate the contrasts that are needed to estimate these comparisons in a linear model. What we did so far is to explain some different contrast codings that exist and the comparisons that are estimated by these contrasts. Given a particular data set, if the goal is to estimate certain comparisons, then the procedure would be to check whether any of the contrasts that we encountered estimate these comparisons of interest. Sometimes it suffices to use one of these existing contrasts. At other times, our research questions may not correspond exactly to any of the contrasts in the default set of standard contrasts provided in R. For these cases, or for more complex designs, it is very useful to know how contrast matrices are created. Indeed, a relatively simple procedure exists in which we write our comparisons formally, extract the weights of the condition means from the comparisons, and then automatically generate the correct  contrast matrix that we need in order to estimate these comparisons in a linear model. Using this powerful method, it is not necessary to find a match to a contrast matrix provided by the family of functions in R starting with the prefix <code>contr</code>.<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a> Instead, it is possible to simply define the comparisons that one wants to estimate, and to obtain the correct contrast matrix for these in an automatic procedure. Here, for pedagogical reasons, we show some examples of how to apply this procedure in cases where the comparisons <em>do</em> correspond to some of the existing contrasts.</p>
<p>Defining a  custom contrast matrix involves four steps:</p>
<ol style="list-style-type: decimal">
<li>Write down the estimated comparisons</li>
<li>Extract the weights and write them into what we will call a <em>hypothesis matrix</em> (and can also be viewed as a  <em>comparison matrix</em>)</li>
<li>Apply the  <em>generalized matrix inverse</em> to the hypothesis matrix to create the contrast matrix</li>
<li>Assign the contrast matrix to the factor and run the (Bayesian) model</li>
</ol>
<p>The term hypothesis matrix is used here because contrast coding is often done to carry out an explicit hypothesis test; but one could of course use contrast coding just to compute the estimates of an effect and their uncertainty, without doing a hypothesis test.</p>
<p>Let us apply this four-step procedure to our example of the sum contrast. The first step, writing down the estimated comparisons, as done in section <a href="ch-contr.html#sumcontrasts">6.2.1</a>. The second step involves writing down the weights that each comparison gives to condition means. The weights for the first comparison are <code>wH01 = c(+2/3, -1/3, -1/3)</code>, and the weights for the second comparison are <code>wH02 = c(-1/3, +2/3, -1/3)</code>.</p>
<p>Before writing these into a hypothesis matrix, we also define the estimated quantity for the intercept term. The intercept parameter estimates the mean across all conditions:</p>
<p><span class="math display">\[\begin{align}
\alpha = \frac{\mu_1 + \mu_2 + \mu_3}{3} \\
\alpha = \frac{1}{3} \mu_1 + \frac{1}{3}\mu_2 + \frac{1}{3}\mu_3
\end{align}\]</span></p>
<p>This estimate has weights of <span class="math inline">\(1/3\)</span> for all condition means.
The weights from all three model parameters that were defined are now combined and written into a matrix that we refer to as the <em>hypothesis matrix</em> (<code>Hc</code>):</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb313-1"><a href="ch-contr.html#cb313-1" aria-hidden="true"></a>HcSum &lt;-</span>
<span id="cb313-2"><a href="ch-contr.html#cb313-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">rbind</span>(<span class="dt">cH00 =</span> <span class="kw">c</span>(<span class="dt">adjectives =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">nouns =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">verbs =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>),</span>
<span id="cb313-3"><a href="ch-contr.html#cb313-3" aria-hidden="true"></a>        <span class="dt">cH01 =</span> <span class="kw">c</span>(<span class="dt">adjectives =</span> <span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">nouns =</span> <span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">verbs =</span> <span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>),</span>
<span id="cb313-4"><a href="ch-contr.html#cb313-4" aria-hidden="true"></a>        <span class="dt">cH02 =</span> <span class="kw">c</span>(<span class="dt">adjectives =</span> <span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">nouns =</span> <span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">verbs =</span> <span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>))</span>
<span id="cb313-5"><a href="ch-contr.html#cb313-5" aria-hidden="true"></a><span class="kw">fractions</span>(<span class="kw">t</span>(HcSum))</span></code></pre></div>
<pre><code>##            cH00 cH01 cH02
## adjectives  1/3  2/3 -1/3
## nouns       1/3 -1/3  2/3
## verbs       1/3 -1/3 -1/3</code></pre>
<p>Each set of weights is first entered as a row into the matrix (command <code>rbind()</code>). We switch rows and columns of the matrix for easier readability using the command <code>t()</code> (this transposes the matrix). The command <code>fractions()</code> from the <code>MASS</code> package turns the decimals into fractions to improve readability.</p>
<p>Now that the condition weights have been written into the hypothesis matrix, the third step of the procedure is implemented: a matrix operation called the <em>generalized matrix inverse</em><a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a> is used to obtain the contrast matrix that is needed to estimate these comparisons in a linear model.</p>
<p>Use the function <code>ginv2()</code> from the <code>bcogsci</code> package for the next step. It is similar to <code>ginv()</code> from <code>MASS</code> but provides nicer formatting of the output.</p>
<p>Applying the generalized inverse to the hypothesis matrix results in the new matrix <code>XcSum</code>. This is the contrast matrix <span class="math inline">\(X_c\)</span> that estimates exactly those comparisons that were specified earlier:</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb315-1"><a href="ch-contr.html#cb315-1" aria-hidden="true"></a>(XcSum &lt;-<span class="st"> </span><span class="kw">ginv2</span>(HcSum))</span></code></pre></div>
<pre><code>##            cH00 cH01 cH02
## adjectives  1    1    0  
## nouns       1    0    1  
## verbs       1   -1   -1</code></pre>
<p>This contrast matrix corresponds exactly to the sum contrasts described above. In the case of the sum contrast, the contrast matrix looks very different from the hypothesis matrix. The contrast matrix in sum contrasts codes with <span class="math inline">\(+1\)</span> the condition that is to be compared to the grand mean. The condition that is never compared to the grand mean is coded as <span class="math inline">\(-1\)</span>. Without knowing the relationship between the hypothesis matrix and the contrast matrix, the meaning of the coefficients is completely opaque.</p>
<p>To verify this custom-made contrast matrix, it is compared to the sum contrast matrix as generated by the R function <code>contr.sum()</code> in the <code>stats</code> package. The resulting contrast matrix is identical to the result when adding the intercept term, a column of ones, to the contrast matrix:</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb317-1"><a href="ch-contr.html#cb317-1" aria-hidden="true"></a><span class="kw">fractions</span>(<span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">contr.sum</span>(<span class="dv">3</span>)))</span></code></pre></div>
<pre><code>##   [,1] [,2] [,3]
## 1  1    1    0  
## 2  1    0    1  
## 3  1   -1   -1</code></pre>
<p>In order to estimate model parameters, step four in our procedure involves assigning sum contrasts to the factor <span class="math inline">\(F\)</span> in our example data, and fitting a (Bayesian) linear model. This allows us to estimate the regression coefficients associated with each contrast. We compare these to the data shown above (Table <a href="ch-contr.html#tab:cTab2Means">6.2</a>) to test whether the regression coefficients actually correspond to the differences of condition means, as intended. To define the contrast, it is necessary to remove the intercept term, as this is automatically added by the modeling function <code>brm()</code>.</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb319-1"><a href="ch-contr.html#cb319-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts2<span class="op">$</span>F) &lt;-<span class="st"> </span>XcSum[, <span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]</span>
<span id="cb319-2"><a href="ch-contr.html#cb319-2" aria-hidden="true"></a>fit_Sum &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span>F,</span>
<span id="cb319-3"><a href="ch-contr.html#cb319-3" aria-hidden="true"></a>               <span class="dt">data =</span> df_contrasts2,</span>
<span id="cb319-4"><a href="ch-contr.html#cb319-4" aria-hidden="true"></a>               <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb319-5"><a href="ch-contr.html#cb319-5" aria-hidden="true"></a>               <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">500</span>, <span class="dv">100</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb319-6"><a href="ch-contr.html#cb319-6" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb319-7"><a href="ch-contr.html#cb319-7" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb320"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb320-1"><a href="ch-contr.html#cb320-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_Sum)</span></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept    450.4      6.89 436.6 464.3
## FcH01        -49.2      9.72 -68.0 -28.9
## FcH02         49.0      9.78  29.2  68.7</code></pre>
<p>The (Bayesian) linear model regression coefficients show the grand mean response time of <span class="math inline">\(450\)</span> ms in the intercept. Remember that the first regression coefficient <code>FcH01</code> was designed to estimate the extent to which adjectives are responded to faster than the grand mean. The regression coefficient <code>FcH01</code> (<code>Estimate</code>) of <span class="math inline">\(-50\)</span> reflects the difference between adjectives (<span class="math inline">\(400\)</span> ms) and the grand mean of <span class="math inline">\(450\)</span> ms. The second estimate of interest tells us the extent to which response times for nouns differ from the grand mean. The fact that the second regression coefficient <code>FcH02</code> is close to <span class="math inline">\(50\)</span> indicates that response times for nouns are (<span class="math inline">\(500\)</span> ms) slower than the grand mean of <span class="math inline">\(450\)</span> ms. Although the nouns are estimated to have <span class="math inline">\(50\)</span> ms longer reading times than the grand mean, the reading times for adjectives are <span class="math inline">\(50\)</span> ms faster than the grand mean.</p>
<p>We have now not only derived contrasts, parameter estimates, and comparisons for the sum contrast, we have also used a powerful and highly general procedure that is used to generate contrasts for many kinds of different comparisons and experimental designs.</p>
</div>
<div id="generating-contrasts-the-hypr-package" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Generating contrasts: The  <code>hypr</code> package<a href="ch-contr.html#generating-contrasts-the-hypr-package" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To work with the four-step procedure, i.e., to flexibly design contrasts to estimate specific comparisons, we have developed the R package <code>hypr</code> <span class="citation">(Rabe et al. <a href="#ref-rabe2020hypr" role="doc-biblioref">2020</a>)</span>. This package allows the researcher to specify the desired comparisons, and based on these comparisons, it automatically generates contrast matrices that allow us to estimate these comparisons in linear models. The functions available in this package thus considerably simplify the implementation of the four-step procedure outlined above. Because <code>hypr</code> was originally written with the frequentist framework in mind, the comparisons are expressed as null hypotheses. In the Bayesian framework, these should be treated as comparisons between (bundles of) condition means.
To illustrate the functionality of the <code>hypr</code> package, we will use the two comparisons that we had defined and analyzed in the previous section:</p>
<p><span class="math display">\[\begin{equation}
\beta_1 = \mu_1 - \frac{\mu_1+\mu_2+\mu_3}{3} = \mu_1 - GM
\end{equation}\]</span></p>
<p>
and</p>
<p><span class="math display">\[\begin{equation}
\beta_2 = \mu_2 - \frac{\mu_1+\mu_2+\mu_3}{3} = \mu_2 - GM
\end{equation}\]</span></p>
<p>These estimates are effectively comparisons between condition means or between bundles of condition means. That is, both <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> are compared to the grand mean. These two comparisons can be directly entered into R using the <code>hypr()</code> function from the <code>hypr</code> package.
To do so, we use some labels to indicate factor levels. E.g., <code>adjectives</code>, <code>nouns</code>, and <code>verbs</code> can represent factor levels <span class="math inline">\(\mu_1\)</span>, <span class="math inline">\(\mu_2\)</span>, and <span class="math inline">\(\mu_3\)</span>. The first comparison specifies that <span class="math inline">\(\mu_1\)</span> is compared to <span class="math inline">\(\frac{\mu_1+\mu_2+\mu_3}{3}\)</span>. This can be written as a formula in R: <code>adjectives ~ (adjectives + nouns + verbs)/3</code>. The second comparison is that <span class="math inline">\(\mu_2\)</span> is compared to <span class="math inline">\(\frac{\mu_1+\mu_2+\mu_3}{3}\)</span>, which can be written as <code>nouns ~ (adjectives + nouns + verbs)/3</code>.</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb322-1"><a href="ch-contr.html#cb322-1" aria-hidden="true"></a>HcSum &lt;-<span class="st"> </span><span class="kw">hypr</span>(<span class="dt">b1 =</span> adjectives <span class="op">~</span><span class="st"> </span>(adjectives <span class="op">+</span><span class="st"> </span>nouns <span class="op">+</span><span class="st"> </span>verbs) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>,</span>
<span id="cb322-2"><a href="ch-contr.html#cb322-2" aria-hidden="true"></a>              <span class="dt">b2 =</span> nouns <span class="op">~</span><span class="st"> </span>(adjectives <span class="op">+</span><span class="st"> </span>nouns <span class="op">+</span><span class="st"> </span>verbs) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>,</span>
<span id="cb322-3"><a href="ch-contr.html#cb322-3" aria-hidden="true"></a>              <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;adjectives&quot;</span>, <span class="st">&quot;nouns&quot;</span>, <span class="st">&quot;verbs&quot;</span>))</span>
<span id="cb322-4"><a href="ch-contr.html#cb322-4" aria-hidden="true"></a>HcSum</span></code></pre></div>
<pre><code>## hypr object containing 2 null hypotheses:
## H0.b1: 0 = (2*adjectives - nouns - verbs)/3
## H0.b2: 0 = (2*nouns - adjectives - verbs)/3
## 
## Call:
## hypr(b1 = ~2/3 * adjectives - 1/3 * nouns - 1/3 * verbs, b2 = ~2/3 * 
##     nouns - 1/3 * adjectives - 1/3 * verbs, levels = c(&quot;adjectives&quot;, 
## &quot;nouns&quot;, &quot;verbs&quot;))
## 
## Hypothesis matrix (transposed):
##            b1   b2  
## adjectives  2/3 -1/3
## nouns      -1/3  2/3
## verbs      -1/3 -1/3
## 
## Contrast matrix:
##            b1 b2
## adjectives  1  0
## nouns       0  1
## verbs      -1 -1</code></pre>
<p>The results show that the comparisons between condition means have been re-written into a form where <span class="math inline">\(0\)</span> is coded on the left side of the equation, and the condition means together with associated weights are written on the right side of the equation. This presentation makes it easy to see the weights of the condition means to code a certain comparison. The next part of the results shows the hypothesis matrix, which contains the weights from the condition means. Thus, <code>hypr</code> takes formulas coding comparisons between condition means as input, and automatically extracts the corresponding weights and encodes them into the hypothesis matrix. <code>hypr</code> moreover applies the generalized matrix inverse to obtain the contrast matrix from the hypothesis matrix. The different steps correspond exactly to the steps we had carried out manually in the preceding section. <code>hypr</code> automatically performs these steps for us. We can now extract the contrast matrix by a simple function call: </p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb324-1"><a href="ch-contr.html#cb324-1" aria-hidden="true"></a><span class="kw">contr.hypothesis</span>(HcSum)</span></code></pre></div>
<pre><code>##            b1 b2
## adjectives  1  0
## nouns       0  1
## verbs      -1 -1
## attr(,&quot;class&quot;)
## [1] &quot;hypr_cmat&quot; &quot;matrix&quot;    &quot;array&quot;</code></pre>
<p>We can assign this contrast to our factor as we did before.</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb326-1"><a href="ch-contr.html#cb326-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts2<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcSum)</span></code></pre></div>
<p>Now, we could again run the same model. However, since the contrast matrix is now the same as used before, the results would also be exactly the same, and we therefore skip the model fitting for brevity.</p>
<p>The <code>hypr</code> package can be used to create contrasts for Bayesian models, where the focus lies on estimation of contrasts that code comparisons between condition means or bundles of condition means. (Of course, one can use contrast coding for carrying out hypothesis tests using the Bayes factor; see chapter <a href="ch-bf.html#ch-bf">13</a>; in this case, the priors specified can lead to very different conclusions.) Thus, the comparison that one specifies implies the estimation of a difference between condition means or bundles of condition means. We see this in the output of the <code>hypr()</code> function (see the first section of the results); these formulate the comparison in a way that also illustrates the estimation of model parameters. That is, the comparison (expressed in the <code>hypr</code> package’s syntax) <span class="math inline">\(\mu_1 \sim \frac{\mu_1+\mu_2+\mu_3}{3}\)</span> corresponds to a parameter estimate of <code>b1 = 2/3*m1 - 1/3*m2 - 1/3*m3</code>, where <span class="math inline">\(m1\)</span> to <span class="math inline">\(m3\)</span> are the means for each of the conditions. The resulting contrasts will then allow us to estimate the specified differences between condition means or bundles of condition means.</p>
</div>
</div>
<div id="sec-4levelFactor" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Other types of contrasts: illustration with a factor of four levels<a href="ch-contr.html#sec-4levelFactor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here, we introduce repeated difference, Helmert, and polynomial contrasts.<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a></p>
<p>Consider an experiment with one between-subject factor with four levels (the discussion below does not depend on the factor being between-subjects–the same logic will hold for within-subjects designs). We load a corresponding data set, which contains simulated data containing response times with a four-level between-subject factor.
The sample sizes for each level and the means and standard errors are shown in Table <a href="ch-contr.html#tab:cTab3Means">6.3</a>, and the means and standard errors are also shown graphically in Figure <a href="ch-contr.html#fig:helmertsimdatFig">6.2</a>.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb327-1"><a href="ch-contr.html#cb327-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_contrasts3&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:helmertsimdatFig"></span>
<img src="bayescogsci_files/figure-html/helmertsimdatFig-1.svg" alt="The means and error bars (showing standard errors) for a simulated data set with one between-subjects factor with four levels." width="672" />
<p class="caption">
FIGURE 6.2: The means and error bars (showing standard errors) for a simulated data set with one between-subjects factor with four levels.
</p>
</div>
<table>
<caption><span id="tab:cTab3Means">TABLE 6.3: </span>Summary statistics per condition for the simulated data.</caption>
<thead>
<tr class="header">
<th align="left">Factor</th>
<th align="right">N data</th>
<th align="right">Est. means</th>
<th align="right">Std. dev.</th>
<th align="right">Std. errors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">F1</td>
<td align="right"><span class="math inline">\(5\)</span></td>
<td align="right"><span class="math inline">\(10.0\)</span></td>
<td align="right"><span class="math inline">\(10.0\)</span></td>
<td align="right"><span class="math inline">\(4.5\)</span></td>
</tr>
<tr class="even">
<td align="left">F2</td>
<td align="right"><span class="math inline">\(5\)</span></td>
<td align="right"><span class="math inline">\(20.0\)</span></td>
<td align="right"><span class="math inline">\(10.0\)</span></td>
<td align="right"><span class="math inline">\(4.5\)</span></td>
</tr>
<tr class="odd">
<td align="left">F3</td>
<td align="right"><span class="math inline">\(5\)</span></td>
<td align="right"><span class="math inline">\(10.0\)</span></td>
<td align="right"><span class="math inline">\(10.0\)</span></td>
<td align="right"><span class="math inline">\(4.5\)</span></td>
</tr>
<tr class="even">
<td align="left">F4</td>
<td align="right"><span class="math inline">\(5\)</span></td>
<td align="right"><span class="math inline">\(40.0\)</span></td>
<td align="right"><span class="math inline">\(10.0\)</span></td>
<td align="right"><span class="math inline">\(4.5\)</span></td>
</tr>
</tbody>
</table>
<p>We assume that the four factor levels <span class="math inline">\(F1\)</span> to <span class="math inline">\(F4\)</span> reflect levels of word frequency, including the levels <code>low</code>, <code>medium-low</code>, <code>medium-high</code>, and <code>high</code> frequency words, and that the dependent variable reflects response time.</p>
<p>Qualitatively, the simulated pattern of results is similar to empirically observed values for word frequency effects on single fixation durations in eye tracking <span class="citation">(see Figure 5.4 in Heister, Würzner, and Kliegl <a href="#ref-heister2012analysing" role="doc-biblioref">2012</a>)</span>.
Normally, one may expect the pattern to be monotonically decreasing, i.e., higher word frequency should lead to faster reading times. However, we choose this more complex pattern because we think it may be helpful in understanding how polynomial and monotonic contrasts work (see below).</p>
<div id="repeatedcontrasts" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span>  Repeated contrasts<a href="ch-contr.html#repeatedcontrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Arguably, the most popular contrast psychologists and psycholinguists are interested in is the comparison between neighboring levels of a factor. This type of contrast is called the repeated contrast. In our example, our research question might be whether the frequency level <code>low</code> leads to slower response times than frequency level <code>medium-low</code>, whether frequency level <code>medium-low</code> leads to slower response times than frequency level <code>medium-high</code>, and whether frequency level <code>medium-high</code> leads to slower response times than frequency level <code>high</code>.</p>
<p>Repeated contrasts are used to implement these comparisons. Consider first how to derive the contrast matrix for repeated contrasts, starting out by specifying the comparisons that are to be estimated. Importantly, this again applies the general strategy of how to translate (any) comparisons between groups or conditions into a set of contrasts, yielding a powerful tool of great value in many research settings. We follow the four-step procedure outlined above.</p>
<p>The first step is to specify our comparisons, and to write them down in a way such that their weights can be extracted easily. For a four-level factor, the three comparisons are:</p>
<p><span class="math display">\[\begin{equation}
\beta_{2-1} = -1 \cdot \mu_1 + 1 \cdot \mu_2 + 0 \cdot \mu_3 + 0 \cdot \mu_4
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\beta_{3-2} = 0 \cdot \mu_1 - 1 \cdot \mu_2 + 1 \cdot \mu_3 + 0 \cdot \mu_4
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\beta_{4-3} = 0 \cdot \mu_1 + 0 \cdot \mu_2 - 1 \cdot \mu_3 + 1 \cdot \mu_4
\end{equation}\]</span></p>
<p>Here, the <span class="math inline">\(\mu_x\)</span> are the mean response times in condition <span class="math inline">\(x\)</span>. Each regression coefficient gives weights to the different condition means. For example, the first estimate (<span class="math inline">\(\beta_{2-1}\)</span>) estimates the difference between condition mean for <span class="math inline">\(F2\)</span> (<span class="math inline">\(\mu_2\)</span>) minus the condition mean for <span class="math inline">\(F1\)</span> (<span class="math inline">\(\mu_1\)</span>), but ignores condition means for <span class="math inline">\(F3\)</span> and <span class="math inline">\(F4\)</span> (<span class="math inline">\(\mu_3\)</span>, <span class="math inline">\(\mu_4\)</span>). <span class="math inline">\(\mu_1\)</span> has a weight of <span class="math inline">\(-1\)</span>, <span class="math inline">\(\mu_2\)</span> has a weight of <span class="math inline">\(+1\)</span>, and <span class="math inline">\(\mu_3\)</span> and <span class="math inline">\(\mu_4\)</span> have weights of <span class="math inline">\(0\)</span>.</p>
<p>We can write these comparisons into <code>hypr</code>:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb328-1"><a href="ch-contr.html#cb328-1" aria-hidden="true"></a>HcRep &lt;-<span class="st"> </span><span class="kw">hypr</span>(<span class="dt">c2vs1 =</span> F2 <span class="op">~</span><span class="st"> </span>F1,</span>
<span id="cb328-2"><a href="ch-contr.html#cb328-2" aria-hidden="true"></a>              <span class="dt">c3vs2 =</span> F3 <span class="op">~</span><span class="st"> </span>F2,</span>
<span id="cb328-3"><a href="ch-contr.html#cb328-3" aria-hidden="true"></a>              <span class="dt">c4vs3 =</span> F4 <span class="op">~</span><span class="st"> </span>F3,</span>
<span id="cb328-4"><a href="ch-contr.html#cb328-4" aria-hidden="true"></a>              <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;F1&quot;</span>, <span class="st">&quot;F2&quot;</span>, <span class="st">&quot;F3&quot;</span>, <span class="st">&quot;F4&quot;</span>))</span>
<span id="cb328-5"><a href="ch-contr.html#cb328-5" aria-hidden="true"></a>HcRep</span></code></pre></div>
<pre><code>## hypr object containing 3 null hypotheses:
## H0.c2vs1: 0 = F2 - F1
## H0.c3vs2: 0 = F3 - F2
## H0.c4vs3: 0 = F4 - F3
## 
## Call:
## hypr(c2vs1 = ~F2 - F1, c3vs2 = ~F3 - F2, c4vs3 = ~F4 - F3, levels = c(&quot;F1&quot;, 
## &quot;F2&quot;, &quot;F3&quot;, &quot;F4&quot;))
## 
## Hypothesis matrix (transposed):
##    c2vs1 c3vs2 c4vs3
## F1 -1     0     0   
## F2  1    -1     0   
## F3  0     1    -1   
## F4  0     0     1   
## 
## Contrast matrix:
##    c2vs1 c3vs2 c4vs3
## F1 -3/4  -1/2  -1/4 
## F2  1/4  -1/2  -1/4 
## F3  1/4   1/2  -1/4 
## F4  1/4   1/2   3/4</code></pre>
<p>The hypothesis matrix shows exactly the weights that we had written down above. Moreover, we see the contrast matrix. In the case of the repeated contrast, the contrast matrix again looks very different from the hypothesis matrix. In this case, the contrast matrix looks a lot less intuitive than the hypothesis matrix, and if one did not know the associated hypothesis matrix, it seems unclear what the contrast matrix would actually test. To verify this custom-made contrast matrix, we compare it to the repeated contrast matrix as generated by the R function <code>contr.sdif()</code> in the <code>MASS</code> package <span class="citation">(Ripley <a href="#ref-R-MASS" role="doc-biblioref">2023</a>)</span>. The resulting contrast matrix is identical to our result:</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb330-1"><a href="ch-contr.html#cb330-1" aria-hidden="true"></a><span class="kw">fractions</span>(<span class="kw">contr.sdif</span>(<span class="dv">4</span>))</span></code></pre></div>
<pre><code>##   2-1  3-2  4-3 
## 1 -3/4 -1/2 -1/4
## 2  1/4 -1/2 -1/4
## 3  1/4  1/2 -1/4
## 4  1/4  1/2  3/4</code></pre>
<p>We can thus use either approach (<code>hypr()</code> or  <code>contr.sdif()</code>) to obtain the contrast matrix in this case.
Next, we apply the repeated contrasts to the factor <span class="math inline">\(F\)</span> in the example data and run a linear model. This allows us to estimate the regression coefficients associated with each contrast. These are compared to the data in Figure <a href="ch-contr.html#fig:helmertsimdatFig">6.2</a> to test whether the regression coefficients actually correspond to the differences between successive condition means, as intended.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb332-1"><a href="ch-contr.html#cb332-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcRep)</span>
<span id="cb332-2"><a href="ch-contr.html#cb332-2" aria-hidden="true"></a>fit_Rep &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span>F,</span>
<span id="cb332-3"><a href="ch-contr.html#cb332-3" aria-hidden="true"></a>               <span class="dt">data =</span> df_contrasts3,</span>
<span id="cb332-4"><a href="ch-contr.html#cb332-4" aria-hidden="true"></a>               <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb332-5"><a href="ch-contr.html#cb332-5" aria-hidden="true"></a>               <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb332-6"><a href="ch-contr.html#cb332-6" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb332-7"><a href="ch-contr.html#cb332-7" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb333"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb333-1"><a href="ch-contr.html#cb333-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_Rep)</span></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5 Q97.5
## Intercept    19.96      2.54  15.02 25.03
## Fc2vs1       10.00      7.01  -3.91 24.12
## Fc3vs2       -9.77      7.09 -24.03  4.55
## Fc4vs3       29.41      6.96  15.86 42.81</code></pre>
<p>The results show that as expected, the regression coefficients reflect the differences that were of interest: the regression coefficient (<code>Estimate</code>) <code>Fc2vs1</code> has a value of approximately <span class="math inline">\(10\)</span>, which corresponds to the difference between the condition mean for <span class="math inline">\(F2\)</span> (<span class="math inline">\(20\)</span>) minus the condition mean for <span class="math inline">\(F1\)</span> (<span class="math inline">\(10\)</span>), i.e., <span class="math inline">\(20 - 10 = 10\)</span>. Likewise, the regression coefficient <code>Fc3vs2</code> has a value of approximately <span class="math inline">\(-10\)</span>, which corresponds to the difference between the condition mean for <span class="math inline">\(F3\)</span> (<span class="math inline">\(10\)</span>) minus the condition mean for <span class="math inline">\(F2\)</span> (<span class="math inline">\(20\)</span>), i.e., <span class="math inline">\(10 - 20 = -10\)</span>. Finally, the regression coefficient <code>Fc4vs3</code> has a value of roughly <span class="math inline">\(30\)</span>, which reflects the difference between condition <span class="math inline">\(F4\)</span> (<span class="math inline">\(40\)</span>) minus condition <span class="math inline">\(F3\)</span> (<span class="math inline">\(10\)</span>), i.e., <span class="math inline">\(40 - 10 = 30\)</span>. Thus, the regression coefficients estimate differences between successive or neighboring condition means.</p>
</div>
<div id="helmertcontrasts" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span>  Helmert contrasts<a href="ch-contr.html#helmertcontrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another common contrast is the Helmert contrast. In a Helmert contrast for our four-level factor, the first contrast compares level <span class="math inline">\(F1\)</span> versus <span class="math inline">\(F2\)</span>. The second contrast compares level <span class="math inline">\(F3\)</span> to the average of the first two, i.e., <code>F3 ~ (F1+F2)/2</code>. The third contrast then compares level <span class="math inline">\(F4\)</span> to the average of the first three. We can code this contrast in <code>hypr</code>:</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb335-1"><a href="ch-contr.html#cb335-1" aria-hidden="true"></a>HcHel &lt;-<span class="st"> </span><span class="kw">hypr</span>(<span class="dt">b1 =</span> F2 <span class="op">~</span><span class="st"> </span>F1,</span>
<span id="cb335-2"><a href="ch-contr.html#cb335-2" aria-hidden="true"></a>              <span class="dt">b2 =</span> F3 <span class="op">~</span><span class="st"> </span>(F1 <span class="op">+</span><span class="st"> </span>F2) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>,</span>
<span id="cb335-3"><a href="ch-contr.html#cb335-3" aria-hidden="true"></a>              <span class="dt">b3 =</span> F4 <span class="op">~</span><span class="st"> </span>(F1 <span class="op">+</span><span class="st"> </span>F2 <span class="op">+</span><span class="st"> </span>F3) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>,</span>
<span id="cb335-4"><a href="ch-contr.html#cb335-4" aria-hidden="true"></a>              <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;F1&quot;</span>, <span class="st">&quot;F2&quot;</span>, <span class="st">&quot;F3&quot;</span>, <span class="st">&quot;F4&quot;</span>))</span>
<span id="cb335-5"><a href="ch-contr.html#cb335-5" aria-hidden="true"></a>HcHel</span></code></pre></div>
<pre><code>## hypr object containing 3 null hypotheses:
## H0.b1: 0 = F2 - F1
## H0.b2: 0 = F3 - 1/2*F1 - 1/2*F2
## H0.b3: 0 = F4 - 1/3*F1 - 1/3*F2 - 1/3*F3
## 
## Call:
## hypr(b1 = ~F2 - F1, b2 = ~F3 - 1/2 * F1 - 1/2 * F2, b3 = ~F4 - 
##     1/3 * F1 - 1/3 * F2 - 1/3 * F3, levels = c(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;, 
## &quot;F4&quot;))
## 
## Hypothesis matrix (transposed):
##    b1   b2   b3  
## F1   -1 -1/2 -1/3
## F2    1 -1/2 -1/3
## F3    0    1 -1/3
## F4    0    0    1
## 
## Contrast matrix:
##    b1   b2   b3  
## F1 -1/2 -1/3 -1/4
## F2  1/2 -1/3 -1/4
## F3    0  2/3 -1/4
## F4    0    0  3/4</code></pre>
<p>The classical Helmert contrast coded by the function  <code>contr.helmert()</code> yields a similar but slightly different result:</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb337-1"><a href="ch-contr.html#cb337-1" aria-hidden="true"></a><span class="kw">contr.helmert</span>(<span class="dv">4</span>)</span></code></pre></div>
<pre><code>##   [,1] [,2] [,3]
## 1   -1   -1   -1
## 2    1   -1   -1
## 3    0    2   -1
## 4    0    0    3</code></pre>
<p>These contrasts are scaled versions of our custom Helmert contrast. I.e., the first column of our custom Helmert contrast has to be multiplied by 2 to get the classical version, the second column has to be multiplied by 3, and the fourth column has to be multiplied by 4 to get to our custom Helmert contrast. In our opinion, the custom Helmert contrast defined using the <code>hypr</code> function is more appropriate and intuitive to use. Probably the only reason the classical Helmert contrast uses these scaled differences is that the rescaling yields an easier contrast matrix, which consists of integers rather than fractions. The estimates from our custom Helmert contrast seem much more relevant in Bayesian approaches today. This is because in our custom Helmert contrast, the coefficients estimate the differences between (groups of) conditions (rather than scaled versions of these differences). That means that we can set the priors intuitively (as priors on differences between (groups of) conditions - rather than on scaled differences) and that we can accordingly interpret the posterior much more straightforwardly. We apply the custom Helmert contrast here:</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb339-1"><a href="ch-contr.html#cb339-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcHel)</span>
<span id="cb339-2"><a href="ch-contr.html#cb339-2" aria-hidden="true"></a>fit_Hel &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span>F,</span>
<span id="cb339-3"><a href="ch-contr.html#cb339-3" aria-hidden="true"></a>               <span class="dt">data =</span> df_contrasts3,</span>
<span id="cb339-4"><a href="ch-contr.html#cb339-4" aria-hidden="true"></a>               <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb339-5"><a href="ch-contr.html#cb339-5" aria-hidden="true"></a>               <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb339-6"><a href="ch-contr.html#cb339-6" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb339-7"><a href="ch-contr.html#cb339-7" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb340"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb340-1"><a href="ch-contr.html#cb340-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_Hel)</span></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5 Q97.5
## Intercept    20.07      2.43  15.24 25.07
## Fb1           9.69      7.02  -4.17 23.84
## Fb2          -4.85      6.11 -17.19  7.68
## Fb3          26.32      5.90  14.55 37.69</code></pre>
<p>When we fit the Bayesian model using our custom Helmert contrast, we can see that the estimates reflect the comparisons outlined above. The first estimate <code>Fb1</code> has a value of roughly <span class="math inline">\(10\)</span>, reflecting the difference between conditions <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>. The second estimate <code>Fb2</code> has a value of <span class="math inline">\(5\)</span>, which reflects the difference between condition <span class="math inline">\(F3\)</span> (<span class="math inline">\(10\)</span>) and the average of the first two conditions (<span class="math inline">\((10+20)/2 = 15\)</span>). The estimate <code>Fb3</code> reflects the difference between <span class="math inline">\(F4\)</span> (<span class="math inline">\(40\)</span>) minus the average of the first three, which is <span class="math inline">\((10+20+10)/3 = 13.3\)</span>, and is thus <span class="math inline">\(40-13.3 = 26.7\)</span>.</p>
<p>Here, we can see that the generalized matrix inverse can be used to create contrasts that are not exactly equivalent to the ones provided by R per default (i.e., here <code>contr.helmert()</code>). Another example for this is discussed in online section <a href="regression-models-with-brms---extended.html#app-cTreatGM">A.8</a>, which implements a treatment contrast, where the intercept captures the grand mean.</p>
</div>
<div id="contrasts-in-linear-regression-analysis-the-design-or-model-matrix" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Contrasts in linear regression analysis: The design or  model matrix<a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have discussed how different contrasts are created from the hypothesis matrix. What we have not treated in detail is how exactly contrasts are used in a linear model. Here, we will see that the contrasts for a factor in a linear model are just the same thing as continuous  numeric predictors (i.e.,  covariates) in a linear/multiple regression analysis. That is, contrasts are the way to encode  discrete factor levels into numeric predictor variables to use in linear/multiple regression analysis, by encoding which differences between factor levels are estimated.
The contrast matrix <span class="math inline">\(X_c\)</span> that we have looked at so far has one entry (row) for each experimental condition. For use in a linear model, the contrast matrix is coded into a design or model matrix <span class="math inline">\(X\)</span>, where each individual data point has one row. The  design matrix <span class="math inline">\(X\)</span> can be extracted using the function  <code>model.matrix()</code>:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb342-1"><a href="ch-contr.html#cb342-1" aria-hidden="true"></a><span class="co"># contrast matrix:</span></span>
<span id="cb342-2"><a href="ch-contr.html#cb342-2" aria-hidden="true"></a>(<span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcRep))</span></code></pre></div>
<pre><code>##    c2vs1 c3vs2 c4vs3
## F1 -0.75  -0.5 -0.25
## F2  0.25  -0.5 -0.25
## F3  0.25   0.5 -0.25
## F4  0.25   0.5  0.75
## attr(,&quot;class&quot;)
## [1] &quot;hypr_cmat&quot; &quot;matrix&quot;    &quot;array&quot;</code></pre>
<div class="sourceCode" id="cb344"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb344-1"><a href="ch-contr.html#cb344-1" aria-hidden="true"></a><span class="co"># design/model matrix:</span></span>
<span id="cb344-2"><a href="ch-contr.html#cb344-2" aria-hidden="true"></a>covars &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F, df_contrasts3)</span>
<span id="cb344-3"><a href="ch-contr.html#cb344-3" aria-hidden="true"></a>(covars &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(covars))</span></code></pre></div>
<pre><code>##    (Intercept) Fc2vs1 Fc3vs2 Fc4vs3
## 1            1  -0.75   -0.5  -0.25
## 2            1  -0.75   -0.5  -0.25
## 3            1  -0.75   -0.5  -0.25
## 4            1  -0.75   -0.5  -0.25
## 5            1  -0.75   -0.5  -0.25
## 6            1   0.25   -0.5  -0.25
## 7            1   0.25   -0.5  -0.25
## 8            1   0.25   -0.5  -0.25
## 9            1   0.25   -0.5  -0.25
## 10           1   0.25   -0.5  -0.25
## 11           1   0.25    0.5  -0.25
## 12           1   0.25    0.5  -0.25
## 13           1   0.25    0.5  -0.25
## 14           1   0.25    0.5  -0.25
## 15           1   0.25    0.5  -0.25
## 16           1   0.25    0.5   0.75
## 17           1   0.25    0.5   0.75
## 18           1   0.25    0.5   0.75
## 19           1   0.25    0.5   0.75
## 20           1   0.25    0.5   0.75</code></pre>
<p>For each of the <span class="math inline">\(20\)</span> subjects, four numbers are stored in this model matrix. They represent the three values of three predictor variables used to predict response times in the task. Indeed, this matrix is exactly the design matrix <span class="math inline">\(X\)</span> commonly used in multiple regression analysis, where each column represents one numeric predictor variable (covariate), and the first column codes the intercept term.</p>
<p>To further illustrate this, the covariates are extracted from this design matrix and stored separately as numeric predictor variables in the data frame:</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb346-1"><a href="ch-contr.html#cb346-1" aria-hidden="true"></a>df_contrasts3[, <span class="kw">c</span>(<span class="st">&quot;Fc2vs1&quot;</span>, <span class="st">&quot;Fc3vs2&quot;</span>, <span class="st">&quot;Fc4vs3&quot;</span>)] &lt;-<span class="st"> </span>covars[, <span class="dv">2</span><span class="op">:</span><span class="dv">4</span>]</span></code></pre></div>
<p>They are now used as numeric predictor variables in a multiple regression analysis:</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb347-1"><a href="ch-contr.html#cb347-1" aria-hidden="true"></a>fit_m3 &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span>Fc2vs1 <span class="op">+</span><span class="st"> </span>Fc3vs2 <span class="op">+</span><span class="st"> </span>Fc4vs3,</span>
<span id="cb347-2"><a href="ch-contr.html#cb347-2" aria-hidden="true"></a>              <span class="dt">data =</span> df_contrasts3,</span>
<span id="cb347-3"><a href="ch-contr.html#cb347-3" aria-hidden="true"></a>              <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb347-4"><a href="ch-contr.html#cb347-4" aria-hidden="true"></a>              <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb347-5"><a href="ch-contr.html#cb347-5" aria-hidden="true"></a>                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb347-6"><a href="ch-contr.html#cb347-6" aria-hidden="true"></a>                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb348"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb348-1"><a href="ch-contr.html#cb348-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_m3)</span></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5 Q97.5
## Intercept    20.00      2.46  15.15  24.9
## Fc2vs1        9.93      7.12  -4.09  24.1
## Fc3vs2       -9.56      7.10 -23.50   4.5
## Fc4vs3       29.24      6.95  14.60  42.3</code></pre>
<p>The results show that the regression coefficients are the same as in the contrast-based analysis shown in the previous section (on repeated contrasts). This demonstrates that contrasts serve to code discrete factor levels into a linear/multiple regression analysis by numerically encoding comparisons between specific condition means, or groups of condition means.</p>
</div>
<div id="polynomialContrasts" class="section level3 hasAnchor" number="6.3.4">
<h3><span class="header-section-number">6.3.4</span>  Polynomial contrasts<a href="ch-contr.html#polynomialContrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Polynomial contrasts are another option for analyzing factors. Suppose that we expect a  linear trend across conditions, where the response increases by a constant magnitude with each successive factor level. This could be the expectation when four levels of a factor reflect decreasing levels of word frequency (i.e., four factor levels: high, medium-high, medium-low, and low word frequency), where one expects the fastest response for high frequency words, and successively slower responses for lower word frequencies. The effect for each individual level of a factor (e.g., as coded via a repeated contrast) may not be strong enough for detecting it in the statistical model. Specifying a linear trend in a polynomial contrast (see effect <code>F.L</code> below) allows us to pool the whole increase (across all four factor levels) into a single coefficient for the linear trend, increasing statistical sensitivity for estimating/detecting the increase. Such a specification constrains the estimate to one interpretable parameter, e.g., a linear increase across factor levels. The larger the number of factor levels, the more parsimonious are polynomial contrasts compared to contrast-based specifications as introduced in the previous sections. Going beyond a linear trend, one may also have expectations about  quadratic trends (see the estimate for <code>F.Q</code> below). For example, one may expect an increase only among very low frequency words, but no difference between high and medium-high frequency words.</p>
<p>Here is an example for how to code polynomial contrasts for a four-level factor. In this case, one can estimate a linear (<code>F.L</code>), a quadratic (<code>F.Q</code>), and a cubic (<code>F.C</code>) trend. If more factor levels are present, then higher order trends can be estimated.</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb350-1"><a href="ch-contr.html#cb350-1" aria-hidden="true"></a>Xpol &lt;-<span class="st"> </span><span class="kw">contr.poly</span>(<span class="dv">4</span>)</span>
<span id="cb350-2"><a href="ch-contr.html#cb350-2" aria-hidden="true"></a>(<span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span>Xpol)</span></code></pre></div>
<pre><code>##          .L   .Q     .C
## [1,] -0.671  0.5 -0.224
## [2,] -0.224 -0.5  0.671
## [3,]  0.224 -0.5 -0.671
## [4,]  0.671  0.5  0.224</code></pre>
<div class="sourceCode" id="cb352"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb352-1"><a href="ch-contr.html#cb352-1" aria-hidden="true"></a>fit_Pol &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span>F,</span>
<span id="cb352-2"><a href="ch-contr.html#cb352-2" aria-hidden="true"></a>               <span class="dt">data =</span> df_contrasts3,</span>
<span id="cb352-3"><a href="ch-contr.html#cb352-3" aria-hidden="true"></a>               <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb352-4"><a href="ch-contr.html#cb352-4" aria-hidden="true"></a>               <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb352-5"><a href="ch-contr.html#cb352-5" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb352-6"><a href="ch-contr.html#cb352-6" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb353"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb353-1"><a href="ch-contr.html#cb353-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_Pol)</span></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept    20.04      2.43 15.24  24.9
## F.L          17.79      4.94  8.26  27.6
## F.Q           9.81      4.80  0.53  19.1
## F.C          13.26      5.01  3.60  23.4</code></pre>
<p>In this example, condition means increase across factor levels in a linear fashion: if one draws a linear regression line through all condition means, this line will increase towards the right. However, there may also be quadratic and cubic trends.</p>
<p>However, one difficulty with the <code>contr.poly()</code> function is that the scaling of the predictor variables is not very clear, making it difficult to define priors. Therefore, an alternative can be to use custom polynomial contrasts:</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb355-1"><a href="ch-contr.html#cb355-1" aria-hidden="true"></a>Xpol2           &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">linear =</span> <span class="kw">c</span>(<span class="dt">F1 =</span> <span class="dv">-3</span>,</span>
<span id="cb355-2"><a href="ch-contr.html#cb355-2" aria-hidden="true"></a>                                         <span class="dt">F2 =</span> <span class="dv">-1</span>,</span>
<span id="cb355-3"><a href="ch-contr.html#cb355-3" aria-hidden="true"></a>                                         <span class="dt">F3 =</span> <span class="dv">1</span>,</span>
<span id="cb355-4"><a href="ch-contr.html#cb355-4" aria-hidden="true"></a>                                         <span class="dt">F4 =</span> <span class="dv">3</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</span>
<span id="cb355-5"><a href="ch-contr.html#cb355-5" aria-hidden="true"></a>Xpol2<span class="op">$</span>quadratic &lt;-<span class="st"> </span>Xpol2<span class="op">$</span>linear<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(Xpol2<span class="op">$</span>linear<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb355-6"><a href="ch-contr.html#cb355-6" aria-hidden="true"></a>Xpol2<span class="op">$</span>cubic     &lt;-<span class="st"> </span>Xpol2<span class="op">$</span>linear<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(Xpol2<span class="op">$</span>linear<span class="op">^</span><span class="dv">3</span>)</span></code></pre></div>
<p>Because the linear and cubic trends are highly correlated, we orthogonalize the two:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb356-1"><a href="ch-contr.html#cb356-1" aria-hidden="true"></a>Xpol2<span class="op">$</span>cubic &lt;-<span class="st"> </span><span class="kw">resid</span>(<span class="kw">lm</span>(cubic <span class="op">~</span><span class="st"> </span>linear, Xpol2))</span>
<span id="cb356-2"><a href="ch-contr.html#cb356-2" aria-hidden="true"></a>(<span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(Xpol2))</span></code></pre></div>
<pre><code>##    linear quadratic cubic
## F1   -1.5         1  -0.3
## F2   -0.5        -1   0.9
## F3    0.5        -1  -0.9
## F4    1.5         1   0.3</code></pre>
<div class="sourceCode" id="cb358"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb358-1"><a href="ch-contr.html#cb358-1" aria-hidden="true"></a>fit_Pol2 &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span>F,</span>
<span id="cb358-2"><a href="ch-contr.html#cb358-2" aria-hidden="true"></a>               <span class="dt">data =</span> df_contrasts3,</span>
<span id="cb358-3"><a href="ch-contr.html#cb358-3" aria-hidden="true"></a>               <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb358-4"><a href="ch-contr.html#cb358-4" aria-hidden="true"></a>               <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb358-5"><a href="ch-contr.html#cb358-5" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb358-6"><a href="ch-contr.html#cb358-6" aria-hidden="true"></a>                         <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb359"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb359-1"><a href="ch-contr.html#cb359-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_Pol2)</span></code></pre></div>
<pre><code>##            Estimate Est.Error  Q2.5 Q97.5
## Intercept     20.03      2.49 15.01 24.96
## Flinear        8.00      2.15  3.68 12.41
## Fquadratic     4.91      2.48  0.05  9.86
## Fcubic         9.96      3.68  2.66 17.49</code></pre>
<p>The effects are now on interpretable scales.</p>
</div>
<div id="an-alternative-to-contrasts-monotonic-effects" class="section level3 hasAnchor" number="6.3.5">
<h3><span class="header-section-number">6.3.5</span> An alternative to contrasts:  Monotonic effects<a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An alternative to specifying contrasts to estimate specific comparisons between factor levels is monotonic effects <span class="citation">(<a href="https://paul-buerkner.github.io/brms/articles/brms_monotonic.html" class="uri" role="doc-biblioref">https://paul-buerkner.github.io/brms/articles/brms_monotonic.html</a>; Bürkner and Charpentier <a href="#ref-burkner2020modelling" role="doc-biblioref">2020</a>)</span>. This simply assumes that the dependent variable increases (or decreases) in a monotonic fashion across levels of an ordered factor. In this kind of analysis, one does not define contrasts specifying differences between (groups of) factor levels. Instead, one estimates one parameter which captures the average increase (or decrease) in the dependent variable associated with two neighboring factor levels. Moreover, one estimates the percentages of the overall increase (or decrease) that is associated with each of the differences between neighboring factor levels (i.e., similar to simple difference contrasts, but measured in percentage increase, and assuming monotonicity, i.e., that the same increase or decrease is present for all simple differences).</p>
<p>To implement a monotonic analysis, we first code the factor <span class="math inline">\(F\)</span> as being an ordered factor (i.e.,<code>ordered = TRUE</code>; remember that the default ordering is alphabetical and may need to be changed). Then, we specify that we want to estimate a monotonic effect of <span class="math inline">\(F\)</span> using the notation <code>mo(F)</code> in our call to <code>brms</code>:</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb361-1"><a href="ch-contr.html#cb361-1" aria-hidden="true"></a>df_contrasts3<span class="op">$</span>F &lt;-<span class="st"> </span><span class="kw">factor</span>(df_contrasts3<span class="op">$</span>F, <span class="dt">ordered =</span> <span class="ot">TRUE</span>)</span>
<span id="cb361-2"><a href="ch-contr.html#cb361-2" aria-hidden="true"></a>fit_mo &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">mo</span>(F),</span>
<span id="cb361-3"><a href="ch-contr.html#cb361-3" aria-hidden="true"></a>              <span class="dt">data =</span> df_contrasts3,</span>
<span id="cb361-4"><a href="ch-contr.html#cb361-4" aria-hidden="true"></a>              <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb361-5"><a href="ch-contr.html#cb361-5" aria-hidden="true"></a>              <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb361-6"><a href="ch-contr.html#cb361-6" aria-hidden="true"></a>                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb361-7"><a href="ch-contr.html#cb361-7" aria-hidden="true"></a>                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)))</span></code></pre></div>
<div class="sourceCode" id="cb362"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb362-1"><a href="ch-contr.html#cb362-1" aria-hidden="true"></a>fit_mo</span></code></pre></div>
<pre><code>## ...
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     9.19      4.32     0.13    16.98 1.00     1946     1700
## moF           9.56      2.43     4.70    14.25 1.00     1898     1862
## 
## Simplex Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## moF1[1]     0.20      0.14     0.01     0.51 1.00     2412     1339
## moF1[2]     0.11      0.10     0.00     0.39 1.00     2755     1766
## moF1[3]     0.69      0.16     0.32     0.94 1.00     2600     2186
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    11.77      2.34     8.20    17.16 1.00     2182     1802
## 
## ...</code></pre>
<p>The results show that there is an overall positive population-level effect of the factor <span class="math inline">\(F\)</span> with an estimate (<code>moF</code>) of <span class="math inline">\(9.56\)</span>, reflecting an average increase in the dependent variables of <span class="math inline">\(9.56\)</span> with each level of <span class="math inline">\(F\)</span>. The model summary shows estimates for the simplex parameters, which represent the ratios of the overall increase associated with <span class="math inline">\(F\)</span> that can be attributed to each of the differences between neighboring factor levels. The results show that most of the increase is associated with <code>moF1[3]</code>, i.e., with the last difference, reflecting the difference between <span class="math inline">\(F3\)</span> and <span class="math inline">\(F4\)</span>, whereas the other two differences (<code>moF1[1]</code>, reflecting the difference between <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>, and <code>moF1[2]</code>, reflecting the difference between <span class="math inline">\(F2\)</span> and <span class="math inline">\(F3\)</span>) are smaller. Comparing conditional effects (i.e., the condition means estimated by the model) between a model using polynomial contrasts and a model assuming monotonic effects makes it clear that the current model “forces” the effects to increase in a monotonic fashion; see Figure <a href="ch-contr.html#fig:condmopol">6.3</a>. </p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb364-1"><a href="ch-contr.html#cb364-1" aria-hidden="true"></a>ppoly &lt;-<span class="st"> </span><span class="kw">conditional_effects</span>(fit_Pol)</span>
<span id="cb364-2"><a href="ch-contr.html#cb364-2" aria-hidden="true"></a>pmon  &lt;-<span class="st"> </span><span class="kw">conditional_effects</span>(fit_mo)</span>
<span id="cb364-3"><a href="ch-contr.html#cb364-3" aria-hidden="true"></a><span class="kw">plot</span>(ppoly, <span class="dt">plot =</span> <span class="ot">FALSE</span>)[[<span class="dv">1</span>]] <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Polynomial contrasts&quot;</span>)</span>
<span id="cb364-4"><a href="ch-contr.html#cb364-4" aria-hidden="true"></a><span class="kw">plot</span>(pmon, <span class="dt">plot =</span> <span class="ot">FALSE</span>)[[<span class="dv">1</span>]] <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Monotonic effects&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:condmopol"></span>
<img src="bayescogsci_files/figure-html/condmopol-1.svg" alt="Conditional effects using the polynomial contrasts on the left side vs. assuming monotonic effects on the right side." width="48%" /><img src="bayescogsci_files/figure-html/condmopol-2.svg" alt="Conditional effects using the polynomial contrasts on the left side vs. assuming monotonic effects on the right side." width="48%" />
<p class="caption">
FIGURE 6.3: Conditional effects using the polynomial contrasts on the left side vs. assuming monotonic effects on the right side.
</p>
</div>
<p>This is regardless of the information provided in the data; see the posterior predictive checks in Figure <a href="ch-contr.html#fig:checkmopol">6.4</a>.</p>
<p></p>
<div class="sourceCode" id="cb365"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb365-1"><a href="ch-contr.html#cb365-1" aria-hidden="true"></a></span>
<span id="cb365-2"><a href="ch-contr.html#cb365-2" aria-hidden="true"></a><span class="kw">pp_check</span>(fit_Pol, <span class="dt">type =</span> <span class="st">&quot;violin_grouped&quot;</span>,</span>
<span id="cb365-3"><a href="ch-contr.html#cb365-3" aria-hidden="true"></a>         <span class="dt">group =</span> <span class="st">&quot;F&quot;</span>, <span class="dt">y_draw =</span> <span class="st">&quot;points&quot;</span>) <span class="op">+</span></span>
<span id="cb365-4"><a href="ch-contr.html#cb365-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)<span class="op">+</span></span>
<span id="cb365-5"><a href="ch-contr.html#cb365-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">55</span>, <span class="dv">105</span>)) <span class="op">+</span></span>
<span id="cb365-6"><a href="ch-contr.html#cb365-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Polynomial contrasts&quot;</span>)</span>
<span id="cb365-7"><a href="ch-contr.html#cb365-7" aria-hidden="true"></a><span class="kw">pp_check</span>(fit_mo, <span class="dt">type =</span> <span class="st">&quot;violin_grouped&quot;</span>,</span>
<span id="cb365-8"><a href="ch-contr.html#cb365-8" aria-hidden="true"></a>         <span class="dt">group =</span> <span class="st">&quot;F&quot;</span>, <span class="dt">y_draw =</span> <span class="st">&quot;points&quot;</span>) <span class="op">+</span></span>
<span id="cb365-9"><a href="ch-contr.html#cb365-9" aria-hidden="true"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>) <span class="op">+</span></span>
<span id="cb365-10"><a href="ch-contr.html#cb365-10" aria-hidden="true"></a><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">55</span>, <span class="dv">105</span>)) <span class="op">+</span></span>
<span id="cb365-11"><a href="ch-contr.html#cb365-11" aria-hidden="true"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Monotonic effects&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:checkmopol"></span>
<img src="bayescogsci_files/figure-html/checkmopol-1.svg" alt="Posterior predictive distributions by condition using the polynomial contrasts on the left side vs. assuming monotonic effects on the right side." width="48%" /><img src="bayescogsci_files/figure-html/checkmopol-2.svg" alt="Posterior predictive distributions by condition using the polynomial contrasts on the left side vs. assuming monotonic effects on the right side." width="48%" />
<p class="caption">
FIGURE 6.4: Posterior predictive distributions by condition using the polynomial contrasts on the left side vs. assuming monotonic effects on the right side.
</p>
</div>
<p>The monotonicity assumption is violated in the current data set, since the mean is larger in condition <span class="math inline">\(F2\)</span> than in condition <span class="math inline">\(F3\)</span>. The monotonic model thus assumes this (negative) difference is due to chance; see Figure <a href="ch-contr.html#fig:checkmopol">6.4</a>.</p>
<p>Estimating such monotonic effects provides an alternative to the contrast coding we treat in the rest of this chapter. It may be relevant when the specific differences between factor levels are not of interest, but when instead the goal is to estimate the overall monotonic effect of a factor and when this overall effect is not well approximated by a simple linear trend.</p>
</div>
</div>
<div id="nonOrthogonal" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> What makes a good set of contrasts?<a href="ch-contr.html#nonOrthogonal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For a factor with <span class="math inline">\(I\)</span> levels one can make only <span class="math inline">\(I-1\)</span> linearly independent comparisons within a single model. For example, in a design with one factor with two levels, only one comparison is possible (between the two factor levels). The reason for this is that the intercept is also estimated. More generally, if we have a factor with <span class="math inline">\(I_1\)</span> and another factor with <span class="math inline">\(I_2\)</span> levels, then the total number of conditions is <span class="math inline">\(I_1\times I_2 = \nu\)</span> (not <span class="math inline">\(I_1 + I_2\)</span>), which implies a maximum of <span class="math inline">\(\nu-1\)</span> contrasts.</p>
<p>For example, in a design with one factor with three levels, <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span>, in principle one could make three comparisons (<span class="math inline">\(A\)</span> vs. <span class="math inline">\(B\)</span>, <span class="math inline">\(A\)</span> vs. <span class="math inline">\(C\)</span>, <span class="math inline">\(B\)</span> vs. <span class="math inline">\(C\)</span>).
However, after defining an intercept, only two means can be compared (or two linear combinations of means). Therefore, for a factor with three levels, we define two comparisons within one statistical model.</p>
<p>One critical precondition for contrasts is that they implement different comparisons that are not  collinear, that is, that none of the contrasts can be generated from the other contrasts by linear combination. For example, the contrast <code>c1 = c(1, 2, 3)</code> can be generated from the contrast <code>c2 = c(3, 4, 5)</code> simply by computing <code>c2 - 2</code>. Therefore, contrasts c1 and c2 cannot be used simultaneously. That is, each contrast needs to encode some independent information about the data.</p>
<p>There are (at least) three criteria to decide what a good contrast is. First, <em>orthogonal contrasts</em> have advantages as they estimate mutually independent comparisons in the data <span class="citation">(see Dobson and Barnett <a href="#ref-dobson2011introduction" role="doc-biblioref">2011</a>, sec. 6.2.5, p. 91 for a detailed explanation of orthogonality)</span>. Second, it is crucial that contrasts are defined in a way such that they answer the research questions. One way to accomplish this second point is to use the hypothesis matrix to generate contrasts (e.g., via the <code>hypr</code> package), as this ensures that one uses contrasts that exactly estimate the comparisons of interest in a given study. Third, in the Bayesian context, the scaling of contrasts is important, and has to be considered when defining priors: the scaling of the priors has to be tailored to the scaling of each individual contrasts. In situations where we define the same prior for all coefficients of a factor, the scaling of the contrasts has to be set accordingly.</p>
<div id="centered-contrasts" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span>  Centered contrasts<a href="ch-contr.html#centered-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Contrasts are often constrained to be centered, such that the individual contrast coefficients <span class="math inline">\(c_i\)</span> for different factor levels <span class="math inline">\(i\)</span> sum to <span class="math inline">\(0\)</span>: <span class="math inline">\(\sum_{i = 1}^I c_i = 0\)</span>. This has advantages when estimating interactions with other factors or covariates (we discuss interactions between factors in the next chapter).
All contrasts discussed here are centered except for the treatment contrast, in which the contrast coefficients for each contrast do not sum to zero:</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb366-1"><a href="ch-contr.html#cb366-1" aria-hidden="true"></a><span class="kw">colSums</span>(<span class="kw">contr.treatment</span>(<span class="dv">4</span>))</span></code></pre></div>
<pre><code>## 2 3 4 
## 1 1 1</code></pre>
<p>Other contrasts, such as repeated contrasts, are centered and the contrast coefficients for each contrast sum to <span class="math inline">\(0\)</span>:</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb368-1"><a href="ch-contr.html#cb368-1" aria-hidden="true"></a><span class="kw">colSums</span>(<span class="kw">contr.sdif</span>(<span class="dv">4</span>))</span></code></pre></div>
<pre><code>## 2-1 3-2 4-3 
##   0   0   0</code></pre>
<p>The contrast coefficients mentioned above appear in the contrast matrix. The weights in the hypothesis matrix are always centered. This is also true for the treatment contrast. The reason is that they code comparisons between conditions or bundles of conditions.
The only exception are the weights for the intercept, which are all the same and together always sum to <span class="math inline">\(1\)</span> in the hypothesis matrix. This is done to ensure that when applying the generalized matrix inverse, the intercept results in a constant term with values of <span class="math inline">\(1\)</span> in the contrast matrix.
An important question concerns whether (or when) the intercept needs to be considered in the generalized matrix inversion, and whether (or when) it can be ignored. This question is closely related to orthogonal contrasts, a concept we turn to below.</p>
</div>
<div id="orthogonal-contrasts" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span>  Orthogonal contrasts<a href="ch-contr.html#orthogonal-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two centered contrasts <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> are orthogonal to each other if the following condition applies. Here, <span class="math inline">\(i\)</span> is the <span class="math inline">\(i\)</span>-th cell of the vector representing the contrast.</p>
<p><span class="math display">\[\begin{equation}
\sum_{i = 1}^I c_{1,i} \cdot c_{2,i} = 0
\end{equation}\]</span></p>
<p>Whether contrasts are orthogonal can often be determined easily by computing the correlation between two contrasts. Orthogonal contrasts have a correlation of <span class="math inline">\(0\)</span>.</p>
<p>For example, coding two factors in a <span class="math inline">\(2 \times 2\)</span> design (we return to this case in a section on designs with two factors below) using sum contrasts, these  sum contrasts and their  interaction are orthogonal to each other:</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb370-1"><a href="ch-contr.html#cb370-1" aria-hidden="true"></a>(Xsum &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dt">F1 =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">-1</span>),</span>
<span id="cb370-2"><a href="ch-contr.html#cb370-2" aria-hidden="true"></a>               <span class="dt">F2 =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dv">-1</span>),</span>
<span id="cb370-3"><a href="ch-contr.html#cb370-3" aria-hidden="true"></a>               <span class="dt">F1xF2 =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">-1</span>, <span class="dv">1</span>)))</span></code></pre></div>
<pre><code>##      F1 F2 F1xF2
## [1,]  1  1     1
## [2,]  1 -1    -1
## [3,] -1  1    -1
## [4,] -1 -1     1</code></pre>
<div class="sourceCode" id="cb372"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb372-1"><a href="ch-contr.html#cb372-1" aria-hidden="true"></a><span class="kw">cor</span>(Xsum)</span></code></pre></div>
<pre><code>##       F1 F2 F1xF2
## F1     1  0     0
## F2     0  1     0
## F1xF2  0  0     1</code></pre>
<p>
The correlations between the different contrasts (i.e., the off-diagonals) are exactly <span class="math inline">\(0\)</span>. Notice that sum contrasts coding one multi-level factor are not orthogonal to each other:</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb374-1"><a href="ch-contr.html#cb374-1" aria-hidden="true"></a><span class="kw">cor</span>(<span class="kw">contr.sum</span>(<span class="dv">4</span>))</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  1.0  0.5  0.5
## [2,]  0.5  1.0  0.5
## [3,]  0.5  0.5  1.0</code></pre>
<p>
Here, the correlations between individual contrasts, which appear in the off-diagonals, deviate from <span class="math inline">\(0\)</span>, indicating non-orthogonality. The same is also true for treatment and repeated contrasts:</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb376-1"><a href="ch-contr.html#cb376-1" aria-hidden="true"></a><span class="kw">cor</span>(<span class="kw">contr.sdif</span>(<span class="dv">4</span>))</span></code></pre></div>
<pre><code>##       2-1   3-2   4-3
## 2-1 1.000 0.577 0.333
## 3-2 0.577 1.000 0.577
## 4-3 0.333 0.577 1.000</code></pre>
<div class="sourceCode" id="cb378"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb378-1"><a href="ch-contr.html#cb378-1" aria-hidden="true"></a><span class="kw">cor</span>(<span class="kw">contr.treatment</span>(<span class="dv">4</span>))</span></code></pre></div>
<pre><code>##        2      3      4
## 2  1.000 -0.333 -0.333
## 3 -0.333  1.000 -0.333
## 4 -0.333 -0.333  1.000</code></pre>
<p>Orthogonality of contrasts plays a critical role when computing the  generalized inverse. In the inversion operation, orthogonal contrasts are converted independently from each other. That is, the presence or absence of another orthogonal contrast does not change the resulting weights. In fact, for orthogonal contrasts, applying the generalized matrix inverse to the hypothesis matrix simply furnishes a scaled version of the hypothesis matrix in the contrast matrix <span class="citation">(for mathematical details see Schad et al. <a href="#ref-schadHowCapitalizePriori2020" role="doc-biblioref">2020</a>)</span>.</p>
<p>In Bayesian models, scaling is always important, since we need to interpret the scale in order to define priors or interpret posteriors. Therefore, when working with contrasts in Bayesian models, the generalized matrix inverse is always a good procedure to use.</p>
</div>
<div id="the-role-of-the-intercept-in-non-centered-contrasts" class="section level3 hasAnchor" number="6.4.3">
<h3><span class="header-section-number">6.4.3</span> The role of the  intercept in  non-centered contrasts<a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A related question concerns whether the intercept needs to be considered when computing the generalized inverse for a contrast. This is of key importance when using the generalized matrix inverse to define contrasts: the resulting contrast matrix and also the definition of estimates can completely change between a situation where the intercept is explicitly considered or not considered, and can thus change the resulting estimates in possibly unintended ways. Thus, if the definition of the intercept is incorrect, the estimates of slopes may also be wrong.</p>
<p>More specifically, it turns out that considering the intercept is necessary for contrasts that are not centered. This is the case for treatment contrasts which are not centered; e.g., the treatment contrast for two factor levels <code>c1vs0 = c(0, 1)</code>: <span class="math inline">\(\sum_i c_i = 0 + 1 = 1\)</span>. One can actually show that the formula to determine whether contrasts are centered (i.e., <span class="math inline">\(\sum_i c_i = 0\)</span>) is the same formula as the formula to test whether a contrast is “orthogonal to the intercept.” Remember that for the intercept, all contrast coefficients are equal to one: <span class="math inline">\(c_{1,i} = 1\)</span> (here, <span class="math inline">\(c_{1,i}\)</span> indicates the vector of contrast coefficients associated with the intercept). We enter these contrast coefficient values into the formula testing whether a contrast is orthogonal to the intercept (here, <span class="math inline">\(c_{2,i}\)</span> indicates the vector of contrast coefficients associated with some contrast for which we want to test whether it is “orthogonal to the intercept”): <span class="math inline">\(\sum_i c_{1,i} \cdot c_{2,i} = \sum_i 1 \cdot c_{2,i} = \sum_i c_{2,i} = 0\)</span>. The resulting formula is: <span class="math inline">\(\sum_i c_{2,i} = 0\)</span>, which is exactly the constraint that needs to be satisfied for a contrast to be centered. Because of this analogy, treatment contrasts can be viewed to be `not orthogonal to the intercept.’ This means that the intercept needs to be considered when computing the generalized inverse for treatment contrasts. As we have discussed above, when the intercept is included in the hypothesis matrix, the weights for this intercept term should sum to one, as this yields a column of ones for the intercept term in the contrast matrix.</p>
<p>We can see that considering the intercept makes a difference for the treatment contrast. First, we define the comparisons involved in a treatment contrast, where two experimental conditions <code>b</code> and <code>c</code> are each compared to a baseline condition <code>a</code> (<code>b~a</code> and <code>c~a</code>). In addition, we explicitly code the intercept term, which involves a comparison of the baseline to 0 (<code>a~0</code>). We take a look at the resulting contrast matrix:</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb380-1"><a href="ch-contr.html#cb380-1" aria-hidden="true"></a><span class="kw">hypr</span>(<span class="dt">int =</span> a <span class="op">~</span><span class="st"> </span><span class="dv">0</span>, <span class="dt">b1 =</span> b <span class="op">~</span><span class="st"> </span>a, <span class="dt">b2 =</span> c <span class="op">~</span><span class="st"> </span>a)</span></code></pre></div>
<pre><code>## hypr object containing 3 null hypotheses:
## H0.int: 0 = a      (Intercept)
##  H0.b1: 0 = b - a
##  H0.b2: 0 = c - a
## 
## Call:
## hypr(int = ~a, b1 = ~b - a, b2 = ~c - a, levels = c(&quot;a&quot;, &quot;b&quot;, 
## &quot;c&quot;))
## 
## Hypothesis matrix (transposed):
##   int b1 b2
## a  1  -1 -1
## b  0   1  0
## c  0   0  1
## 
## Contrast matrix:
##   int b1 b2
## a 1   0  0 
## b 1   1  0 
## c 1   0  1</code></pre>
<div class="sourceCode" id="cb382"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb382-1"><a href="ch-contr.html#cb382-1" aria-hidden="true"></a><span class="kw">contr.treatment</span>(<span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>, <span class="st">&quot;c&quot;</span>))</span></code></pre></div>
<pre><code>##   b c
## a 0 0
## b 1 0
## c 0 1</code></pre>
<p>This shows a contrast matrix that we know from the treatment contrast. The intercept is coded as a column of ones. And each of the comparisons is coded as a <span class="math inline">\(1\)</span> in the condition which is compared to the baseline, and a <span class="math inline">\(0\)</span> in other conditions. The point here is that this gives us the contrast matrix that is expected and known for the treatment contrast.</p>
<p>We can also ignore the intercept in the specification of the comparisons:</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb384-1"><a href="ch-contr.html#cb384-1" aria-hidden="true"></a><span class="kw">hypr</span>(<span class="dt">b1 =</span> m1 <span class="op">~</span><span class="st"> </span>m0, <span class="dt">b2 =</span> m2 <span class="op">~</span><span class="st"> </span>m0)</span></code></pre></div>
<pre><code>## hypr object containing 2 null hypotheses:
## H0.b1: 0 = m1 - m0
## H0.b2: 0 = m2 - m0
## 
## Call:
## hypr(b1 = ~m1 - m0, b2 = ~m2 - m0, levels = c(&quot;m0&quot;, &quot;m1&quot;, &quot;m2&quot;
## ))
## 
## Hypothesis matrix (transposed):
##    b1 b2
## m0 -1 -1
## m1  1  0
## m2  0  1
## 
## Contrast matrix:
##    b1   b2  
## m0 -1/3 -1/3
## m1  2/3 -1/3
## m2 -1/3  2/3</code></pre>
<p>Notice that the resulting contrast matrix now looks very different from the contrast matrix that we know from the treatment contrast. Indeed, this contrast also estimates a reasonable set of quantities. It again estimates whether the condition mean <code>m1</code> differs from the baseline and whether <code>m2</code> differs from baseline. However, the intercept now estimates the average dependent variable across all three conditions (i.e., the grand mean). This can be seen by explicitly adding a comparison of the average of all three conditions to <span class="math inline">\(0\)</span>:</p>
<div class="sourceCode" id="cb386"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb386-1"><a href="ch-contr.html#cb386-1" aria-hidden="true"></a><span class="kw">hypr</span>(<span class="dt">int =</span> (m0 <span class="op">+</span><span class="st"> </span>m1 <span class="op">+</span><span class="st"> </span>m2) <span class="op">/</span><span class="st"> </span><span class="dv">3</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>, <span class="dt">b1 =</span> m1 <span class="op">~</span><span class="st"> </span>m0, <span class="dt">b2 =</span> m2 <span class="op">~</span><span class="st"> </span>m0)</span></code></pre></div>
<pre><code>## hypr object containing 3 null hypotheses:
## H0.int: 0 = (m0 + m1 + m2)/3  (Intercept)
##  H0.b1: 0 = m1 - m0
##  H0.b2: 0 = m2 - m0
## 
## Call:
## hypr(int = ~1/3 * m0 + 1/3 * m1 + 1/3 * m2, b1 = ~m1 - m0, b2 = ~m2 - 
##     m0, levels = c(&quot;m0&quot;, &quot;m1&quot;, &quot;m2&quot;))
## 
## Hypothesis matrix (transposed):
##    int b1  b2 
## m0 1/3  -1  -1
## m1 1/3   1   0
## m2 1/3   0   1
## 
## Contrast matrix:
##    int  b1   b2  
## m0    1 -1/3 -1/3
## m1    1  2/3 -1/3
## m2    1 -1/3  2/3</code></pre>
<p>The last two columns of the resulting contrast matrix are now the same as when the intercept was ignored, which confirms that the two columns encode the same comparison.</p>
</div>
</div>
<div id="computing-condition-means-from-estimated-contrasts" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Computing condition means from estimated contrasts<a href="ch-contr.html#computing-condition-means-from-estimated-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As mentioned earlier, one advantage of Bayesian modeling is that based on the posterior samples, it is possible to very flexibly compute new comparisons and estimates. Above (see section <a href="ch-contr.html#sec-cellMeans">6.1.4</a>), we had discussed the case where the Bayesian model estimated the condition means instead of contrasts by removing the intercept from the <code>brms</code> model (the formula in <code>brms</code> was: <code>DV ~ -1 + F</code>). This allowed us to get posterior samples from each condition mean, and then to compute any possible comparison between condition means by subtracting the corresponding samples.</p>
<p>Importantly, posterior samples for the condition means can also be obtained after fitting a model with contrasts. We illustrate this here for the case of sum contrasts. Let’s use our above example of a design where we assess response times (in milliseconds, <code>DV</code>) for three different word classes adjectives, nouns, and verbs, that is, for a 3-level factor <span class="math inline">\(F\)</span>. In the above example, factor <span class="math inline">\(F\)</span> was coded using a sum contrast, where the first contrast coded the difference of adjectives from the grand mean, and the second contrast coded the difference of nouns from the grand mean. This was the corresponding contrast matrix:</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb388-1"><a href="ch-contr.html#cb388-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts2<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcSum)</span>
<span id="cb388-2"><a href="ch-contr.html#cb388-2" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts2<span class="op">$</span>F)</span></code></pre></div>
<pre><code>##            b1 b2
## adjectives  1  0
## nouns       0  1
## verbs      -1 -1</code></pre>
<p>We had estimated a <code>brms</code> model for this data. The posterior estimates show results for the intercept (which is estimated to be <span class="math inline">\(450\)</span> ms) and for our two coded comparisons. The effect <code>FcH01</code> codes our first comparison that response times for adjectives differ from the grand mean, and show an estimate that response times for adjectives are about <span class="math inline">\(50\)</span> ms shorter than the grand mean. Moreover, the effect <code>FcH02</code> codes our second comparison that response times for nouns differ from the grand mean, and show the estimate that response times for nouns are <span class="math inline">\(50\)</span> ms longer than the grand mean.</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb390-1"><a href="ch-contr.html#cb390-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_Sum)</span></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept    450.4      6.89 436.6 464.3
## FcH01        -49.2      9.72 -68.0 -28.9
## FcH02         49.0      9.78  29.2  68.7</code></pre>
<p>Of course other comparisons might be of interest to us as well. For example, we might be interested in estimating how strongly response times for verbs differ from the grand mean.</p>
<p>To do so, one possible first step is to obtain the posteriors for the response times in each of the three conditions. How can this be done? The first step is to again extract the posterior samples from the model:</p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb392-1"><a href="ch-contr.html#cb392-1" aria-hidden="true"></a>df_postSamp_Sum &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_Sum)</span></code></pre></div>
<p>We can see the samples for our first contrast (<code>b_FcH01</code>) and for our second contrast (<code>b_FcH02</code>). How can we now compute the posterior samples for each of the condition means, i.e., for adjectives, nouns, and verbs? For this, we need to take another look at the contrast matrix.</p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb393-1"><a href="ch-contr.html#cb393-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts2<span class="op">$</span>F)</span></code></pre></div>
<pre><code>##            b1 b2
## adjectives  1  0
## nouns       0  1
## verbs      -1 -1</code></pre>
<p>It tells us how the condition means are computed. For adjectives (see the first row of the contrast matrix), we can see that the response time is computed by taking <span class="math inline">\(1\)</span> times the coefficient for <code>b1</code> (i.e., <code>FcH01</code>) and <span class="math inline">\(0\)</span> times the coefficient for <code>b2</code> (i.e., <code>FcH02</code>). Thus, response times for adjectives are simply the samples for the <code>b1</code> (i.e., <code>FcH01</code>) contrast. The contrast matrix does not show the intercept term, which is implicitly added. Thus, we also have to add the estimates for the intercept. Thus, the condition mean for adjectives is computed as follows:</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb395-1"><a href="ch-contr.html#cb395-1" aria-hidden="true"></a>df_postSamp_Sum &lt;-<span class="st"> </span>df_postSamp_Sum <span class="op">%&gt;%</span></span>
<span id="cb395-2"><a href="ch-contr.html#cb395-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">b_adjectives =</span> b_Intercept <span class="op">+</span><span class="st"> </span>b_FcH01)</span></code></pre></div>
<p>Similarly, we can obtain the posterior samples for the response times for nouns. The computation can be seen from the second row of the contrast matrix, which shows that the contrast <code>b1</code> (i.e., <code>FcH01</code>) has weight <span class="math inline">\(0\)</span> times, whereas the contrast <code>b2</code> (i.e., <code>FcH02</code>) has weight <span class="math inline">\(1\)</span>. Adding the intercept thus gives:</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb396-1"><a href="ch-contr.html#cb396-1" aria-hidden="true"></a>df_postSamp_Sum &lt;-<span class="st"> </span>df_postSamp_Sum <span class="op">%&gt;%</span></span>
<span id="cb396-2"><a href="ch-contr.html#cb396-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">b_nouns =</span> b_Intercept <span class="op">+</span><span class="st"> </span>b_FcH02)</span></code></pre></div>
<p>Finally, we want to obtain posterior samples for the average response times for verbs. For verbs, the third row of the contrast matrix shows two times a <span class="math inline">\(-1\)</span>. Thus, contrasts <code>b1</code> (i.e., <code>FcH01</code>) and <code>b2</code> (i.e., <code>FcH02</code>) have to be subtracted from the intercept:</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb397-1"><a href="ch-contr.html#cb397-1" aria-hidden="true"></a>df_postSamp_Sum<span class="op">$</span>b_verbs &lt;-</span>
<span id="cb397-2"><a href="ch-contr.html#cb397-2" aria-hidden="true"></a><span class="st">  </span>df_postSamp_Sum<span class="op">$</span>b_Intercept <span class="op">-</span><span class="st"> </span>df_postSamp_Sum<span class="op">$</span>b_FcH01 <span class="op">-</span></span>
<span id="cb397-3"><a href="ch-contr.html#cb397-3" aria-hidden="true"></a><span class="st">  </span>df_postSamp_Sum<span class="op">$</span>b_FcH02</span></code></pre></div>
<p>This yields posterior samples for the mean response times for verbs.</p>
<p>We can now look at the posterior means and 95% credible intervals for adjectives, nouns, and verbs by computing the means and quantiles across all computed samples.</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb398-1"><a href="ch-contr.html#cb398-1" aria-hidden="true"></a>postTab &lt;-<span class="st"> </span>df_postSamp_Sum <span class="op">%&gt;%</span></span>
<span id="cb398-2"><a href="ch-contr.html#cb398-2" aria-hidden="true"></a><span class="st">  </span><span class="co"># remove the meta-data:</span></span>
<span id="cb398-3"><a href="ch-contr.html#cb398-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span></span>
<span id="cb398-4"><a href="ch-contr.html#cb398-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">select</span>(b_adjectives, b_nouns, b_verbs) <span class="op">%&gt;%</span></span>
<span id="cb398-5"><a href="ch-contr.html#cb398-5" aria-hidden="true"></a><span class="st">  </span><span class="co"># transform from wide to long with tidyr:</span></span>
<span id="cb398-6"><a href="ch-contr.html#cb398-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">pivot_longer</span>(<span class="dt">cols =</span> <span class="kw">everything</span>(),</span>
<span id="cb398-7"><a href="ch-contr.html#cb398-7" aria-hidden="true"></a>               <span class="dt">names_to =</span> <span class="st">&quot;condition&quot;</span>,</span>
<span id="cb398-8"><a href="ch-contr.html#cb398-8" aria-hidden="true"></a>               <span class="dt">values_to =</span> <span class="st">&quot;samp&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb398-9"><a href="ch-contr.html#cb398-9" aria-hidden="true"></a><span class="st">  </span><span class="kw">group_by</span>(condition) <span class="op">%&gt;%</span></span>
<span id="cb398-10"><a href="ch-contr.html#cb398-10" aria-hidden="true"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">post_mean =</span> <span class="kw">mean</span>(samp),</span>
<span id="cb398-11"><a href="ch-contr.html#cb398-11" aria-hidden="true"></a>            <span class="st">`</span><span class="dt">2.5%</span><span class="st">`</span> =<span class="st"> </span><span class="kw">quantile</span>(samp, <span class="dt">p =</span> <span class="fl">0.025</span>),</span>
<span id="cb398-12"><a href="ch-contr.html#cb398-12" aria-hidden="true"></a>            <span class="st">`</span><span class="dt">97.5%</span><span class="st">`</span> =<span class="st"> </span><span class="kw">quantile</span>(samp, <span class="dt">p =</span> <span class="fl">0.975</span>))</span></code></pre></div>
<div class="sourceCode" id="cb399"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb399-1"><a href="ch-contr.html#cb399-1" aria-hidden="true"></a>postTab</span></code></pre></div>
<pre><code>## # A tibble: 3 × 4
##   condition    post_mean `2.5%` `97.5%`
##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
## 1 b_adjectives      401.   378.    426.
## 2 b_nouns           499.   475.    523.
## 3 b_verbs           451.   426.    475.</code></pre>
<p>The results show that as expected the posterior mean for adjectives is <span class="math inline">\(400\)</span> ms, for nouns it is <span class="math inline">\(500\)</span> ms, and for verbs, the posterior mean is <span class="math inline">\(450\)</span> ms. Moreover, we have now posterior credible intervals for each of these estimates.</p>
<p>In fact, <code>brms</code> has a very convenient built-in function that allows us to compute these nested effects automatically (<code>robust = FALSE</code> shows the posterior mean; by default <code>brms</code> shows the posterior median). Notice that you need to add a <code>[]</code> after the function call, otherwise <code>brms</code> will plot the results.</p>
<div class="sourceCode" id="cb401"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb401-1"><a href="ch-contr.html#cb401-1" aria-hidden="true"></a><span class="kw">conditional_effects</span>(fit_Sum, <span class="dt">robust =</span> <span class="ot">FALSE</span>)[]</span></code></pre></div>
<pre><code>## $F
##            F  DV cond__  effect1__ estimate__ se__ lower__ upper__
## 1 adjectives 450      1 adjectives        401 11.9     378     426
## 2      nouns 450      1      nouns        499 11.8     475     523
## 3      verbs 450      1      verbs        451 12.3     426     475</code></pre>
<p>The same function allows us to visualize the effects, as shown in Figure <a href="ch-contr.html#fig:cFigCond">6.5</a>.</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb403-1"><a href="ch-contr.html#cb403-1" aria-hidden="true"></a><span class="kw">conditional_effects</span>(fit_Sum, <span class="dt">robust =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:cFigCond"></span>
<img src="bayescogsci_files/figure-html/cFigCond-1.svg" alt="Estimated condition means, computed from a `brms` model fitted with a sum contrast." width="672" />
<p class="caption">
FIGURE 6.5: Estimated condition means, computed from a <code>brms</code> model fitted with a sum contrast.
</p>
</div>
<p>Coming back to our hand-crafted computations, the posterior samples can be used to compute additional comparisons. For example, we might be interested in how much response times for verbs differ from the grand mean. This can be computed based on the samples for the condition means: we first compute the grand mean from the three condition means, and then we compare this to the estimate for verbs.</p>
<div class="sourceCode" id="cb404"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb404-1"><a href="ch-contr.html#cb404-1" aria-hidden="true"></a>df_postSamp_Sum &lt;-<span class="st"> </span>df_postSamp_Sum <span class="op">%&gt;%</span></span>
<span id="cb404-2"><a href="ch-contr.html#cb404-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">GM =</span> (b_adjectives <span class="op">+</span><span class="st"> </span>b_nouns <span class="op">+</span><span class="st"> </span>b_verbs) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>,</span>
<span id="cb404-3"><a href="ch-contr.html#cb404-3" aria-hidden="true"></a>         <span class="dt">b_FcH03 =</span> b_verbs <span class="op">-</span><span class="st"> </span>GM)</span>
<span id="cb404-4"><a href="ch-contr.html#cb404-4" aria-hidden="true"></a><span class="kw">c</span>(<span class="dt">post_mean =</span> <span class="kw">mean</span>(df_postSamp_Sum<span class="op">$</span>b_FcH03),</span>
<span id="cb404-5"><a href="ch-contr.html#cb404-5" aria-hidden="true"></a>  <span class="kw">quantile</span>(df_postSamp_Sum<span class="op">$</span>b_FcH03, <span class="dt">p =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</span></code></pre></div>
<pre><code>## post_mean      2.5%     97.5% 
##     0.121   -19.541    19.624</code></pre>
<p>The results show that reading times for verbs are quite the same as the grand mean, with a posterior mean estimate for the differences of nearly <span class="math inline">\(0\)</span> ms, and with a 95% credible interval ranging between <span class="math inline">\(-20\)</span> and <span class="math inline">\(+20\)</span> ms.</p>
<p>The key message here is that based on the contrast matrix, it is possible to compute posterior samples for the condition means, and then to compute any arbitrary further comparisons or contrasts. We want to stress again that just obtaining the posterior distribution of a comparison does not allow us to argue that we have evidence for the effect; to argue that we have evidence for an effect being present/absent, we need Bayes factors. But the approach we outline above does allow us to obtain posterior means and credible intervals for arbitrary comparisons. However, different combinations of contrasts and priors may yield different results, especially when they are used to compute Bayes factors, due to their sensitivity for the priors.</p>
<p>We briefly show how to compute posterior samples for condition means for one more example contrast, namely for repeated contrasts. Here, the contrast matrix is:</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb406-1"><a href="ch-contr.html#cb406-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcRep)</span>
<span id="cb406-2"><a href="ch-contr.html#cb406-2" aria-hidden="true"></a><span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F)</span></code></pre></div>
<pre><code>##    c2vs1 c3vs2 c4vs3
## F1 -3/4  -1/2  -1/4 
## F2  1/4  -1/2  -1/4 
## F3  1/4   1/2  -1/4 
## F4  1/4   1/2   3/4</code></pre>
<p>The model estimates were:</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb408-1"><a href="ch-contr.html#cb408-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_Rep)</span></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5 Q97.5
## Intercept    19.96      2.54  15.02 25.03
## Fc2vs1       10.00      7.01  -3.91 24.12
## Fc3vs2       -9.77      7.09 -24.03  4.55
## Fc4vs3       29.41      6.96  15.86 42.81</code></pre>
<p>We first obtain the posterior samples for the contrasts:</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb410-1"><a href="ch-contr.html#cb410-1" aria-hidden="true"></a>df_postSamp_Rep &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_Rep)</span></code></pre></div>
<p>Then we compute the posterior samples for condition <span class="math inline">\(F1\)</span>. First, we have to add the intercept. Then, we can see in the contrast matrix that to compute the condition mean for <span class="math inline">\(F1\)</span>, we have to add up all contrasts, using the weights <code>c(-3/4, -1/2, -1/4)</code> for each of the three contrasts (see first row of the contrast matrix). Thus, the posterior samples are computed as follows:</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb411-1"><a href="ch-contr.html#cb411-1" aria-hidden="true"></a>df_postSamp_Rep &lt;-<span class="st"> </span>df_postSamp_Rep <span class="op">%&gt;%</span></span>
<span id="cb411-2"><a href="ch-contr.html#cb411-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">b_F1 =</span> b_Intercept <span class="op">+</span></span>
<span id="cb411-3"><a href="ch-contr.html#cb411-3" aria-hidden="true"></a><span class="st">           </span><span class="dv">-3</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc2vs1 <span class="op">+</span></span>
<span id="cb411-4"><a href="ch-contr.html#cb411-4" aria-hidden="true"></a><span class="st">           </span><span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>b_Fc3vs2 <span class="op">+</span></span>
<span id="cb411-5"><a href="ch-contr.html#cb411-5" aria-hidden="true"></a><span class="st">           </span><span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc4vs3)</span></code></pre></div>
<p>The other condition means are computed correspondingly:</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb412-1"><a href="ch-contr.html#cb412-1" aria-hidden="true"></a>df_postSamp_Rep &lt;-<span class="st"> </span>df_postSamp_Rep <span class="op">%&gt;%</span></span>
<span id="cb412-2"><a href="ch-contr.html#cb412-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">b_F2 =</span> b_Intercept <span class="op">+</span></span>
<span id="cb412-3"><a href="ch-contr.html#cb412-3" aria-hidden="true"></a><span class="st">           </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc2vs1 <span class="op">+</span></span>
<span id="cb412-4"><a href="ch-contr.html#cb412-4" aria-hidden="true"></a><span class="st">           </span><span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>b_Fc3vs2 <span class="op">+</span></span>
<span id="cb412-5"><a href="ch-contr.html#cb412-5" aria-hidden="true"></a><span class="st">           </span><span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc4vs3,</span>
<span id="cb412-6"><a href="ch-contr.html#cb412-6" aria-hidden="true"></a>         <span class="dt">b_F3 =</span> b_Intercept <span class="op">+</span></span>
<span id="cb412-7"><a href="ch-contr.html#cb412-7" aria-hidden="true"></a><span class="st">           </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc2vs1 <span class="op">+</span></span>
<span id="cb412-8"><a href="ch-contr.html#cb412-8" aria-hidden="true"></a><span class="st">           </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>b_Fc3vs2 <span class="op">+</span></span>
<span id="cb412-9"><a href="ch-contr.html#cb412-9" aria-hidden="true"></a><span class="st">           </span><span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span>b_Fc4vs3,</span>
<span id="cb412-10"><a href="ch-contr.html#cb412-10" aria-hidden="true"></a>         <span class="dt">b_F4 =</span> b_Intercept <span class="op">+</span></span>
<span id="cb412-11"><a href="ch-contr.html#cb412-11" aria-hidden="true"></a><span class="st">           </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc2vs1 <span class="op">+</span></span>
<span id="cb412-12"><a href="ch-contr.html#cb412-12" aria-hidden="true"></a><span class="st">           </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>b_Fc3vs2 <span class="op">+</span></span>
<span id="cb412-13"><a href="ch-contr.html#cb412-13" aria-hidden="true"></a><span class="st">           </span><span class="dv">3</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc4vs3)</span></code></pre></div>
<p>Now we can look at the posterior means and credible intervals:</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb413-1"><a href="ch-contr.html#cb413-1" aria-hidden="true"></a>postTab &lt;-<span class="st"> </span>df_postSamp_Rep <span class="op">%&gt;%</span></span>
<span id="cb413-2"><a href="ch-contr.html#cb413-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">select</span>(b_F1, b_F2, b_F3, b_F4) <span class="op">%&gt;%</span></span>
<span id="cb413-3"><a href="ch-contr.html#cb413-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">pivot_longer</span>(<span class="dt">cols =</span> <span class="kw">everything</span>(),</span>
<span id="cb413-4"><a href="ch-contr.html#cb413-4" aria-hidden="true"></a>               <span class="dt">names_to =</span> <span class="st">&quot;condition&quot;</span>,</span>
<span id="cb413-5"><a href="ch-contr.html#cb413-5" aria-hidden="true"></a>               <span class="dt">values_to =</span> <span class="st">&quot;samp&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb413-6"><a href="ch-contr.html#cb413-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">group_by</span>(condition) <span class="op">%&gt;%</span></span>
<span id="cb413-7"><a href="ch-contr.html#cb413-7" aria-hidden="true"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">post_mean =</span> <span class="kw">round</span>(<span class="kw">mean</span>(samp)),</span>
<span id="cb413-8"><a href="ch-contr.html#cb413-8" aria-hidden="true"></a>            <span class="st">`</span><span class="dt">2.5%</span><span class="st">`</span> =<span class="st"> </span><span class="kw">round</span>(<span class="kw">quantile</span>(samp, <span class="dt">p =</span> <span class="fl">0.025</span>)),</span>
<span id="cb413-9"><a href="ch-contr.html#cb413-9" aria-hidden="true"></a>            <span class="st">`</span><span class="dt">97.5%</span><span class="st">`</span> =<span class="st"> </span><span class="kw">round</span>(<span class="kw">quantile</span>(samp, <span class="dt">p =</span> <span class="fl">0.975</span>)))</span></code></pre></div>
<div class="sourceCode" id="cb414"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb414-1"><a href="ch-contr.html#cb414-1" aria-hidden="true"></a><span class="kw">print</span>(postTab, <span class="dt">n =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## # A tibble: 4 × 4
##   condition post_mean `2.5%` `97.5%`
##   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
## 1 b_F1             10      0      20
## 2 b_F2             20     11      30
## 3 b_F3             10      0      20
## 4 b_F4             40     30      50</code></pre>
<p>We can verify that <code>brms</code> function return the same values:</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb416-1"><a href="ch-contr.html#cb416-1" aria-hidden="true"></a><span class="kw">conditional_effects</span>(fit_Rep, <span class="dt">robust =</span> <span class="ot">FALSE</span>)[]</span></code></pre></div>
<pre><code>## $F
##    F DV cond__ effect1__ estimate__ se__ lower__ upper__
## 1 F1 20      1        F1       10.0 5.08  -0.380    20.1
## 2 F2 20      1        F2       20.0 5.02  10.555    30.3
## 3 F3 20      1        F3       10.2 4.91   0.451    20.1
## 4 F4 20      1        F4       39.6 5.02  29.756    49.5</code></pre>
<p>The posterior means reflect exactly the means in the data (for comparison see Figure <a href="ch-contr.html#fig:helmertsimdatFig">6.2</a> and Table <a href="ch-contr.html#tab:cTab3Means">6.3</a>). We now have posterior samples for each of the conditions and can compute posterior credible intervals as well as new comparisons between conditions.</p>
</div>
<div id="summary-5" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Summary<a href="ch-contr.html#summary-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Contrasts in Bayesian models work in exactly the same way as in frequentist models. Contrasts provide a way to tell the model how to code factors into numeric covariates. That is, they provide a way to define which comparisons between which condition means or bundles of condition means should be estimated in the Bayesian model. There are a number of default contrasts, like treatment contrasts, sum contrasts, repeated contrasts, and Helmert contrasts, that estimate specific comparisons between (groups of) condition means. A much more powerful procedure is to use the generalized matrix inverse, e.g., as implemented in the <code>hypr</code> package, to derive contrasts automatically after specifying the comparisons that a contrast should estimate. We have seen that in Bayesian models, it is quite straightforward to compute posterior samples for new contrasts post-hoc, after the model is fit. However, specifying precise contrasts is still of key importance when doing model comparisons (via Bayes factors) to answer the question of whether the data provide evidence for an effect of interest. If the effect of interest relates to a factor, then it has to be defined using contrast coding.</p>
</div>
<div id="further-reading-3" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Further reading<a href="ch-contr.html#further-reading-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A good discussion on contrast coding appears in Chapter 15 of <span class="citation">Baguley (<a href="#ref-baguley2012serious" role="doc-biblioref">2012</a>)</span>. A book-length treatment is by <span class="citation">Rosenthal, Rosnow, and Rubin (<a href="#ref-rosenthal2000contrasts" role="doc-biblioref">2000</a>)</span>. A brief discussion on contrast coding appears in <span class="citation">Venables and Ripley (<a href="#ref-venablesripley" role="doc-biblioref">2002</a>)</span>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references hanging-indent">
<div id="ref-baguley2012serious">
<p>Baguley, Thomas. 2012. <em>Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences</em>. Macmillan International Higher Education.</p>
</div>
<div id="ref-Bolker2018">
<p>Bolker, Ben. 2018. “Contrasts.” <a href="https://computationalcognitivescience.github.io/lovelace/">https://computationalcognitivescience.github.io/lovelace/</a>.</p>
</div>
<div id="ref-burkner2020modelling">
<p>Bürkner, Paul-Christian, and Emmanuel Charpentier. 2020. “Modelling Monotonic Effects of Ordinal Predictors in Bayesian Regression Models.” <em>British Journal of Mathematical and Statistical Psychology</em>. <a href="https://doi.org/https://doi.org/10.1111/bmsp.12195">https://doi.org/https://doi.org/10.1111/bmsp.12195</a>.</p>
</div>
<div id="ref-dablander2022puzzle">
<p>Dablander, Fabian, Karoline Huth, Quentin F. Gronau, Alexander Etz, and Eric-Jan Wagenmakers. 2022. “A Puzzle of Proportions: Two Popular Bayesian Tests Can Yield Dramatically Different Conclusions.” <em>Statistics in Medicine</em> 41 (8): 1319–33.</p>
</div>
<div id="ref-dobson2011introduction">
<p>Dobson, Annette J., and Adrian Barnett. 2011. <em>An Introduction to Generalized Linear Models</em>. CRC Press.</p>
</div>
<div id="ref-friendly_matlib">
<p>Friendly, Michael, John Fox, and Phil Chalmers. 2020. <em>Matlib: Matrix Functions for Teaching and Learning Linear Algebra and Multivariate Statistics</em>. <a href="https://CRAN.R-project.org/package=matlib">https://CRAN.R-project.org/package=matlib</a>.</p>
</div>
<div id="ref-heister2012analysing">
<p>Heister, Julian, Kay-Michael Würzner, and Reinhold Kliegl. 2012. “Analysing Large Datasets of Eye Movements During Reading.” <em>Visual Word Recognition</em> 2: 102–30.</p>
</div>
<div id="ref-rabe2020hypr">
<p>Rabe, Maximilian M., Shravan Vasishth, Sven Hohenstein, Reinhold Kliegl, and Daniel J. Schad. 2020. “hypr: An R Package for Hypothesis-Driven Contrast Coding.” <em>Journal of Open Source Software</em> 5 (48): 2134.</p>
</div>
<div id="ref-R-MASS">
<p>Ripley, Brian D. 2023. <em>MASS: Support Functions and Datasets for Venables and Ripley’s MASS</em>. <a href="http://www.stats.ox.ac.uk/pub/MASS4/">http://www.stats.ox.ac.uk/pub/MASS4/</a>.</p>
</div>
<div id="ref-rosenthal2000contrasts">
<p>Rosenthal, Robert, Ralph L. Rosnow, and Donald B. Rubin. 2000. <em>Contrasts and Effect Sizes in Behavioral Research: A Correlational Approach</em>. Cambridge University Press.</p>
</div>
<div id="ref-rouder2012default">
<p>Rouder, Jeffrey N, Richard D Morey, Paul L Speckman, and Jordan M Province. 2012. “Default Bayes Factors for Anova Designs.” <em>Journal of Mathematical Psychology</em> 56 (5): 356–74.</p>
</div>
<div id="ref-schadHowCapitalizePriori2020">
<p>Schad, Daniel J., Shravan Vasishth, Sven Hohenstein, and Reinhold Kliegl. 2020. “How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: A Tutorial.” <em>Journal of Memory and Language</em> 110 (February): 104038. <a href="https://doi.org/10/gf9tjp">https://doi.org/10/gf9tjp</a>.</p>
</div>
<div id="ref-venablesripley">
<p>Venables, William N., and Brian D. Ripley. 2002. <em>Modern Applied Statistics with S-PLUS</em>. New York: Springer.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="26">
<li id="fn26"><p>A Bayesian analog to the frequentist <span class="math inline">\(F\)</span>-test is Bayesian model comparison (i.e., Bayes factors), where we might compare an alternative model including word class as a predictor term with a null model lacking this predictor. We will discuss such Bayesian model comparison using Bayes factors in chapter <a href="ch-bf.html#ch-bf">13</a>.<a href="ch-contr.html#fnref26" class="footnote-back">↩︎</a></p></li>
<li id="fn27"><p>When using treatment coding, it’s important to be aware that the intercept won’t be centered. As mentioned in online section <a href="regression-models-with-brms---extended.html#app-intercept">A.3</a>, this means the prior one sets for the intercept in <code>brms</code> will actually be for assigned to its centered version. Although this distinction may not have any impact when one uses wide priors, it’s still worth keeping in mind.<a href="ch-contr.html#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p>These are the functions <code>contr.treatment()</code>, <code>contr.sum()</code>, <code>contr.poly()</code>, <code>contr.helmert()</code>, and <code>contr.sdif()</code>. The last contrast is available in the <code>MASS</code> package.<a href="ch-contr.html#fnref28" class="footnote-back">↩︎</a></p></li>
<li id="fn29"><p>At this point, there is no need to understand in detail what this means. We refer the interested reader to <span class="citation">Schad et al. (<a href="#ref-schadHowCapitalizePriori2020" role="doc-biblioref">2020</a>)</span>. For a quick overview, we recommend a vignette explaining the generalized inverse in the <code>matlib</code> package <span class="citation">(<a href="https://cran.r-project.org/web/packages/matlib/vignettes/ginv.html" class="uri" role="doc-biblioref">https://cran.r-project.org/web/packages/matlib/vignettes/ginv.html</a>; Friendly, Fox, and Chalmers <a href="#ref-friendly_matlib" role="doc-biblioref">2020</a>)</span>.<a href="ch-contr.html#fnref29" class="footnote-back">↩︎</a></p></li>
<li id="fn30"><p>If one is trying to set a general prior for differences between means, then the function <code>bayestestR::contr.equalprior()</code> can be used. The <code>bayestestR::contr.equalprior()</code> function can be used when one aims to set equal marginal priors for differences between means across all levels of a factor. This avoids the unequal prior distributions that can result from other contrast methods (e.g., <code>stats::contr.treatment()</code>) and supports correct Bayes factor estimation in multi-level factors. It is particularly useful for factors with more than two levels, following the orthogonal-normal contrasts described in <span class="citation">Rouder et al. (<a href="#ref-rouder2012default" role="doc-biblioref">2012</a>, 363)</span>. For more information, see <a href="https://easystats.github.io/bayestestR/reference/contr.equalprior.html" class="uri">https://easystats.github.io/bayestestR/reference/contr.equalprior.html</a>.<a href="ch-contr.html#fnref30" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-hierarchical.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-coding2x2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
