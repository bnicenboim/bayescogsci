<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>G Exercises | Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="G Exercises | Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://bruno.nicenboim.me/bayescogsci/images/temporarycover.jpg" />
  <meta property="og:description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/bnicenboim/bayescogsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="G Exercises | Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bruno.nicenboim.me/bayescogsci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel J. Schad, and Shravan Vasishth" />


<meta name="date" content="2025-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-workflow.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<script data-goatcounter="https://bayescogsci.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b>  Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The  law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b>  Discrete random variables: An example using the  binomial distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b>  Continuous random variables: An example using the  normal distribution</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1:  Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (or multivariate) data</a></li>
<li class="chapter" data-level="1.6.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-decomposevcovmatrix"><i class="fa fa-check"></i><b>1.6.4</b> Decomposing a variance-covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-some-useful-r-functions"><i class="fa fa-check"></i><b>1.8</b> Summary of some useful R functions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b>  Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the  posterior using Bayes’ rule: An analytical example</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a  likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a  prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using  Bayes’ rule to compute the  posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The  posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the  posterior through  sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b>  Bayesian Regression Models using Stan:  brms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors:  sensitivity analysis</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b>  Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b>  Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b>  Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b>  Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b>  Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The  log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first  linear regression: Does attentional load affect pupil size?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b>  Likelihood and  priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b>  Log-normal model: Does trial affect response times?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b>  Logistic regression: Does  set size affect  free recall?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b>  Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b>  Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b>  No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b>  Varying intercepts and  varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A  hierarchical log-normal model: The  Stroop effect</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes  log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>6.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>6.1.1</b> Default contrast coding:  Treatment contrasts</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>6.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>6.1.3</b>  Sum contrasts</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>6.1.4</b>  Cell means parameterization and  posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>6.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>6.2.1</b>  Sum contrasts</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>6.2.2</b> The  hypothesis matrix</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>6.2.3</b> Generating contrasts: The  <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>6.3</b> Other types of contrasts: illustration with a factor of four levels</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>6.3.1</b>  Repeated contrasts</a></li>
<li class="chapter" data-level="6.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>6.3.2</b>  Helmert contrasts</a></li>
<li class="chapter" data-level="6.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>6.3.3</b> Contrasts in linear regression analysis: The design or  model matrix</a></li>
<li class="chapter" data-level="6.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>6.3.4</b>  Polynomial contrasts</a></li>
<li class="chapter" data-level="6.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>6.3.5</b> An alternative to contrasts:  Monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>6.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>6.4.1</b>  Centered contrasts</a></li>
<li class="chapter" data-level="6.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>6.4.2</b>  Orthogonal contrasts</a></li>
<li class="chapter" data-level="6.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> The role of the  intercept in  non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>6.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="6.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-5"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
<li class="chapter" data-level="6.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-3"><i class="fa fa-check"></i><b>6.7</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>7</b> Contrast coding with two predictor variables</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>7.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>7.1.1</b>  Nested effects</a></li>
<li class="chapter" data-level="7.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>7.1.2</b>  Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>7.2</b> One factor and one  covariate</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>7.2.1</b> Estimating a  group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>7.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>7.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="7.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="8" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>8</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>8.1</b> Stan syntax</a></li>
<li class="chapter" data-level="8.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>8.2</b> A first simple example with Stan:  Normal likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>8.3</b> Another simple example:  Cloze probability with Stan with the  binomial likelihood</a></li>
<li class="chapter" data-level="8.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>8.4</b>  Regression models in Stan</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>8.4.1</b> A first  linear regression in Stan: Does attentional load affect  pupil size?</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>8.4.2</b>  Interactions in Stan: Does attentional load interact with trial number affecting  pupil size?</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>8.4.3</b>  Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-7"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-5"><i class="fa fa-check"></i><b>8.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>9</b> Hierarchical models and reparameterization </a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>9.1</b> Hierarchical models with Stan</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>9.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>9.1.2</b> Uncorrelated  varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="9.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>9.1.3</b>  Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="9.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>9.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-8"><i class="fa fa-check"></i><b>9.2</b> Summary</a></li>
<li class="chapter" data-level="9.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-6"><i class="fa fa-check"></i><b>9.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>10</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>10.1</b> A change of variables with the reciprocal normal distribution</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>10.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>10.2</b>  Validation of a computed posterior distribution</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>10.2.1</b> The  simulation-based calibration procedure</a></li>
<li class="chapter" data-level="10.2.2" data-path="ch-custom.html"><a href="ch-custom.html#an-example-where-simulation-based-calibration-reveals-a-problem"><i class="fa fa-check"></i><b>10.2.2</b> An example where simulation-based calibration reveals a problem</a></li>
<li class="chapter" data-level="10.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-with-and-limitations-of-simulation-based-calibration"><i class="fa fa-check"></i><b>10.2.3</b> Issues with and limitations of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-the-exponential-distribution-implemented-manually"><i class="fa fa-check"></i><b>10.3</b> Another  custom distribution: The exponential distribution  implemented manually</a></li>
<li class="chapter" data-level="10.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-9"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
<li class="chapter" data-level="10.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-7"><i class="fa fa-check"></i><b>10.5</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="11" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>11</b>  Meta-analysis and  measurement error models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>11.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>11.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>11.2</b>  Measurement-error models</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>11.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-10"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-8"><i class="fa fa-check"></i><b>11.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison</b></span></li>
<li class="chapter" data-level="12" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>12</b> Introduction to model comparison</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-comparison.html"><a href="ch-comparison.html#prior-predictive-vs.-posterior-predictive-model-comparison"><i class="fa fa-check"></i><b>12.1</b> Prior predictive vs. posterior predictive model comparison</a></li>
<li class="chapter" data-level="12.2" data-path="ch-comparison.html"><a href="ch-comparison.html#some-important-points-to-consider-when-comparing-models"><i class="fa fa-check"></i><b>12.2</b> Some important points to consider when comparing models</a></li>
<li class="chapter" data-level="12.3" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-9"><i class="fa fa-check"></i><b>12.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>13</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>13.1</b> Hypothesis testing using the Bayes factor</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>13.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="13.1.2" data-path="ch-bf.html"><a href="ch-bf.html#the-bayes-factor"><i class="fa fa-check"></i><b>13.1.2</b> The Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>13.2</b> Examining the N400 effect with the Bayes factor</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>13.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="13.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>13.2.2</b>  Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>13.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="13.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>13.4</b>  The Bayes factor in Stan</a></li>
<li class="chapter" data-level="13.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>13.5</b> Bayes factors in theory and in practice</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>13.5.1</b> Bayes factors in theory: Stability and  accuracy</a></li>
<li class="chapter" data-level="13.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>13.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="13.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>13.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="ch-bf.html"><a href="ch-bf.html#sample-size-determination-using-bayes-factors"><i class="fa fa-check"></i><b>13.6</b> Sample size determination using Bayes factors</a></li>
<li class="chapter" data-level="13.7" data-path="ch-bf.html"><a href="ch-bf.html#summary-11"><i class="fa fa-check"></i><b>13.7</b> Summary</a></li>
<li class="chapter" data-level="13.8" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-10"><i class="fa fa-check"></i><b>13.8</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>14</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>14.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="14.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>14.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="14.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>14.3</b> Testing the N400 effect using cross-validation</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>14.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="14.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>14.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="14.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>14.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>14.4</b>  Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="14.5" data-path="ch-cv.html"><a href="ch-cv.html#sec-issuesCV"><i class="fa fa-check"></i><b>14.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="14.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>14.6</b> Cross-validation in Stan</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>14.6.1</b>  PSIS-LOO-CV in Stan</a></li>
<li class="chapter" data-level="14.6.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-cv-in-stan"><i class="fa fa-check"></i><b>14.6.2</b>  K-fold-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-12"><i class="fa fa-check"></i><b>14.7</b> Summary</a></li>
<li class="chapter" data-level="14.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-11"><i class="fa fa-check"></i><b>14.8</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>VI Cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="15" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>15</b> Introduction to cognitive modeling</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#what-characterizes-a-computational-cognitive-model"><i class="fa fa-check"></i><b>15.1</b> What characterizes a computational cognitive model?</a></li>
<li class="chapter" data-level="15.2" data-path="ch-cogmod.html"><a href="ch-cogmod.html#some-advantages-of-taking-the-latent-variable-modeling-approach"><i class="fa fa-check"></i><b>15.2</b> Some advantages of taking the latent-variable modeling approach</a></li>
<li class="chapter" data-level="15.3" data-path="ch-cogmod.html"><a href="ch-cogmod.html#types-of-computational-cognitive-model"><i class="fa fa-check"></i><b>15.3</b> Types of computational cognitive model</a></li>
<li class="chapter" data-level="15.4" data-path="ch-cogmod.html"><a href="ch-cogmod.html#summary-13"><i class="fa fa-check"></i><b>15.4</b> Summary</a></li>
<li class="chapter" data-level="15.5" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-12"><i class="fa fa-check"></i><b>15.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>16</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>16.1</b> Modeling  multiple categorical responses</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>16.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="16.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>16.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>16.2</b> Modeling picture naming abilities in aphasia with MPT models</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>16.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="16.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>16.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="16.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>16.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="16.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>16.2.4</b> A  hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-14"><i class="fa fa-check"></i><b>16.3</b> Summary</a></li>
<li class="chapter" data-level="16.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-13"><i class="fa fa-check"></i><b>16.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>17</b> Mixture models</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>17.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>17.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="17.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>17.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="17.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>17.1.3</b> A  multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="17.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>17.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="17.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>17.1.5</b> A  hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-15"><i class="fa fa-check"></i><b>17.2</b> Summary</a></li>
<li class="chapter" data-level="17.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-14"><i class="fa fa-check"></i><b>17.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>18</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>18.1</b> Modeling a lexical decision task</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>18.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>18.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="18.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>18.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="18.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>18.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="18.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>18.1.5</b> Dealing with  contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>18.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="18.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-16"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-closing.html"><a href="ch-closing.html"><i class="fa fa-check"></i><b>19</b> In closing</a></li>
<li class="appendix"><span><b>Online materials</b></span></li>
<li class="chapter" data-level="A" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html"><i class="fa fa-check"></i><b>A</b> Regression models with <code>brms</code> - Extended</a>
<ul>
<li class="chapter" data-level="A.1" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-efficientpriorpd"><i class="fa fa-check"></i><b>A.1</b> An efficient function for generating prior predictive distributions in R</a></li>
<li class="chapter" data-level="A.2" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-truncation"><i class="fa fa-check"></i><b>A.2</b> Truncated distributions</a></li>
<li class="chapter" data-level="A.3" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-intercept"><i class="fa fa-check"></i><b>A.3</b> Intercepts in <code>brms</code></a></li>
<li class="chapter" data-level="A.4" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-lognormal"><i class="fa fa-check"></i><b>A.4</b> Understanding the log-normal likelihood</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#log-normal-distributions-everywhere"><i class="fa fa-check"></i><b>A.4.1</b> Log-normal distributions everywhere</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-priorR"><i class="fa fa-check"></i><b>A.5</b> Prior predictive checks in R</a></li>
<li class="chapter" data-level="A.6" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-exch"><i class="fa fa-check"></i><b>A.6</b> Finitely exchangeable random variables</a></li>
<li class="chapter" data-level="A.7" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-matrixHierachicalModel"><i class="fa fa-check"></i><b>A.7</b> The Matrix Formulation of Hierarchical Models (the Laird-Ware form)</a></li>
<li class="chapter" data-level="A.8" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-cTreatGM"><i class="fa fa-check"></i><b>A.8</b> Treatment contrast with intercept as the grand mean</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html"><i class="fa fa-check"></i><b>B</b> Advanced models with Stan - Extended</a>
<ul>
<li class="chapter" data-level="B.1" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-target"><i class="fa fa-check"></i><b>B.1</b> What does <code>target</code> do in Stan models?</a></li>
<li class="chapter" data-level="B.2" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-tilde"><i class="fa fa-check"></i><b>B.2</b> Explicitly incrementing the log probability function (<code>target</code>) vs. using the sampling or distribution <code>~</code> notation</a></li>
<li class="chapter" data-level="B.3" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-cmdstanr"><i class="fa fa-check"></i><b>B.3</b> An alternative R interface to Stan: <code>cmdstanr</code></a></li>
<li class="chapter" data-level="B.4" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-stancontainers"><i class="fa fa-check"></i><b>B.4</b> Matrix, vector, or array in Stan?</a></li>
<li class="chapter" data-level="B.5" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-noncenterparam"><i class="fa fa-check"></i><b>B.5</b> A simple non-centered parameterization</a></li>
<li class="chapter" data-level="B.6" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-cholesky"><i class="fa fa-check"></i><b>B.6</b> Cholesky factorization for reparameterizing hierarchical models with correlations between adjustments to different parameters</a></li>
<li class="chapter" data-level="B.7" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-sbc"><i class="fa fa-check"></i><b>B.7</b> Different rank visualizations and the <code>SBC</code> package.</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="evidence-synthesis-and-measurements-with-error---extended.html"><a href="evidence-synthesis-and-measurements-with-error---extended.html"><i class="fa fa-check"></i><b>C</b> Evidence synthesis and measurements with error - Extended</a>
<ul>
<li class="chapter" data-level="C.1" data-path="evidence-synthesis-and-measurements-with-error---extended.html"><a href="evidence-synthesis-and-measurements-with-error---extended.html#app-sigmatrue"><i class="fa fa-check"></i><b>C.1</b> What happens if we set <code>sigma = TRUE</code> in <code>resp_se()</code> function in <code>brms</code>?</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html"><i class="fa fa-check"></i><b>D</b> Model comparison - Extended</a>
<ul>
<li class="chapter" data-level="D.1" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-null"><i class="fa fa-check"></i><b>D.1</b> Credible intervals should not be used to reject a null hypothesis</a></li>
<li class="chapter" data-level="D.2" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-likR"><i class="fa fa-check"></i><b>D.2</b> The likelihood ratio vs the Bayes factor</a></li>
<li class="chapter" data-level="D.3" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-integral"><i class="fa fa-check"></i><b>D.3</b> Approximation of the (expected) log predictive density of a model without integration</a></li>
<li class="chapter" data-level="D.4" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-CV-alg"><i class="fa fa-check"></i><b>D.4</b> The cross-validation algorithm for the expected log predictive density of a model</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>E</b> The Art and Science of Prior Elicitation</a>
<ul>
<li class="chapter" data-level="E.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>E.1</b> Eliciting priors from oneself for a self-paced reading study: An example</a>
<ul>
<li class="chapter" data-level="E.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>E.1.1</b> An example: English  relative clauses</a></li>
<li class="chapter" data-level="E.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>E.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="E.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>E.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="E.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>E.1.4</b> Eliciting priors for the  variance components</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>E.2</b>  Eliciting priors from experts</a></li>
<li class="chapter" data-level="E.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>E.3</b> Deriving priors from  meta-analyses</a></li>
<li class="chapter" data-level="E.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>E.4</b> Using previous experiments’  posteriors as priors for a new study</a></li>
<li class="chapter" data-level="E.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-17"><i class="fa fa-check"></i><b>E.5</b> Summary</a></li>
<li class="chapter" data-level="E.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-16"><i class="fa fa-check"></i><b>E.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>F</b> Workflow</a>
<ul>
<li class="chapter" data-level="F.1" data-path="ch-workflow.html"><a href="ch-workflow.html#building-a-model"><i class="fa fa-check"></i><b>F.1</b>  Building a model</a></li>
<li class="chapter" data-level="F.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-to-ask-on-a-model"><i class="fa fa-check"></i><b>F.2</b> Principled questions to ask on a model</a>
<ul>
<li class="chapter" data-level="F.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#checking-whether-assumptions-are-consistent-with-domain-expertise-prior-predictive-checks"><i class="fa fa-check"></i><b>F.2.1</b>  Checking whether assumptions are consistent with  domain expertise: Prior predictive checks</a></li>
<li class="chapter" data-level="F.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#testing-for-correct-posterior-approximations-checks-of-computational-faithfulness"><i class="fa fa-check"></i><b>F.2.2</b>  Testing for correct posterior approximations: Checks of computational faithfulness</a></li>
<li class="chapter" data-level="F.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#sensitivity-of-the-model"><i class="fa fa-check"></i><b>F.2.3</b>  Sensitivity of the model</a></li>
<li class="chapter" data-level="F.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#does-the-model-adequately-capture-the-dataposterior-predictive-checks"><i class="fa fa-check"></i><b>F.2.4</b>  Does the model adequately capture the data?–Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="F.3" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-17"><i class="fa fa-check"></i><b>F.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>G</b> Exercises</a>
<ul>
<li class="chapter" data-level="G.1" data-path="exercises.html"><a href="exercises.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>G.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="G.1.1" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart1"><i class="fa fa-check"></i><b>G.1.1</b> Practice using the <code>pnorm()</code> function–Part 1</a></li>
<li class="chapter" data-level="G.1.2" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart2"><i class="fa fa-check"></i><b>G.1.2</b> Practice using the <code>pnorm()</code> function–Part 2</a></li>
<li class="chapter" data-level="G.1.3" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart3"><i class="fa fa-check"></i><b>G.1.3</b> Practice using the <code>pnorm()</code> function–Part 3</a></li>
<li class="chapter" data-level="G.1.4" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisesqnormPart1"><i class="fa fa-check"></i><b>G.1.4</b> Practice using the <code>qnorm()</code> function–Part 1</a></li>
<li class="chapter" data-level="G.1.5" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisesqnormPart2"><i class="fa fa-check"></i><b>G.1.5</b> Practice using the <code>qnorm()</code> function–Part 2</a></li>
<li class="chapter" data-level="G.1.6" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisessamples1"><i class="fa fa-check"></i><b>G.1.6</b> Practice getting summaries from samples–Part 1</a></li>
<li class="chapter" data-level="G.1.7" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisessamples2"><i class="fa fa-check"></i><b>G.1.7</b> Practice getting summaries from samples–Part 2.</a></li>
<li class="chapter" data-level="G.1.8" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisesvcov1"><i class="fa fa-check"></i><b>G.1.8</b> Practice with a variance-covariance matrix for a bivariate distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.2" data-path="exercises.html"><a href="exercises.html#sec-BDAexercises"><i class="fa fa-check"></i><b>G.2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="G.2.1" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesDerivingBayes"><i class="fa fa-check"></i><b>G.2.1</b> Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="G.2.2" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj1"><i class="fa fa-check"></i><b>G.2.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="G.2.3" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj2"><i class="fa fa-check"></i><b>G.2.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="G.2.4" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj3"><i class="fa fa-check"></i><b>G.2.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="G.2.5" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj4"><i class="fa fa-check"></i><b>G.2.5</b> Conjugate forms 4</a></li>
<li class="chapter" data-level="G.2.6" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesWeightedMean"><i class="fa fa-check"></i><b>G.2.6</b> The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)</a></li>
</ul></li>
<li class="chapter" data-level="G.3" data-path="exercises.html"><a href="exercises.html#ex:compbda"><i class="fa fa-check"></i><b>G.3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="G.3.1" data-path="exercises.html"><a href="exercises.html#exr:simulatedlinearmod"><i class="fa fa-check"></i><b>G.3.1</b> Check for parameter recovery in a linear model using simulated data.</a></li>
<li class="chapter" data-level="G.3.2" data-path="exercises.html"><a href="exercises.html#exr:linearmod"><i class="fa fa-check"></i><b>G.3.2</b> A simple linear model.</a></li>
<li class="chapter" data-level="G.3.3" data-path="exercises.html"><a href="exercises.html#exr:compbda-biasedpost"><i class="fa fa-check"></i><b>G.3.3</b> Revisiting the button-pressing example with different priors.</a></li>
<li class="chapter" data-level="G.3.4" data-path="exercises.html"><a href="exercises.html#exr:ppd"><i class="fa fa-check"></i><b>G.3.4</b> Posterior predictive checks with a log-normal model.</a></li>
<li class="chapter" data-level="G.3.5" data-path="exercises.html"><a href="exercises.html#exr:skew"><i class="fa fa-check"></i><b>G.3.5</b> A skew normal distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.4" data-path="exercises.html"><a href="exercises.html#sec-LMexercises"><i class="fa fa-check"></i><b>G.4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="G.4.1" data-path="exercises.html"><a href="exercises.html#exr:powerposing"><i class="fa fa-check"></i><b>G.4.1</b> A simple linear regression: Power posing and testosterone.</a></li>
<li class="chapter" data-level="G.4.2" data-path="exercises.html"><a href="exercises.html#exr:pupils"><i class="fa fa-check"></i><b>G.4.2</b> Another linear regression model: Revisiting attentional load effect on pupil size.</a></li>
<li class="chapter" data-level="G.4.3" data-path="exercises.html"><a href="exercises.html#exr:lognormalm"><i class="fa fa-check"></i><b>G.4.3</b> Log-normal model: Revisiting the effect of trial on finger tapping times.</a></li>
<li class="chapter" data-level="G.4.4" data-path="exercises.html"><a href="exercises.html#exr:reg-logistic"><i class="fa fa-check"></i><b>G.4.4</b> Logistic regression: Revisiting the effect of set size on free recall.</a></li>
<li class="chapter" data-level="G.4.5" data-path="exercises.html"><a href="exercises.html#exr:red"><i class="fa fa-check"></i><b>G.4.5</b> Red is the sexiest color.</a></li>
</ul></li>
<li class="chapter" data-level="G.5" data-path="exercises.html"><a href="exercises.html#sec-HLMexercises"><i class="fa fa-check"></i><b>G.5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="G.5.1" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-normal"><i class="fa fa-check"></i><b>G.5.1</b> A hierarchical model (normal likelihood) of cognitive load on pupil size.</a></li>
<li class="chapter" data-level="G.5.2" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-logn"><i class="fa fa-check"></i><b>G.5.2</b> Are subject relatives easier to process than object relatives (log-normal likelihood)?</a></li>
<li class="chapter" data-level="G.5.3" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseMandarinRC"><i class="fa fa-check"></i><b>G.5.3</b> Relative clause processing in Mandarin Chinese</a></li>
<li class="chapter" data-level="G.5.4" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseEnglishAgrmt"><i class="fa fa-check"></i><b>G.5.4</b>  Agreement attraction in comprehension</a></li>
<li class="chapter" data-level="G.5.5" data-path="exercises.html"><a href="exercises.html#exr:ab"><i class="fa fa-check"></i><b>G.5.5</b>  Attentional blink (Bernoulli likelihood)</a></li>
<li class="chapter" data-level="G.5.6" data-path="exercises.html"><a href="exercises.html#exr:strooplogis-brms"><i class="fa fa-check"></i><b>G.5.6</b> Is there a Stroop effect in accuracy?</a></li>
<li class="chapter" data-level="G.5.7" data-path="exercises.html"><a href="exercises.html#exr:stroop-dist"><i class="fa fa-check"></i><b>G.5.7</b>  Distributional regression for the Stroop effect.</a></li>
<li class="chapter" data-level="G.5.8" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseGramCE"><i class="fa fa-check"></i><b>G.5.8</b> The  grammaticality illusion</a></li>
</ul></li>
<li class="chapter" data-level="G.6" data-path="exercises.html"><a href="exercises.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>G.6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="G.6.1" data-path="exercises.html"><a href="exercises.html#exr:ContrastsPersian"><i class="fa fa-check"></i><b>G.6.1</b> Contrast coding for a four-condition design</a></li>
<li class="chapter" data-level="G.6.2" data-path="exercises.html"><a href="exercises.html#exr:ContrastsNPIHelmert"><i class="fa fa-check"></i><b>G.6.2</b>  Helmert coding for a six-condition design.</a></li>
<li class="chapter" data-level="G.6.3" data-path="exercises.html"><a href="exercises.html#exr:ContrastsNcomparisons"><i class="fa fa-check"></i><b>G.6.3</b> Number of possible comparisons in a single model.</a></li>
</ul></li>
<li class="chapter" data-level="G.7" data-path="exercises.html"><a href="exercises.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>G.7</b> Contrast coding with two predictor variables</a>
<ul>
<li class="chapter" data-level="G.7.1" data-path="exercises.html"><a href="exercises.html#exr:ContrastsPersianANOVA"><i class="fa fa-check"></i><b>G.7.1</b> ANOVA coding for a four-condition design.</a></li>
<li class="chapter" data-level="G.7.2" data-path="exercises.html"><a href="exercises.html#exr:Contrasts2x2x2Dillon2013"><i class="fa fa-check"></i><b>G.7.2</b> ANOVA and nested comparisons in a <span class="math inline">\(2\times 2\times 2\)</span> design</a></li>
</ul></li>
<li class="chapter" data-level="G.8" data-path="exercises.html"><a href="exercises.html#introduction-to-the-probabilistic-programming-language-stan"><i class="fa fa-check"></i><b>G.8</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="G.8.1" data-path="exercises.html"><a href="exercises.html#exr:first"><i class="fa fa-check"></i><b>G.8.1</b> A very simple model.</a></li>
<li class="chapter" data-level="G.8.2" data-path="exercises.html"><a href="exercises.html#exr:badstan"><i class="fa fa-check"></i><b>G.8.2</b> Incorrect Stan model.</a></li>
<li class="chapter" data-level="G.8.3" data-path="exercises.html"><a href="exercises.html#exr:skewstan"><i class="fa fa-check"></i><b>G.8.3</b> Using Stan documentation.</a></li>
<li class="chapter" data-level="G.8.4" data-path="exercises.html"><a href="exercises.html#exr:linkfunction"><i class="fa fa-check"></i><b>G.8.4</b> The probit link function as an alternative to the logit function.</a></li>
<li class="chapter" data-level="G.8.5" data-path="exercises.html"><a href="exercises.html#exr:logisticstan"><i class="fa fa-check"></i><b>G.8.5</b> Examining the position of the queued word on recall.</a></li>
<li class="chapter" data-level="G.8.6" data-path="exercises.html"><a href="exercises.html#exr:fallacy"><i class="fa fa-check"></i><b>G.8.6</b> The conjunction fallacy.</a></li>
</ul></li>
<li class="chapter" data-level="G.9" data-path="exercises.html"><a href="exercises.html#hierarchical-models-and-reparameterization"><i class="fa fa-check"></i><b>G.9</b> Hierarchical models and reparameterization</a>
<ul>
<li class="chapter" data-level="G.9.1" data-path="exercises.html"><a href="exercises.html#exr:stroop"><i class="fa fa-check"></i><b>G.9.1</b> A log-normal model in Stan.</a></li>
<li class="chapter" data-level="G.9.2" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-logn-stan"><i class="fa fa-check"></i><b>G.9.2</b> A by-subjects and by-items hierarchical model with a log-normal likelihood.</a></li>
<li class="chapter" data-level="G.9.3" data-path="exercises.html"><a href="exercises.html#exr:strooplogis"><i class="fa fa-check"></i><b>G.9.3</b> A hierarchical logistic regression with Stan.</a></li>
<li class="chapter" data-level="G.9.4" data-path="exercises.html"><a href="exercises.html#exr:distr-stan"><i class="fa fa-check"></i><b>G.9.4</b> A distributional regression model of the effect of cloze probability on the N400.</a></li>
</ul></li>
<li class="chapter" data-level="G.10" data-path="exercises.html"><a href="exercises.html#sec-customexercises"><i class="fa fa-check"></i><b>G.10</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="G.10.1" data-path="exercises.html"><a href="exercises.html#exr:shiftedlogn"><i class="fa fa-check"></i><b>G.10.1</b> Fitting a  shifted log-normal distribution.</a></li>
<li class="chapter" data-level="G.10.2" data-path="exercises.html"><a href="exercises.html#exr:wald"><i class="fa fa-check"></i><b>G.10.2</b> Fitting a Wald distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.11" data-path="exercises.html"><a href="exercises.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>G.11</b> Meta-analysis and measurement error models</a>
<ul>
<li class="chapter" data-level="G.11.1" data-path="exercises.html"><a href="exercises.html#exr:REMAMEExtracting"><i class="fa fa-check"></i><b>G.11.1</b> Extracting estimates from published papers</a></li>
<li class="chapter" data-level="G.11.2" data-path="exercises.html"><a href="exercises.html#exr:REMAMEBuerki"><i class="fa fa-check"></i><b>G.11.2</b> A meta-analysis of picture-word interference data</a></li>
<li class="chapter" data-level="G.11.3" data-path="exercises.html"><a href="exercises.html#exr:REMAMELiEnglish"><i class="fa fa-check"></i><b>G.11.3</b> Measurement error model for English VOT data</a></li>
</ul></li>
<li class="chapter" data-level="G.12" data-path="exercises.html"><a href="exercises.html#introduction-to-model-comparison"><i class="fa fa-check"></i><b>G.12</b> Introduction to model comparison</a></li>
<li class="chapter" data-level="G.13" data-path="exercises.html"><a href="exercises.html#bayes-factors"><i class="fa fa-check"></i><b>G.13</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="G.13.1" data-path="exercises.html"><a href="exercises.html#exr:bysubjects"><i class="fa fa-check"></i><b>G.13.1</b> Is there evidence for differences in the effect of cloze probability among the subjects?</a></li>
<li class="chapter" data-level="G.13.2" data-path="exercises.html"><a href="exercises.html#exr:bf-logn"><i class="fa fa-check"></i><b>G.13.2</b> Is there evidence for the claim that English subject relative clauses are easier to process than object relative clauses?</a></li>
<li class="chapter" data-level="G.13.3" data-path="exercises.html"><a href="exercises.html#exr:bf-logistic"><i class="fa fa-check"></i><b>G.13.3</b> In the Grodner and Gibson 2005 data, in question-response accuracies, is there evidence for the claim that sentences with subject relative clauses are easier to comprehend?</a></li>
<li class="chapter" data-level="G.13.4" data-path="exercises.html"><a href="exercises.html#exr:lognstan"><i class="fa fa-check"></i><b>G.13.4</b> Bayes factor and bounded parameters using Stan.</a></li>
</ul></li>
<li class="chapter" data-level="G.14" data-path="exercises.html"><a href="exercises.html#cross-validation"><i class="fa fa-check"></i><b>G.14</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="G.14.1" data-path="exercises.html"><a href="exercises.html#exr:logcv"><i class="fa fa-check"></i><b>G.14.1</b> Predictive accuracy of the linear and the logarithm effect of cloze probability.</a></li>
<li class="chapter" data-level="G.14.2" data-path="exercises.html"><a href="exercises.html#exr:stroopcv"><i class="fa fa-check"></i><b>G.14.2</b> Log-normal model</a></li>
<li class="chapter" data-level="G.14.3" data-path="exercises.html"><a href="exercises.html#exr:logrec"><i class="fa fa-check"></i><b>G.14.3</b> Log-normal vs rec-normal model in Stan</a></li>
</ul></li>
<li class="chapter" data-level="G.15" data-path="exercises.html"><a href="exercises.html#introduction-to-cognitive-modeling"><i class="fa fa-check"></i><b>G.15</b> Introduction to cognitive modeling</a></li>
<li class="chapter" data-level="G.16" data-path="exercises.html"><a href="exercises.html#multinomial-processing-trees"><i class="fa fa-check"></i><b>G.16</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="G.16.1" data-path="exercises.html"><a href="exercises.html#exr:mult"><i class="fa fa-check"></i><b>G.16.1</b> Modeling multiple categorical responses.</a></li>
<li class="chapter" data-level="G.16.2" data-path="exercises.html"><a href="exercises.html#exr:mpt-mnm"><i class="fa fa-check"></i><b>G.16.2</b> An alternative MPT to model the picture recognition task.</a></li>
<li class="chapter" data-level="G.16.3" data-path="exercises.html"><a href="exercises.html#exr:edit-mpt-cat"><i class="fa fa-check"></i><b>G.16.3</b> A simple MPT model that incorporates phonological complexity in the picture recognition task.</a></li>
<li class="chapter" data-level="G.16.4" data-path="exercises.html"><a href="exercises.html#exr:mpt"><i class="fa fa-check"></i><b>G.16.4</b> A more hierarchical MPT.</a></li>
<li class="chapter" data-level="G.16.5" data-path="exercises.html"><a href="exercises.html#exr:mpt-adv"><i class="fa fa-check"></i><b>G.16.5</b> <strong>Advanced</strong>: Multinomial processing trees.</a></li>
</ul></li>
<li class="chapter" data-level="G.17" data-path="exercises.html"><a href="exercises.html#mixture-models"><i class="fa fa-check"></i><b>G.17</b> Mixture models</a>
<ul>
<li class="chapter" data-level="G.17.1" data-path="exercises.html"><a href="exercises.html#exr:pcorrect"><i class="fa fa-check"></i><b>G.17.1</b> Changes in the true point values.</a></li>
<li class="chapter" data-level="G.17.2" data-path="exercises.html"><a href="exercises.html#exr:mixhier"><i class="fa fa-check"></i><b>G.17.2</b> RTs in schizophrenic patients and control.</a></li>
<li class="chapter" data-level="G.17.3" data-path="exercises.html"><a href="exercises.html#exr:mixbias"><i class="fa fa-check"></i><b>G.17.3</b> <strong>Advanced:</strong> Guessing bias in the model.</a></li>
</ul></li>
<li class="chapter" data-level="G.18" data-path="exercises.html"><a href="exercises.html#a-simple-accumulator-model-to-account-for-choice-response-time"><i class="fa fa-check"></i><b>G.18</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="G.18.1" data-path="exercises.html"><a href="exercises.html#exr:recovery"><i class="fa fa-check"></i><b>G.18.1</b> Can we recover the true point values of the parameters of a model when dealing with a contaminant distribution?</a></li>
<li class="chapter" data-level="G.18.2" data-path="exercises.html"><a href="exercises.html#exr:lnracescale"><i class="fa fa-check"></i><b>G.18.2</b> Can the log-normal race model account for fast errors?</a></li>
<li class="chapter" data-level="G.18.3" data-path="exercises.html"><a href="exercises.html#exr:lnldt"><i class="fa fa-check"></i><b>G.18.3</b> Accounting for response time and choice in the lexical decision task using the log-normal race model.</a></li>
</ul></li>
<li class="chapter" data-level="G.19" data-path="exercises.html"><a href="exercises.html#sec-priorsexercises"><i class="fa fa-check"></i><b>G.19</b> The Art and Science of  Prior Elicitation</a>
<ul>
<li class="chapter" data-level="G.19.1" data-path="exercises.html"><a href="exercises.html#exr:PriorsRCs"><i class="fa fa-check"></i><b>G.19.1</b> Develop a plausible informative prior for the difference between object and subject relative clause reading times</a></li>
<li class="chapter" data-level="G.19.2" data-path="exercises.html"><a href="exercises.html#exr:Priorslocalcoherence"><i class="fa fa-check"></i><b>G.19.2</b> Extracting an informative prior from a published paper for a future study</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="exercises" class="section level1 hasAnchor" number="26">
<h1><span class="header-section-number">G</span> Exercises<a href="exercises.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="sec-Foundationsexercises" class="section level2 hasAnchor" number="26.1">
<h2><span class="header-section-number">G.1</span> Introduction<a href="exercises.html#sec-Foundationsexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:FoundationsexercisespnormPart1" class="section level3 unlisted hasAnchor" number="26.1.1">
<h3><span class="header-section-number">G.1.1</span> Practice using the <code>pnorm()</code> function–Part 1<a href="exercises.html#exr:FoundationsexercisespnormPart1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a normal distribution with mean 500 and standard deviation 100, use the <code>pnorm()</code> function to calculate the probability of obtaining values between 200 and 800 from this distribution.</p>
</div>
<div id="exr:FoundationsexercisespnormPart2" class="section level3 unlisted hasAnchor" number="26.1.2">
<h3><span class="header-section-number">G.1.2</span> Practice using the <code>pnorm()</code> function–Part 2<a href="exercises.html#exr:FoundationsexercisespnormPart2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Calculate the following probabilities.
Given a normal distribution with mean 800 and standard deviation 150, what is the probability of obtaining:</p>
<ul>
<li>a score of 700 or less</li>
<li>a score of 900 or more</li>
<li>a score of 800 or more</li>
</ul>
</div>
<div id="exr:FoundationsexercisespnormPart3" class="section level3 unlisted hasAnchor" number="26.1.3">
<h3><span class="header-section-number">G.1.3</span> Practice using the <code>pnorm()</code> function–Part 3<a href="exercises.html#exr:FoundationsexercisespnormPart3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a normal distribution with mean 600 and standard deviation 200, what is the probability of obtaining:</p>
<ul>
<li>a score of 550 or less.</li>
<li>a score between 300 and 800.</li>
<li>a score of 900 or more.</li>
</ul>
</div>
<div id="exr:FoundationsexercisesqnormPart1" class="section level3 unlisted hasAnchor" number="26.1.4">
<h3><span class="header-section-number">G.1.4</span> Practice using the <code>qnorm()</code> function–Part 1<a href="exercises.html#exr:FoundationsexercisesqnormPart1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a normal distribution with mean 1 and standard deviation 1.
Compute the lower and upper boundaries such that:</p>
<ul>
<li>the area (the probability) to the left of the lower boundary is 0.10.</li>
<li>the area (the probability) to the left of the upper boundary is 0.90.</li>
</ul>
</div>
<div id="exr:FoundationsexercisesqnormPart2" class="section level3 unlisted hasAnchor" number="26.1.5">
<h3><span class="header-section-number">G.1.5</span> Practice using the <code>qnorm()</code> function–Part 2<a href="exercises.html#exr:FoundationsexercisesqnormPart2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a normal distribution with mean 650 and standard deviation 125. There exist two quantiles, the lower quantile q1 and the upper quantile q2, that are equidistant from the mean 650, such that the  area under the curve of the normal between q1 and q2 is 80%. Find q1 and q2.</p>
</div>
<div id="exr:Foundationsexercisessamples1" class="section level3 unlisted hasAnchor" number="26.1.6">
<h3><span class="header-section-number">G.1.6</span> Practice getting summaries from samples–Part 1<a href="exercises.html#exr:Foundationsexercisessamples1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given data that is generated as follows:</p>
<div class="sourceCode" id="cb1199"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1199-1"><a href="exercises.html#cb1199-1" aria-hidden="true"></a>data_gen1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dt">mean =</span> <span class="dv">300</span>, <span class="dt">sd =</span> <span class="dv">200</span>)</span></code></pre></div>
<p>Calculate the mean, variance, and the lower quantile q1 and the upper quantile q2, that are equidistant and such that the range of probability between them is 80%.</p>
</div>
<div id="exr:Foundationsexercisessamples2" class="section level3 unlisted hasAnchor" number="26.1.7">
<h3><span class="header-section-number">G.1.7</span> Practice getting summaries from samples–Part 2.<a href="exercises.html#exr:Foundationsexercisessamples2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This time we generate the data with a truncated normal distribution from the package <code>extraDistr</code>. The details of this distribution will be discussed later in section <a href="ch-reg.html#sec-pupil">4.1</a> and in the online section <a href="regression-models-with-brms---extended.html#app-truncation">A.2</a>, but for now we can treat it as an unknown generative process:</p>
<div class="sourceCode" id="cb1200"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1200-1"><a href="exercises.html#cb1200-1" aria-hidden="true"></a>data_gen1 &lt;-<span class="st"> </span><span class="kw">rtnorm</span>(<span class="dv">1000</span>, <span class="dt">mean =</span> <span class="dv">300</span>, <span class="dt">sd =</span> <span class="dv">200</span>, <span class="dt">a =</span> <span class="dv">0</span>)</span></code></pre></div>
<p>Using the sample data, calculate the mean, variance, and the lower quantile q1 and the upper quantile q2, such that the probability of observing values between these two quantiles is 80%.</p>
</div>
<div id="exr:Foundationsexercisesvcov1" class="section level3 unlisted hasAnchor" number="26.1.8">
<h3><span class="header-section-number">G.1.8</span> Practice with a variance-covariance matrix for a bivariate distribution.<a href="exercises.html#exr:Foundationsexercisesvcov1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose that you have a bivariate distribution where one of the two random variables comes from a normal distribution with mean <span class="math inline">\(\mu_X=600\)</span> and standard deviation <span class="math inline">\(\sigma_X=100\)</span>, and the other from a normal distribution with mean <span class="math inline">\(\mu_Y=400\)</span> and standard deviation <span class="math inline">\(\sigma_Y=50\)</span>. The correlation <span class="math inline">\(\rho_{XY}\)</span> between the two random variables is <span class="math inline">\(0.4\)</span>. Write down the variance-covariance matrix of this bivariate distribution as a matrix (with numerical values, not mathematical symbols), and then use it to generate <span class="math inline">\(100\)</span> pairs of simulated data points. Plot the simulated data such that the relationship between the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is clear. Generate two sets of new data (<span class="math inline">\(100\)</span> pairs of data points each) with correlation <span class="math inline">\(-0.4\)</span> and <span class="math inline">\(0\)</span>, and plot these alongside the plot for the data with correlation <span class="math inline">\(0.4\)</span>.</p>
</div>
</div>
<div id="sec-BDAexercises" class="section level2 hasAnchor" number="26.2">
<h2><span class="header-section-number">G.2</span> Introduction to Bayesian data analysis<a href="exercises.html#sec-BDAexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:BDAexercisesDerivingBayes" class="section level3 unlisted hasAnchor" number="26.2.1">
<h3><span class="header-section-number">G.2.1</span> Deriving Bayes’ rule<a href="exercises.html#exr:BDAexercisesDerivingBayes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two observable events. <span class="math inline">\(P(A)\)</span> is the probability that <span class="math inline">\(A\)</span> occurs, and <span class="math inline">\(P(B)\)</span> is the probability that <span class="math inline">\(B\)</span> occurs. <span class="math inline">\(P(A|B)\)</span> is the conditional probability that <span class="math inline">\(A\)</span> occurs given that <span class="math inline">\(B\)</span> has happened. <span class="math inline">\(P(A,B)\)</span> is the joint probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> both occurring.</p>
<p>You are given the definition of conditional probability:</p>
<p><span class="math display">\[\begin{equation}
P(A|B)= \frac{P(A,B)}{P(B)} \hbox{ where } P(B)&gt;0
\end{equation}\]</span></p>
<p>Using the above definition, and using the fact that <span class="math inline">\(P(A,B)=P(B,A)\)</span> (i.e., the probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> both occurring is the same as the probability of <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> both occurring),
derive an expression for <span class="math inline">\(P(B|A)\)</span>. Show the steps clearly in the derivation.</p>
</div>
<div id="exr:BDAexercisesConj1" class="section level3 unlisted hasAnchor" number="26.2.2">
<h3><span class="header-section-number">G.2.2</span> Conjugate forms 1<a href="exercises.html#exr:BDAexercisesConj1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Computing the general form of a PDF for a posterior</li>
</ul>
<p>Suppose you are given data <span class="math inline">\(k\)</span> consisting of the number of successes, coming from a <span class="math inline">\(\mathit{Binomial}(n,\theta)\)</span> distribution.
Given <span class="math inline">\(k\)</span> successes in n trials coming from a binomial distribution, we define a <span class="math inline">\(\mathit{Beta}(a,b)\)</span> prior on the parameter <span class="math inline">\(\theta\)</span>.</p>
<p>Write down the Beta distribution that represents the posterior, in terms of <span class="math inline">\(a,b, n,\)</span> and <span class="math inline">\(k\)</span>.</p>
<ul>
<li>Practical application</li>
</ul>
<p>We ask 10 yes/no questions from a subject, and the subject returns 0 correct answers. We assume a binomial likelihood function for these data. Also assume a <span class="math inline">\(\mathit{Beta}(1,1)\)</span> prior on the parameter <span class="math inline">\(\theta\)</span>, which represents the probability of success. Use the result you derived above to write down the posterior distribution of the <span class="math inline">\(\theta\)</span> parameter.</p>
</div>
<div id="exr:BDAexercisesConj2" class="section level3 unlisted hasAnchor" number="26.2.3">
<h3><span class="header-section-number">G.2.3</span> Conjugate forms 2<a href="exercises.html#exr:BDAexercisesConj2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose that we perform <span class="math inline">\(n\)</span> independent trials until we get a success (e.g., a heads in a coin toss). For repeated coin tosses, observing T,T, H would correspond to a score of <span class="math inline">\(n=3\)</span>. The probability of success in each trial is <span class="math inline">\(\theta\)</span>. Then, the Geometric random variable, call it <span class="math inline">\(X\)</span>, gives us the probability of getting a success in <span class="math inline">\(n\)</span> trials as follows:</p>
<p><span class="math display">\[\begin{equation}
Prob(X=n)=\theta(1-\theta)^{ n-1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(n=1,2,\dots\)</span>.</p>
<p>Let the prior on <span class="math inline">\(\theta\)</span> be <span class="math inline">\(\mathit{Beta}(a,b)\)</span>, a beta distribution with parameters <span class="math inline">\(a\)</span>,<span class="math inline">\(b\)</span>.
The posterior distribution is a beta distribution with parameters <span class="math inline">\(a^*\)</span> and <span class="math inline">\(b^*\)</span>. Determine these parameters in terms of <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(n\)</span>.</p>
</div>
<div id="exr:BDAexercisesConj3" class="section level3 unlisted hasAnchor" number="26.2.4">
<h3><span class="header-section-number">G.2.4</span> Conjugate forms 3<a href="exercises.html#exr:BDAexercisesConj3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose that we have <span class="math inline">\(n\)</span> data points, <span class="math inline">\(x_1,\dots,x_n\)</span>, drawn independently from an exponential distribution with parameter <span class="math inline">\(\lambda\)</span>. The parameter of interest here (what we want to learn about from the data) is <span class="math inline">\(\lambda\)</span>.</p>
<p>The  exponential likelihood function is:</p>
<p><span class="math display">\[\begin{equation}
p(x_1,\dots,x_n | \lambda)=\lambda^n \exp (-\lambda \sum_{i=1}^n x_i )
\end{equation}\]</span></p>
<p>Starting with a Gamma prior distribution for <span class="math inline">\(\lambda\)</span> (see below), show that the posterior distribution for <span class="math inline">\(\lambda\)</span> is also a Gamma distribution. Provide formulas giving the posterior parameters <span class="math inline">\(a^*, b^*\)</span> in terms of the prior parameters <span class="math inline">\(a, b\)</span> and the data. Use the following facts about Gamma distributions.</p>
<p>The  Gamma distribution is defined in terms of the parameters <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>: <span class="math inline">\(\mathit{Gamma}(a,b)\)</span>. In general, if there is a random variable <span class="math inline">\(Y\)</span> (where <span class="math inline">\(y\geq 0\)</span>) that has a Gamma distribution as a PDF (<span class="math inline">\(Y\sim \mathit{Gamma}(a,b)\)</span>), then:</p>
<p><span class="math display">\[\begin{equation}
\mathit{Gamma}(y | a,b)=\frac{b^a y^{a-1} \exp(-by)}{\Gamma(a)}
\end{equation}\]</span></p>
<p>The <span class="math inline">\(\mathit{Gamma}(a,b)\)</span> prior on the <span class="math inline">\(\lambda\)</span> parameter in the exponential distribution will be written:</p>
<p><span class="math display">\[\begin{equation}
\mathit{Gamma}(\lambda | a,b)=\frac{b^a \lambda^{a-1} \exp(-b\lambda)}{\Gamma(a)}
\end{equation}\]</span></p>
</div>
<div id="exr:BDAexercisesConj4" class="section level3 unlisted hasAnchor" number="26.2.5">
<h3><span class="header-section-number">G.2.5</span> Conjugate forms 4<a href="exercises.html#exr:BDAexercisesConj4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Computing the posterior</li>
</ul>
<p>This is a contrived example. Suppose we are modeling the number of times that a speaker says the word “I” per day. This could be of interest if we are studying, for example, how self-oriented a speaker is. The number of times <span class="math inline">\(x\)</span> that the word is uttered in over a particular time period (here, one day) can be modeled by a  Poisson distribution (<span class="math inline">\(x=0,1,2,\dots\)</span>):</p>
<p><span class="math display">\[\begin{equation}
f(x\mid \theta) = \frac{\exp(-\theta) \theta^x}{x!} \hbox{ for } x=0,1,2,\dots 
\end{equation}\]</span></p>
<p>where the rate <span class="math inline">\(\theta\)</span> is unknown, and the numbers of utterances of the target word on each day are independent given <span class="math inline">\(\theta\)</span>.</p>
<p>As an aside: Given <span class="math inline">\(n\)</span> independent observations of a Poisson random variable with rate parameter <span class="math inline">\(\theta\)</span>, the maximum-likelihood estimator (MLE) for <span class="math inline">\(\theta\)</span> turns out to be <span class="math inline">\(\hat{\theta} = \frac{\sum_{i=1}^n x_i}{n}\)</span>. When we are talking about a particular sample of data, the maximum-likelihood estimate is computed using the formula for the estimator, <span class="math inline">\(\frac{\sum_{i=1}^n x_i}{n}\)</span>, and is represented as <span class="math inline">\(\bar{x}\)</span>.</p>
<p>We are told that the prior mean of <span class="math inline">\(\theta\)</span> is 100 and prior variance for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(225\)</span>. This information is based on the results of previous studies on the topic. We will use the <span class="math inline">\(\mathit{Gamma}(a,b)\)</span> density (see previous question) as a prior for <span class="math inline">\(\theta\)</span> because this is a conjugate prior to the Poisson distribution.</p>
<ol style="list-style-type: lower-alpha">
<li>First, visualize the prior, a Gamma density prior for <span class="math inline">\(\theta\)</span> based on the above information.</li>
</ol>
<p>[Hint: we know that for a Gamma density with parameters <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, the mean is <span class="math inline">\(\frac{a}{b}\)</span> and the variance is <span class="math inline">\(\frac{a}{b^2}\)</span>. Since we are given values for the mean and variance, we can solve for <span class="math inline">\(a,b\)</span>, which gives us the Gamma density.]</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Next, derive the posterior distribution of the parameter <span class="math inline">\(\theta\)</span> up to proportionality, and write down the posterior distribution in terms of the parameters of a Gamma distribution.</li>
</ol>
<ul>
<li>Practical application</li>
</ul>
<p>Suppose we know that the number of “I” utterances from a particular individual is <span class="math inline">\(115, 97, 79, 131\)</span>. Use the result you derived above to obtain the posterior distribution. In other words, write down the parameters of the Gamma distribution (call them <span class="math inline">\(a^*,b^*\)</span>) representing the posterior distribution of <span class="math inline">\(\theta\)</span>.</p>
<p>Plot the prior and the posterior distributions alongside each other.</p>
<p>Now suppose you get one new data point: 200. Using the posterior <span class="math inline">\(\mathit{Gamma}(a^*,b^*)\)</span> as your prior, write down the updated posterior (in terms of the updated parameters of the Gamma distribution) given this new data point. Add the updated posterior to the plot you made above.</p>
</div>
<div id="exr:BDAexercisesWeightedMean" class="section level3 unlisted hasAnchor" number="26.2.6">
<h3><span class="header-section-number">G.2.6</span> The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)<a href="exercises.html#exr:BDAexercisesWeightedMean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The number of times an event happens per unit time can be modeled using a Poisson distribution, whose PMF is:</p>
<p><span class="math display">\[\begin{equation}
f(x\mid \theta) = \frac{\exp(-\theta) \theta^x}{x!}
\end{equation}\]</span></p>
<p>Suppose that we define a Gamma(a,b) prior for the rate parameter <span class="math inline">\(\theta\)</span>. It is a fact (see exercises above) that the posterior of the <span class="math inline">\(\theta\)</span> parameter is a <span class="math inline">\(Gamma(a^*,b^*)\)</span> distribution, where <span class="math inline">\(a^*\)</span> and <span class="math inline">\(b^*\)</span> are the updated parameters given the data: <span class="math inline">\(\theta \sim Gamma(a^*,b^*)\)</span>.</p>
<ul>
<li>Prove that the posterior mean is a weighted mean of the prior mean and the maximum likelihood estimate (mean) of the Poisson-distributed data, <span class="math inline">\(\bar{x} = \sum_{i=1}^n x/n\)</span>. Hint: the mean of a Gamma distribution is <span class="math inline">\(\frac{a}{b}\)</span>. Specifically, what you have to prove is that:</li>
</ul>
<p><span class="math display" id="eq:weightingpoisga">\[\begin{equation}
\frac{a^*}{b^*} = \frac{a}{b} \times \frac{w_1}{w_1 + w_2} + \bar{x} \times \frac{w_2}{w_1 + w_2}
\tag{G.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(w_1 = 1\)</span> and <span class="math inline">\(w_2=\frac{n}{b}\)</span>.</p>
<ul>
<li><p>Given equation <a href="exercises.html#eq:weightingpoisga">(G.1)</a>, make an informal argument showing that as <span class="math inline">\(n\)</span> increases (as sample size goes up), the maximum likelihood estimate <span class="math inline">\(\bar{x}\)</span> dominates in determining the posterior mean, and when <span class="math inline">\(n\)</span> gets smaller and smaller, the prior mean dominates in determining the posterior mean.</p></li>
<li><p>Finally, given that the variance of a Gamma distribution is <span class="math inline">\(\frac{a}{b^2}\)</span>, show that as <span class="math inline">\(n\)</span> increases, the posterior variance will get smaller and smaller (the uncertainty on the posterior will go down).</p></li>
</ul>
</div>
</div>
<div id="ex:compbda" class="section level2 hasAnchor" number="26.3">
<h2><span class="header-section-number">G.3</span> Computational Bayesian data analysis<a href="exercises.html#ex:compbda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:simulatedlinearmod" class="section level3 unlisted hasAnchor" number="26.3.1">
<h3><span class="header-section-number">G.3.1</span> Check for parameter recovery in a linear model using simulated data.<a href="exercises.html#exr:simulatedlinearmod" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Generate some simulated independent and identically distributed data with <span class="math inline">\(n=100\)</span> data points as follows:</p>
<div class="sourceCode" id="cb1201"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1201-1"><a href="exercises.html#cb1201-1" aria-hidden="true"></a>y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">500</span>, <span class="dt">sd =</span> <span class="dv">50</span>)</span></code></pre></div>
<p>Next, fit a simple linear model with a normal likelihood:</p>
<p><span class="math display" id="eq:simrtlikLM">\[\begin{equation}
y_n  \sim \mathit{Normal}(\mu,\sigma) \tag{G.2}
\end{equation}\]</span></p>
<p>Specify the following priors:</p>
<p><span class="math display" id="eq:simrtpriors">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim \mathit{Uniform}(0, 60000) \\
\sigma &amp;\sim \mathit{Uniform}(0, 2000)
\end{aligned}
\tag{G.3}
\end{equation}\]</span></p>
<p>Generate posterior distributions of the parameters and check that the true values of the parameters <span class="math inline">\(\mu=500, \sigma=50\)</span> are recovered by the model. What this means is that you should check whether these true values lie within the range of the posterior distributions of the two parameters. This is a good sanity check for finding out whether a model can in principle recover the true parameter values correctly.</p>
</div>
<div id="exr:linearmod" class="section level3 unlisted hasAnchor" number="26.3.2">
<h3><span class="header-section-number">G.3.2</span> A simple linear model.<a href="exercises.html#exr:linearmod" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: lower-alpha">
<li><p>Fit the model <code>fit_press</code> with just a few iterations, say 50 iterations (set warmup to the default of 25, and use four chains). Does the model converge?</p></li>
<li><p>Using normal distributions, choose priors that better represent <strong>your</strong> assumptions/beliefs about finger tapping times. To think about a reasonable set of priors for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, you should come up with your own subjective assessment about what you think a reasonable range of values can be for <span class="math inline">\(\mu\)</span> and how much variability might happen. There is no correct answer here, we’ll discuss priors in depth in chapter <a href="ch-priors.html#ch-priors">E</a>. Fit this model to the data. Do the posterior distributions change?</p></li>
</ol>
</div>
<div id="exr:compbda-biasedpost" class="section level3 unlisted hasAnchor" number="26.3.3">
<h3><span class="header-section-number">G.3.3</span> Revisiting the button-pressing example with different priors.<a href="exercises.html#exr:compbda-biasedpost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: lower-alpha">
<li><p>Can you come up with very informative priors that influence the posterior in a noticeable way (use normal distributions for priors, not uniform priors)? Again, there are no correct answers here; you may have to try several different priors before you can noticeably influence the posterior.</p></li>
<li><p>Generate and plot prior predictive distributions based on this prior and plot them.</p></li>
<li><p>Generate posterior predictive distributions based on this prior and plot them.</p></li>
</ol>
</div>
<div id="exr:ppd" class="section level3 unlisted hasAnchor" number="26.3.4">
<h3><span class="header-section-number">G.3.4</span> Posterior predictive checks with a log-normal model.<a href="exercises.html#exr:ppd" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: lower-alpha">
<li>For the log-normal model <code>fit_press_ln</code>, change the prior of <span class="math inline">\(\sigma\)</span> so that it is a log-normal distribution with location (<span class="math inline">\(\mu\)</span>) of <span class="math inline">\(-2\)</span> and scale (<span class="math inline">\(\sigma\)</span>) of <span class="math inline">\(0.5\)</span>. What does such a prior imply about your belief regarding button-pressing times in milliseconds? Is it a good prior? Generate and plot prior predictive distributions. Do the new estimates change compared to earlier models when you fit the model?</li>
<li>For the log-normal model, what is the mean (rather than median) time that takes to press the space bar, what is the standard deviation of the finger tapping times in milliseconds?</li>
</ol>
</div>
<div id="exr:skew" class="section level3 unlisted hasAnchor" number="26.3.5">
<h3><span class="header-section-number">G.3.5</span> A skew normal distribution.<a href="exercises.html#exr:skew" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Would it make sense to use a “skew normal distribution” instead of the log-normal? The skew normal distribution has three parameters: location <span class="math inline">\(\xi\)</span> (this is the lower-case version of the Greek letter <span class="math inline">\(\Xi\)</span>, pronounced “chi”, with the “ch” pronounced like the “ch” in “Bach”), scale <span class="math inline">\(\omega\)</span> (omega), and shape <span class="math inline">\(\alpha\)</span>. The distribution is right skewed if <span class="math inline">\(\alpha &gt;0\)</span>, is left skewed if <span class="math inline">\(\alpha &lt;0\)</span>, and is identical to the regular normal distribution if <span class="math inline">\(\alpha =0\)</span>. For fitting this in <code>brms</code>, one needs to change <code>family</code> and set it to <code>skew_normal()</code>, and add a prior of <code>class = alpha</code> (location remains <code>class = Intercept</code> and scale, <code>class = sigma</code>).</p>
<ol style="list-style-type: lower-alpha">
<li>Fit this model with a prior that assigns approximately 95% of the prior probability of <code>alpha</code> to be between 0 and 10.</li>
<li>Generate posterior predictive distributions and compare the posterior distribution of summary statistics of the skew normal with the normal and log-normal.</li>
</ol>
</div>
</div>
<div id="sec-LMexercises" class="section level2 hasAnchor" number="26.4">
<h2><span class="header-section-number">G.4</span> Bayesian regression models<a href="exercises.html#sec-LMexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:powerposing" class="section level3 unlisted hasAnchor" number="26.4.1">
<h3><span class="header-section-number">G.4.1</span> A simple linear regression: Power posing and testosterone.<a href="exercises.html#exr:powerposing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Load the following data set:</p>
<div class="sourceCode" id="cb1202"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1202-1"><a href="exercises.html#cb1202-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_powerpose&quot;</span>)</span>
<span id="cb1202-2"><a href="exercises.html#cb1202-2" aria-hidden="true"></a><span class="kw">head</span>(df_powerpose)</span></code></pre></div>
<pre><code>##   id hptreat female age testm1 testm2
## 2 29    High   Male  19   38.7   62.4
## 3 30     Low Female  20   32.8   29.2
## 4 31    High Female  20   32.3   27.5
## 5 32     Low Female  18   18.0   28.7
## 7 34     Low Female  21   73.6   44.7
## 8 35    High Female  20   80.7  105.5</code></pre>
<p>The data set, which was originally published in <span class="citation">Carney, Cuddy, and Yap (<a href="#ref-carney2010power" role="doc-biblioref">2010</a>)</span> but released in modified form by <span class="citation">Fosse (<a href="#ref-FossePowerPose" role="doc-biblioref">2016</a>)</span>, shows the testosterone levels of 39 different individuals, before and after treatment, where treatment refers to each individual being assigned to a high power pose or a low power pose. In the original paper by <span class="citation">Carney, Cuddy, and Yap (<a href="#ref-carney2010power" role="doc-biblioref">2010</a>)</span>, the unit given for testosterone measurement (estimated from saliva samples) was picograms per milliliter (pg/ml). One picogram per milliliter is 0.001 nanogram per milliliter (ng/ml).</p>
<p>The research hypothesis is that on average, assigning a subject a high power pose vs. a low power pose will lead to higher testosterone levels after treatment. Assuming that you know nothing about typical ranges of testosterone using salivary measurement, you can use the default priors in <code>brms</code> for the target parameter(s).</p>
<p>Investigate this claim using a linear model and the default priors of <code>brms</code>. You’ll need to estimate the effect of a new variable that encodes the change in testosterone.</p>
</div>
<div id="exr:pupils" class="section level3 unlisted hasAnchor" number="26.4.2">
<h3><span class="header-section-number">G.4.2</span> Another linear regression model: Revisiting attentional load effect on pupil size.<a href="exercises.html#exr:pupils" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we revisit the analysis shown in the chapter, on how attentional load affects pupil size.</p>
<ol style="list-style-type: lower-alpha">
<li>Our priors for this experiment were quite arbitrary. How do the prior predictive distributions look like? Do they make sense?</li>
<li>Is our posterior distribution sensitive to the priors that we selected? Perform a sensitivity analysis to find out whether the posterior is affected by our choice of prior for the <span class="math inline">\(\sigma\)</span>.</li>
<li>Our data set includes also a column that indicates the trial number. Could it be that trial has also an effect on the pupil size? As in <code>lm()</code>, we indicate another main effect with a <code>+</code> sign. How would you communicate the new results?</li>
</ol>
</div>
<div id="exr:lognormalm" class="section level3 unlisted hasAnchor" number="26.4.3">
<h3><span class="header-section-number">G.4.3</span> Log-normal model: Revisiting the effect of trial on finger tapping times.<a href="exercises.html#exr:lognormalm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We continue considering the effect of trial on finger tapping times.</p>
<ol style="list-style-type: lower-alpha">
<li>Estimate the slowdown in milliseconds between the last two times the subject pressed the space bar in the experiment.</li>
<li>How would you change your model (keeping the log-normal likelihood) so that it includes centered log-transformed trial numbers or square-root-transformed trial numbers (instead of centered trial numbers)? Does the effect in milliseconds change?</li>
</ol>
</div>
<div id="exr:reg-logistic" class="section level3 unlisted hasAnchor" number="26.4.4">
<h3><span class="header-section-number">G.4.4</span> Logistic regression: Revisiting the effect of set size on free recall.<a href="exercises.html#exr:reg-logistic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our data set includes also a column coded as <code>tested</code> that indicates the position of the queued word. (In Figure <a href="ch-reg.html#fig:oberauer">4.9</a> <code>tested</code> would be 3). Could it be that position also has an effect on recall accuracy? How would you incorporate this in the model? (We indicate another main effect with a <code>+</code> sign).</p>
</div>
<div id="exr:red" class="section level3 unlisted hasAnchor" number="26.4.5">
<h3><span class="header-section-number">G.4.5</span> Red is the sexiest color.<a href="exercises.html#exr:red" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Load the following data set:</p>
<div class="sourceCode" id="cb1204"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1204-1"><a href="exercises.html#cb1204-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_red&quot;</span>)</span>
<span id="cb1204-2"><a href="exercises.html#cb1204-2" aria-hidden="true"></a><span class="kw">head</span>(df_red)</span></code></pre></div>
<pre><code>##    risk age red pink redorpink
## 8     0  19   0    0         0
## 9     0  25   0    0         0
## 10    0  20   0    0         0
## 11    0  20   0    0         0
## 14    0  20   0    0         0
## 15    0  18   0    0         0</code></pre>
<p>The data set is from a study <span class="citation">(Beall and Tracy <a href="#ref-beall2013women" role="doc-biblioref">2013</a>)</span> that contains information about the color of the clothing worn (red, pink, or red or pink) when the subject (female) is at risk of becoming pregnant (is ovulating, self-reported). The broader issue being investigated is whether women wear red more often when they are ovulating (in order to attract a mate). Using logistic regressions, fit three different models to investigate whether being ovulating increases the probability of wearing (a) red, (b) pink, or (c) either pink or red. Use priors that are reasonable (in your opinion).</p>
</div>
</div>
<div id="sec-HLMexercises" class="section level2 hasAnchor" number="26.5">
<h2><span class="header-section-number">G.5</span> Bayesian hierarchical models<a href="exercises.html#sec-HLMexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:hierarchical-normal" class="section level3 unlisted hasAnchor" number="26.5.1">
<h3><span class="header-section-number">G.5.1</span> A hierarchical model (normal likelihood) of cognitive load on pupil size.<a href="exercises.html#exr:hierarchical-normal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As in section <a href="ch-reg.html#sec-pupil">4.1</a>, we focus on the effect of cognitive load on pupil size, but this time we look at all the subjects of <span class="citation">Wahn et al. (<a href="#ref-wahnPupilSizesScale2016" role="doc-biblioref">2016</a>)</span>:</p>
<div class="sourceCode" id="cb1206"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1206-1"><a href="exercises.html#cb1206-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_pupil_complete&quot;</span>)</span>
<span id="cb1206-2"><a href="exercises.html#cb1206-2" aria-hidden="true"></a>df_pupil_complete</span></code></pre></div>
<pre><code>## # A tibble: 2,228 × 4
##    subj trial  load p_size
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
## 1   701     1     2  1021.
## 2   701     2     1   951.
## 3   701     3     5  1064.
## # ℹ 2,225 more rows</code></pre>
<p>You should be able to now fit a “maximal” model (correlated varying intercept and slopes for subjects) assuming a normal likelihood. Base your priors in the priors discussed in section <a href="ch-reg.html#sec-pupil">4.1</a>.</p>
<ol style="list-style-type: lower-alpha">
<li>Examine the effect of load on pupil size, and the average pupil size. What do you conclude?</li>
<li>Do a sensitivity analysis for the prior on the intercept (<span class="math inline">\(\alpha\)</span>). What is the estimate of the effect (<span class="math inline">\(\beta\)</span>) under different priors?</li>
<li>Is the effect of load consistent across subjects? Investigate this visually.</li>
</ol>
</div>
<div id="exr:hierarchical-logn" class="section level3 unlisted hasAnchor" number="26.5.2">
<h3><span class="header-section-number">G.5.2</span> Are subject relatives easier to process than object relatives (log-normal likelihood)?<a href="exercises.html#exr:hierarchical-logn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We begin with a classic question from the psycholinguistics literature: Are subject relatives easier to process than object relatives? The data come from Experiment 1 in a paper by <span class="citation">Grodner and Gibson (<a href="#ref-grodner" role="doc-biblioref">2005</a>)</span>.</p>
<p><em>Scientific question</em>: Is there a subject relative advantage in reading?</p>
<p><span class="citation">Grodner and Gibson (<a href="#ref-grodner" role="doc-biblioref">2005</a>)</span> investigate an old claim in psycholinguistics that  object relative clause (ORC) sentences are more difficult to process than  subject relative clause (SRC) sentences. One explanation for this predicted difference is that the distance between the relative clause verb (<em>sent</em> in the example below) and the head noun phrase of the relative clause (<em>reporter</em> in the example below) is longer in ORC vs. SRC. Examples are shown below. The relative clause is shown in square brackets.</p>
<p>(1a) The <em>reporter</em> [who the photographer <em>sent</em> to the editor] was hoping for a good story. (ORC)</p>
<p>(1b) The <em>reporter</em> [who <em>sent</em> the photographer to the editor] was hoping for a good story. (SRC)</p>
<p>The underlying explanation has to do with memory processes: Shorter linguistic dependencies are easier to process due to either reduced interference or decay, or both. For implemented computational models that spell this point out, see <span class="citation">Lewis and Vasishth (<a href="#ref-lewisvasishth:cogsci05" role="doc-biblioref">2005</a>)</span> and <span class="citation">Engelmann, Jäger, and Vasishth (<a href="#ref-EngelmannJaegerVasishth2019" role="doc-biblioref">2020</a>)</span>.</p>
<p>In the Grodner and Gibson data, the dependent measure is  reading time at the relative clause verb, (e.g., <em>sent</em>) of different sentences with either ORC or SRC. The dependent variable is in milliseconds and was measured in a self-paced reading task.  Self-paced reading is a task where subjects read a sentence or a short text word-by-word or phrase-by-phrase, pressing a button to get each word or phrase displayed; the preceding word disappears every time the button is pressed. In <a href="ch-priors.html#sec-simpleexamplepriors">E.1</a>, we provide a more detailed explanation of this experimental method.</p>
<p>For this experiment, we are expecting longer reading times at the relative clause verbs of ORC sentences in comparison to the relative clause verb of SRC sentences.</p>
<div class="sourceCode" id="cb1208"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1208-1"><a href="exercises.html#cb1208-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_gg05_rc&quot;</span>)</span>
<span id="cb1208-2"><a href="exercises.html#cb1208-2" aria-hidden="true"></a>df_gg05_rc</span></code></pre></div>
<pre><code>## # A tibble: 672 × 7
##    subj  item condition    RT residRT qcorrect experiment
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;     
## 1     1     1 objgap      320   -21.4        0 tedrg3    
## 2     1     2 subjgap     424    74.7        1 tedrg2    
## 3     1     3 objgap      309   -40.3        0 tedrg3    
## # ℹ 669 more rows</code></pre>
<p>You should use a sum coding for the predictors. Here, object relative clauses (<code>"objgaps"</code>) are coded <span class="math inline">\(+1/2\)</span>, subject relative clauses
<span class="math inline">\(-1/2\)</span>.</p>
<div class="sourceCode" id="cb1210"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1210-1"><a href="exercises.html#cb1210-1" aria-hidden="true"></a>df_gg05_rc &lt;-<span class="st"> </span>df_gg05_rc <span class="op">%&gt;%</span></span>
<span id="cb1210-2"><a href="exercises.html#cb1210-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cond =</span> <span class="kw">if_else</span>(condition <span class="op">==</span><span class="st"> &quot;objgap&quot;</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">-1</span><span class="op">/</span><span class="dv">2</span>))</span></code></pre></div>
<p>You should be able to now fit a “maximal” model (correlated varying intercept and slopes for subjects and for items) assuming a log-normal likelihood.</p>
<ol style="list-style-type: lower-alpha">
<li>Examine the effect of relative clause attachment site (the predictor <code>c_cond</code>) on reading times <code>RT</code> (<span class="math inline">\(\beta\)</span>).</li>
<li>Estimate the median difference between relative clause attachment sites in milliseconds, and report the mean and 95% CI.</li>
<li>Do a sensitivity analysis. What is the estimate of the effect (<span class="math inline">\(\beta\)</span>) under different priors? What is the difference in milliseconds between conditions under different priors?</li>
</ol>
</div>
<div id="exr:HLMExerciseMandarinRC" class="section level3 unlisted hasAnchor" number="26.5.3">
<h3><span class="header-section-number">G.5.3</span> Relative clause processing in Mandarin Chinese<a href="exercises.html#exr:HLMExerciseMandarinRC" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Load the following two data sets:</p>
<div class="sourceCode" id="cb1211"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1211-1"><a href="exercises.html#cb1211-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_gibsonwu&quot;</span>)</span>
<span id="cb1211-2"><a href="exercises.html#cb1211-2" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_gibsonwu2&quot;</span>)</span></code></pre></div>
<p>The data are taken from two experiments that investigate (inter alia) the effect of relative clause type on reading time in Chinese. The data are from <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu" role="doc-biblioref">2013</a>)</span> and <span class="citation">Vasishth et al. (<a href="#ref-VasishthetalPLoSOne2013" role="doc-biblioref">2013</a>)</span> respectively. The second data set is a direct replication attempt of the <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu" role="doc-biblioref">2013</a>)</span> experiment.</p>
<p>Chinese relative clauses are interesting theoretically because they are prenominal: the relative clause appears before the head noun. For example, the English relative clauses shown above would appear in the following order in Mandarin. The square brackets mark the relative clause, and REL refers to the Chinese equivalent of the English relative pronoun <em>who</em>.</p>
<p>(2a) [The photographer <em>sent</em> to the editor] REL the <em>reporter</em> was hoping for a good story. (ORC)</p>
<p>(2b) [<em>sent</em> the photographer to the editor] REL the <em>reporter</em> who was hoping for a good story. (SRC)</p>
<p>As discussed in <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu" role="doc-biblioref">2013</a>)</span>, the consequence of Chinese relative clauses being prenominal is that the distance between the verb in relative clause and the head noun is larger in subject relatives than object relatives. <span class="citation">Hsiao and Gibson (<a href="#ref-hsiao03" role="doc-biblioref">2003</a>)</span> were the first to suggest that the larger distance in subject relatives leads to longer reading time at the head noun. Under this view, the prediction is that subject relatives are harder to process than object relatives. If this is true, this is interesting and surprising because in most other languages that have been studied, subject relatives are easier to process than object relatives; so Chinese will be a very unusual exception cross-linguistically.</p>
<p>The data provided are for the critical region (the head noun; here, <em>reporter</em>). The experiment method is self-paced reading, so we have reading times in milliseconds. The second data set is a direct replication attempt of the first data set, which is from <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu" role="doc-biblioref">2013</a>)</span>.</p>
<p>The research hypothesis is whether the difference in reading times between object and subject relative clauses is negative. For the first data set (<code>df_gibsonwu</code>), investigate this question by fitting two “maximal” hierarchical models (correlated varying intercept and slopes for subjects and items). The dependent variable in both models is the raw reading time in milliseconds. The first model should use the normal likelihood in the model; the second model should use the log-normal likelihood. In both models, use <span class="math inline">\(\pm 0.5\)</span> sum coding to model the effect of relative clause type. You will need to decide on appropriate priors for the various parameters.</p>
<ol style="list-style-type: lower-alpha">
<li>Plot the posterior predictive distributions from the two models. What is the difference in the posterior predictive distributions of the two models; and why is there a difference?</li>
<li>Examine the posterior distributions of the effect estimates (in milliseconds) in the two models. Why are these different?</li>
<li>Given the posterior predictive distributions you plotted above, why is the log-normal likelihood model better for carrying out inference and hypothesis testing?</li>
</ol>
<p>Next, work out a normal approximation of the log-normal model’s posterior distribution for the relative clause effect that you obtained from the above data analysis. Then use that normal approximation as an informative prior for the slope parameter when fitting a hierarchical model to the second data set. This is an example of incrementally building up knowledge by successively using a previous study’s posterior as a prior for the next study; this is essentially equivalent to pooling both data sets (check that pooling the data and using a Normal(0,1) prior for the effect of interest, with a log-normal likelihood, gives you approximately the same posterior as the informative-prior model fit above).</p>
</div>
<div id="exr:HLMExerciseEnglishAgrmt" class="section level3 unlisted hasAnchor" number="26.5.4">
<h3><span class="header-section-number">G.5.4</span>  Agreement attraction in comprehension<a href="exercises.html#exr:HLMExerciseEnglishAgrmt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Load the following data:</p>
<div class="sourceCode" id="cb1212"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1212-1"><a href="exercises.html#cb1212-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_dillonE1&quot;</span>)</span>
<span id="cb1212-2"><a href="exercises.html#cb1212-2" aria-hidden="true"></a>dillonE1 &lt;-<span class="st"> </span>df_dillonE1</span>
<span id="cb1212-3"><a href="exercises.html#cb1212-3" aria-hidden="true"></a><span class="kw">head</span>(dillonE1)</span></code></pre></div>
<pre><code>##         subj       item   rt int     expt
## 49 dillonE11 dillonE119 2918 low dillonE1
## 56 dillonE11 dillonE119 1338 low dillonE1
## 63 dillonE11 dillonE119  424 low dillonE1
## 70 dillonE11 dillonE119  186 low dillonE1
## 77 dillonE11 dillonE119  195 low dillonE1
## 84 dillonE11 dillonE119 1218 low dillonE1</code></pre>
<p>The data are taken from an experiment that investigate (inter alia) the effect of number similarity between a noun and the auxiliary verb in sentences like the following. There are two levels to a factor called Int(erference): low and high.</p>
<p>(3a) low: The key to the cabinet <em>are</em> on the table
(3b) high: The key to the <em>cabinets</em> <em>are</em> on the table</p>
<p>Here, in (3b), the auxiliary verb <em>are</em> is predicted to be read faster than in (3a), because the plural marking on the noun <em>cabinets</em> leads the reader to think that the sentence is grammatical. (Both sentences are ungrammatical.) This phenomenon, where the high condition is read faster than the low condition, is called <strong>agreement attraction</strong>.</p>
<p>The data provided are for the critical region (the auxiliary verb <em>are</em>). The experiment method is eye-tracking; we have total reading times in milliseconds.</p>
<p>The research question is whether the difference in reading times between high and low conditions is negative.</p>
<ul>
<li>First, using a log-normal likelihood, fit a hierarchical model with correlated varying intercept and slopes for subjects and items. You will need to decide on the priors for the model.</li>
<li>By simply looking at the posterior distribution of the slope parameter <span class="math inline">\(\beta\)</span>, what would you conclude about the theoretical claim relating to agreement attraction?</li>
</ul>
</div>
<div id="exr:ab" class="section level3 unlisted hasAnchor" number="26.5.5">
<h3><span class="header-section-number">G.5.5</span>  Attentional blink (Bernoulli likelihood)<a href="exercises.html#exr:ab" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The attentional blink <span class="citation">(AB; first described by Raymond, Shapiro, and Arnell <a href="#ref-raymond1992temporary" role="doc-biblioref">1992</a>; though it has been noticed before e.g., Broadbent and Broadbent <a href="#ref-Broadbent1987" role="doc-biblioref">1987</a>)</span> refers to a temporary reduction in the accuracy of detecting a <em>probe</em> (e.g., a letter “X”) presented closely after a <em>target</em> that has been detected (e.g., a white letter). We will focus on the experimental condition of Experiment 2 of <span class="citation">Raymond, Shapiro, and Arnell (<a href="#ref-raymond1992temporary" role="doc-biblioref">1992</a>)</span>. Subjects are presented with letters in  rapid serial visual presentation (RSVP) at the center of the screen at a constant rate and are required to identify the only white letter (target) in the stream of black letters, and then to report whether the letter X (probe) occurred in the subsequent letter stream. The AB is defined as having occurred when the target is reported correctly but the report of the probe is inaccurate at a short <em>lag</em> or <em>target-probe</em> interval.</p>
<p>The data set <code>df_ab</code> is a subset of the data of this paradigm from a replication conducted by <span class="citation">Grassi et al. (<a href="#ref-grassi_two_2021" role="doc-biblioref">2021</a>)</span>. In this subset, the probe was always present and the target was correctly identified. We want to find out how the lag affects the accuracy of the identification of the probe.</p>
<div class="sourceCode" id="cb1214"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1214-1"><a href="exercises.html#cb1214-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_ab&quot;</span>)</span>
<span id="cb1214-2"><a href="exercises.html#cb1214-2" aria-hidden="true"></a>df_ab</span></code></pre></div>
<pre><code>## # A tibble: 2,101 × 4
##    subj probe_correct trial   lag
##   &lt;int&gt;         &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1     1             0     2     5
## 2     1             1     4     4
## 3     1             1     8     6
## # ℹ 2,098 more rows</code></pre>
<p>Fit a logistic regression assuming a linear relationship between <code>lag</code> and accuracy (<code>probe_correct</code>). Assume a hierarchical structure with correlated varying intercept and slopes for subjects. You will need to decide on the priors for this model.</p>
<ol style="list-style-type: lower-alpha">
<li>How is the accuracy of the probe identification affected by the lag? Estimate this in log-odds and percentages.</li>
<li>Is the linear relationship justified? Use posterior predictive checks to verify this.</li>
<li>Can you think about a better relationship between lag and accuracy? Fit a new model and use posterior predictive checks to verify if the fit improved.</li>
</ol>
</div>
<div id="exr:strooplogis-brms" class="section level3 unlisted hasAnchor" number="26.5.6">
<h3><span class="header-section-number">G.5.6</span> Is there a Stroop effect in accuracy?<a href="exercises.html#exr:strooplogis-brms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Instead of the response times of the correct answers, we want to find out whether accuracy also changes by condition in the Stroop task. Fit the Stroop data with a hierarchical logistic regression (i.e., a Bernoulli likelihood with a logit link). Use the complete data set, <code>df_stroop_complete</code> which also includes incorrect answers, and subset it selecting the first 50 subjects.</p>
<ol style="list-style-type: lower-alpha">
<li>Fit the model.</li>
<li>Report the Stroop effect in log-odds and accuracy.</li>
</ol>
</div>
<div id="exr:stroop-dist" class="section level3 unlisted hasAnchor" number="26.5.7">
<h3><span class="header-section-number">G.5.7</span>  Distributional regression for the Stroop effect.<a href="exercises.html#exr:stroop-dist" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will relax some of the assumptions of the model of Stroop presented in section <a href="ch-hierarchical.html#sec-stroop">5.3</a>. We will no longer assume that all subjects share the same variance component, and, in addition, we’ll investigate whether the experimental manipulation affects the scale of the response times. A reasonable hypothesis could be that the incongruent condition is noisier than the congruent one.</p>
<p>Assume the following likelihood, and fit the model with sensible priors (recall that our initial prior for <span class="math inline">\(\beta\)</span> wasn’t reasonable). (Priors for all the <code>sigma</code> parameters require us to set <code>dpar = sigma</code>).</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  rt_n &amp;\sim \mathit{LogNormal}(\alpha + u_{subj[n],1}  + c\_cond_n \cdot  (\beta + u_{subj[n],2}), \sigma_n)\\
  \sigma_n &amp;= \exp(\sigma_\alpha + \sigma_{u_{subj[n],1}} + c\_cond \cdot (\sigma_\beta + \sigma_{u_{subj[n],2}}) )
\end{aligned}
\end{equation}\]</span></p>
<p>In this likelihood <span class="math inline">\(\sigma_n\)</span> has both population- and group-level parameters: <span class="math inline">\(\sigma_\alpha\)</span> and <span class="math inline">\(\sigma_\beta\)</span> are the intercept and slope of the population level effects repectively, and <span class="math inline">\(\sigma_{u_{subj[n],1}}\)</span> and <span class="math inline">\(\sigma_{u_{subj[n],2}}\)</span> are the intercept and slope of the group-level effects.</p>
<ol style="list-style-type: lower-alpha">
<li>Is our hypothesis reasonable in light of the results?</li>
<li>Why is the intercept for the scale negative?</li>
<li>What’s the posterior estimate of the scale for congruent and incongruent conditions?</li>
</ol>
</div>
<div id="exr:HLMExerciseGramCE" class="section level3 unlisted hasAnchor" number="26.5.8">
<h3><span class="header-section-number">G.5.8</span> The  grammaticality illusion<a href="exercises.html#exr:HLMExerciseGramCE" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Load the following two data sets:</p>
<div class="sourceCode" id="cb1216"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1216-1"><a href="exercises.html#cb1216-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_english&quot;</span>)</span>
<span id="cb1216-2"><a href="exercises.html#cb1216-2" aria-hidden="true"></a>english &lt;-<span class="st"> </span>df_english</span>
<span id="cb1216-3"><a href="exercises.html#cb1216-3" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_dutch&quot;</span>)</span>
<span id="cb1216-4"><a href="exercises.html#cb1216-4" aria-hidden="true"></a>dutch &lt;-<span class="st"> </span>df_dutch</span></code></pre></div>
<p>In an offline accuracy rating study on English double center-embedding constructions, <span class="citation">Gibson and Thomas (<a href="#ref-gibsonthomas99" role="doc-biblioref">1999</a>)</span> found that grammatical constructions (e.g., example 4a below) were no less acceptable than ungrammatical constructions (e.g., example 4b) where a middle verb phrase (e.g., <em>was cleaning every week</em>) was missing.</p>
<p>(4a) The apartment that the maid who the service had sent over was cleaning every week was well decorated.</p>
<p>(4b) *The apartment that the maid who the service had sent over — was well decorated</p>
<p>Based on these results from English, <span class="citation">Gibson and Thomas (<a href="#ref-gibsonthomas99" role="doc-biblioref">1999</a>)</span> proposed that working-memory overload leads the comprehender to forget the prediction of the upcoming verb phrase (VP), which reduces working-memory load. This came to be known as the  <em>VP-forgetting hypothesis</em>. The prediction is that in the word immediately following the final verb, the grammatical condition (which is coded as +1 in the data frames) should be harder to read than the ungrammatical condition (which is coded as -1).</p>
<p>The design shown above is set up to test this hypothesis using self-paced reading for English <span class="citation">(Vasishth et al. <a href="#ref-VSLK08" role="doc-biblioref">2011</a>)</span>, and for Dutch <span class="citation">(Frank, Trompenaars, and Vasishth <a href="#ref-FrankEtAl2015" role="doc-biblioref">2015</a>)</span>. The data provided are for the critical region (the noun phrase, labeled NP1, following the final verb); this is the region for which the theory predicts differences between the two conditions. We have reading times in log milliseconds.</p>
<ol style="list-style-type: lower-alpha">
<li>First, fit a linear model with a full hierarchical structure by subjects and by items for the English data. Because we have log milliseconds data, we can simply use the normal likelihood (not the log-normal). What scale will be the parameters be in, milliseconds or log milliseconds?</li>
<li>Second, using the posterior for the effect of interest from the English data, derive a prior distribution for the effect in the Dutch data. Then fit two linear mixed models: (i) one model with relatively uninformative priors for <span class="math inline">\(\beta\)</span> (for example, <span class="math inline">\(Normal(0,1)\)</span>), and (ii) one model with the prior for <span class="math inline">\(\beta\)</span> you derived from the English data. Do the posterior distributions of the Dutch data’s effect show any important differences given the two priors? If yes, why; if not, why not?</li>
<li>Finally, just by looking at the English and Dutch posteriors, what can we say about the VP-forgetting hypothesis? Are the posteriors of the effect from these two languages consistent with the hypothesis?</li>
</ol>
</div>
</div>
<div id="sec-Contrastsexercises" class="section level2 hasAnchor" number="26.6">
<h2><span class="header-section-number">G.6</span> Contrast coding<a href="exercises.html#sec-Contrastsexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:ContrastsPersian" class="section level3 unlisted hasAnchor" number="26.6.1">
<h3><span class="header-section-number">G.6.1</span> Contrast coding for a four-condition design<a href="exercises.html#exr:ContrastsPersian" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Load the following data. These data are from Experiment 1 in a set of reading studies on Persian <span class="citation">(Safavi, Husain, and Vasishth <a href="#ref-SafaviEtAlFrontiers2016" role="doc-biblioref">2016</a>)</span>. This is a self-paced reading study on particle-verb constructions, with a <span class="math inline">\(2\times 2\)</span> design: distance (short, long) and predictability (predictable, unpredictable). The data are from a critical region in the sentence. All the data from the <span class="citation">Safavi, Husain, and Vasishth (<a href="#ref-SafaviEtAlFrontiers2016" role="doc-biblioref">2016</a>)</span> paper are available from <a href="https://github.com/vasishth/SafaviEtAl2016" class="uri">https://github.com/vasishth/SafaviEtAl2016</a>.</p>
<div class="sourceCode" id="cb1217"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1217-1"><a href="exercises.html#cb1217-1" aria-hidden="true"></a><span class="kw">library</span>(bcogsci)</span>
<span id="cb1217-2"><a href="exercises.html#cb1217-2" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_persianE1&quot;</span>)</span>
<span id="cb1217-3"><a href="exercises.html#cb1217-3" aria-hidden="true"></a>dat1 &lt;-<span class="st"> </span>df_persianE1</span>
<span id="cb1217-4"><a href="exercises.html#cb1217-4" aria-hidden="true"></a><span class="kw">head</span>(dat1)</span></code></pre></div>
<pre><code>##     subj item   rt distance   predability
## 60     4    6  568    short   predictable
## 94     4   17  517     long unpredictable
## 146    4   22  675    short   predictable
## 185    4    5  575     long unpredictable
## 215    4    3  581     long   predictable
## 285    4    7 1171     long   predictable</code></pre>
<p>The four conditions are:</p>
<ul>
<li>Distance=short and Predictability=unpredictable</li>
<li>Distance=short and Predictability=predictable</li>
<li>Distance=long and Predictability=unpredictable</li>
<li>Distance=long and Predictability=predictable</li>
</ul>
<p>The researcher wants to do the following sets of comparisons between condition means:</p>
<p>Compare the condition labeled Distance=short and Predictability=unpredictable with each of the following conditions:</p>
<ul>
<li>Distance=short and Predictability=predictable</li>
<li>Distance=long and Predictability=unpredictable</li>
<li>Distance=long and Predictability=predictable</li>
</ul>
<p>Questions:</p>
<ul>
<li>Which contrast coding is needed for such a comparison?</li>
<li>First, define the relevant contrast coding. Hint: You can do it by creating a condition column labeled a,b,c,d and then use a built-in contrast coding function.</li>
<li>Then, use the <code>hypr</code> library function to confirm that your contrast coding actually does the comparison you need.</li>
<li>Fit a simple linear model with the above contrast coding and display the slopes, which constitute the relevant comparisons.</li>
<li>Now, compute each of the four conditions’ means and check that the slopes from the linear model correspond to the relevant differences between means that you obtained from the data.</li>
</ul>
</div>
<div id="exr:ContrastsNPIHelmert" class="section level3 unlisted hasAnchor" number="26.6.2">
<h3><span class="header-section-number">G.6.2</span>  Helmert coding for a six-condition design.<a href="exercises.html#exr:ContrastsNPIHelmert" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This data-set is from a psycholinguistics study, and although we explain the theoretical background below, one does not need to deeply understand the research questions to be able to define the contrasts.</p>
<p>Load the following data:</p>
<div class="sourceCode" id="cb1219"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1219-1"><a href="exercises.html#cb1219-1" aria-hidden="true"></a><span class="kw">library</span>(bcogsci)</span>
<span id="cb1219-2"><a href="exercises.html#cb1219-2" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_polarity&quot;</span>)</span>
<span id="cb1219-3"><a href="exercises.html#cb1219-3" aria-hidden="true"></a><span class="kw">head</span>(df_polarity)</span></code></pre></div>
<pre><code>##   subject item condition times value
## 1       1    6         f   SFD   328
## 2       1   24         f   SFD   206
## 3       1   35         e   SFD   315
## 4       1   17         e   SFD   265
## 5       1   34         d   SFD   252
## 6       1    7         a   SFD   156</code></pre>
<p>The data come from an eyetracking study in German reported in <span class="citation">Vasishth et al. (<a href="#ref-VBLD07" role="doc-biblioref">2008</a>)</span>. The experiment is a reading study involving six conditions. The sentences are in English, but the original design was involved German sentences. In German, the word <em>durchaus</em> (certainly) is a positive polarity item: in the constructions used in this experiment, <em>durchaus</em> cannot have a c-commanding element that is a negative polarity item licensor. By contrast, the German negative polarity item <em>jemals</em> (ever) is a negative polarity item: in the constructions used in this experiment, <em>jemals</em> must have a c-commanding element that is a negative polarity item licensor.</p>
<p>Here are the conditions:</p>
<ul>
<li>Negative polarity items
<ol style="list-style-type: lower-alpha">
<li>Grammatical: No man who had a beard was ever thrifty.</li>
<li>Ungrammatical (Intrusive NPI licensor): A man who had no beard was ever thrifty.</li>
<li>Ungrammatical: A man who had a beard was ever thrifty.</li>
</ol></li>
<li>Positive polarity items
<ol start="4" style="list-style-type: lower-alpha">
<li>Ungrammatical: No man who had a beard was certainly thrifty.</li>
<li>Grammatical (Intrusive NPI licensor): A man who had no beard was certainly thrifty.</li>
<li>Grammatical: A man who had a beard was certainly thrifty.</li>
</ol></li>
</ul>
<p>We will focus only on re-reading time in this data set. Subset the data so that we only have re-reading times in the data frame:</p>
<div class="sourceCode" id="cb1221"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1221-1"><a href="exercises.html#cb1221-1" aria-hidden="true"></a>dat2 &lt;-<span class="st"> </span><span class="kw">subset</span>(df_polarity, times <span class="op">==</span><span class="st"> &quot;RRT&quot;</span>)</span>
<span id="cb1221-2"><a href="exercises.html#cb1221-2" aria-hidden="true"></a><span class="kw">head</span>(dat2)</span></code></pre></div>
<pre><code>##      subject item condition times value
## 6365       1   20         b   RRT   240
## 6366       1    3         c   RRT  1866
## 6367       1   13         a   RRT   530
## 6368       1   19         a   RRT   269
## 6369       1   27         c   RRT   845
## 6370       1   26         b   RRT   635</code></pre>
<p>The comparisons we are interested in are as follows:</p>
<ul>
<li>What is the difference in reading time between negative polarity items and positive polarity items? In other words, we want to compare the mean of conditions (a), (b), (c) with the mean of (d), (e), (f).</li>
<li>Within negative polarity items, what is the difference between grammatical and ungrammatical conditions? In other words, we want to compare condition (a) with the average of (b) and (c).</li>
<li>Within negative polarity items, what is the difference between the two ungrammatical conditions? Here, we want to compare conditions (b) and (c).</li>
<li>Within positive polarity items, what is the difference between grammatical and ungrammatical conditions? Here, we want to compare condition (d) with the average of (e) and (f).</li>
<li>Within positive polarity items, what is the
difference between the two grammatical conditions? Here, the comparison is between (e) and (f).</li>
</ul>
<p>Use the <code>hypr</code> package to specify the comparisons specified above, and then extract the contrast matrix. Finally, specify the contrasts to the condition column in the data frame. Fit a linear model using this contrast specification, and then check that the estimates from the model match the mean differences between the conditions being compared.</p>
</div>
<div id="exr:ContrastsNcomparisons" class="section level3 unlisted hasAnchor" number="26.6.3">
<h3><span class="header-section-number">G.6.3</span> Number of possible comparisons in a single model.<a href="exercises.html#exr:ContrastsNcomparisons" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>How many comparisons can one make in a single model when there is a single factor with four levels? Why can we not code four comparisons in a single model?</li>
<li>How many comparisons can one code in a model where there are two factors, one with three levels and one with two levels?</li>
<li>How about a model for a <span class="math inline">\(2 \times 2 \times 3\)</span> design?</li>
</ul>
</div>
</div>
<div id="sec-Contrasts2x2exercises" class="section level2 hasAnchor" number="26.7">
<h2><span class="header-section-number">G.7</span> Contrast coding with two predictor variables<a href="exercises.html#sec-Contrasts2x2exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:ContrastsPersianANOVA" class="section level3 unlisted hasAnchor" number="26.7.1">
<h3><span class="header-section-number">G.7.1</span> ANOVA coding for a four-condition design.<a href="exercises.html#exr:ContrastsPersianANOVA" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Load the following data. These data are from Experiment 1 in a set of reading studies on Persian <span class="citation">(Safavi, Husain, and Vasishth <a href="#ref-SafaviEtAlFrontiers2016" role="doc-biblioref">2016</a>)</span>; we encountered these data in the preceding chapter’s exercises.</p>
<div class="sourceCode" id="cb1223"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1223-1"><a href="exercises.html#cb1223-1" aria-hidden="true"></a><span class="kw">library</span>(bcogsci)</span>
<span id="cb1223-2"><a href="exercises.html#cb1223-2" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_persianE1&quot;</span>)</span>
<span id="cb1223-3"><a href="exercises.html#cb1223-3" aria-hidden="true"></a>dat1 &lt;-<span class="st"> </span>df_persianE1</span>
<span id="cb1223-4"><a href="exercises.html#cb1223-4" aria-hidden="true"></a><span class="kw">head</span>(dat1)</span></code></pre></div>
<pre><code>##     subj item   rt distance   predability
## 60     4    6  568    short   predictable
## 94     4   17  517     long unpredictable
## 146    4   22  675    short   predictable
## 185    4    5  575     long unpredictable
## 215    4    3  581     long   predictable
## 285    4    7 1171     long   predictable</code></pre>
<p>The four conditions are:</p>
<ul>
<li>Distance=short and Predictability=unpredictable</li>
<li>Distance=short and Predictability=predictable</li>
<li>Distance=long and Predictability=unpredictable</li>
<li>Distance=long and Predictability=predictable</li>
</ul>
<p>For the data given above, define an ANOVA-style contrast coding, and compute main effects and interactions. Check with <code>hypr</code> what the estimated comparisons are with an ANOVA coding.</p>
</div>
<div id="exr:Contrasts2x2x2Dillon2013" class="section level3 unlisted hasAnchor" number="26.7.2">
<h3><span class="header-section-number">G.7.2</span> ANOVA and nested comparisons in a <span class="math inline">\(2\times 2\times 2\)</span> design<a href="exercises.html#exr:Contrasts2x2x2Dillon2013" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Load the following data set. This is a <span class="math inline">\(2\times 2\times 2\)</span> design from <span class="citation">Jäger et al. (<a href="#ref-JaegerMertzenVanDykeVasishth2019" role="doc-biblioref">2020</a>)</span>, with the factors Grammaticality (grammatical vs. ungrammatical), Dependency (Agreement vs. Reflexives), and Interference (Interference vs. no interference). The experiment is a replication attempt of Experiment 1 reported in <span class="citation">Dillon et al. (<a href="#ref-Dillon-EtAl-2013" role="doc-biblioref">2013</a>)</span>.</p>
<div class="sourceCode" id="cb1225"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1225-1"><a href="exercises.html#cb1225-1" aria-hidden="true"></a><span class="kw">library</span>(bcogsci)</span>
<span id="cb1225-2"><a href="exercises.html#cb1225-2" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_dillonrep&quot;</span>)</span></code></pre></div>
<ul>
<li>The grammatical conditions are a,b,e,f. The rest of the conditions are ungrammatical.</li>
<li>The agreement conditions are a,b,c,d. The other conditions are reflexives.</li>
<li>The interference conditions are a,d,e,h, and the others are the no-interference conditions.</li>
</ul>
<p>The dependent measure of interest is TFT (total fixation time, in milliseconds).</p>
<p>Using a linear model, do a main effects and interactions ANOVA contrast coding, and obtain an estimate of the main effects of Grammaticality, Dependency, and Interference, and all interactions. You may find it easier to code the contrasts coding the main effects as +1, -1, using <code>ifelse()</code> in R to code vectors corresponding to each main effect. This will make the specification of the interactions easy.</p>
<p>The researchers had a further research hypothesis: in ungrammatical sentences only, agreement would show an interference effect but reflexives would not. In grammatical sentences, both agreement and reflexives are expected to show interference effects. This kind of research question can be answered with nested contrast coding.</p>
<p>To carry out the relevant nested contrasts, define contrasts that estimate the effects of</p>
<ul>
<li>grammaticality</li>
<li>dependency type</li>
<li>the interaction between grammaticality and dependency type</li>
<li>reflexives interference within grammatical conditions</li>
<li>agreement interference within grammatical conditions</li>
<li>reflexives interference within ungrammatical conditions</li>
<li>agreement interference within ungrammatical conditions</li>
</ul>
<p>Do the estimates match expectations? Check this by computing the condition means and checking that the estimates from the models match the relevant differences between conditions or clusters of conditions.</p>
</div>
</div>
<div id="introduction-to-the-probabilistic-programming-language-stan" class="section level2 hasAnchor" number="26.8">
<h2><span class="header-section-number">G.8</span> Introduction to the probabilistic programming language Stan<a href="exercises.html#introduction-to-the-probabilistic-programming-language-stan" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:first" class="section level3 unlisted hasAnchor" number="26.8.1">
<h3><span class="header-section-number">G.8.1</span> A very simple model.<a href="exercises.html#exr:first" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this exercise we revisit the model from <a href="ch-compbda.html#sec-simplenormal">3.2.1</a>. Assume the following:</p>
<ol style="list-style-type: decimal">
<li>There is a true underlying time, <span class="math inline">\(\mu\)</span>, that the subject needs to press the space bar.</li>
<li>There is some noise in this process.</li>
<li>The noise is normally distributed (this assumption is questionable given that response times are generally skewed; we fix this assumption later).</li>
</ol>
<p>That is the likelihood for each observation <span class="math inline">\(n\)</span> will be:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
t_n \sim \mathit{Normal}(\mu, \sigma)
\end{aligned}
\end{equation}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Decide on appropriate priors and fit this model in Stan. Data can be found in <code>df_spacebar</code>.</li>
<li>Change the likelihood to a log-normal distribution and change the priors. Fit the model in Stan.</li>
</ol>
</div>
<div id="exr:badstan" class="section level3 unlisted hasAnchor" number="26.8.2">
<h3><span class="header-section-number">G.8.2</span> Incorrect Stan model.<a href="exercises.html#exr:badstan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to fit both response times and accuracy with the same model. We simulate the data as follows:</p>
<div class="sourceCode" id="cb1226"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1226-1"><a href="exercises.html#cb1226-1" aria-hidden="true"></a>N &lt;-<span class="st"> </span><span class="dv">500</span></span>
<span id="cb1226-2"><a href="exercises.html#cb1226-2" aria-hidden="true"></a>df_sim &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">rt =</span> <span class="kw">rlnorm</span>(N, <span class="dt">mean =</span> <span class="dv">6</span>, <span class="dt">sd =</span> <span class="fl">.5</span>),</span>
<span id="cb1226-3"><a href="exercises.html#cb1226-3" aria-hidden="true"></a>                 <span class="dt">correct =</span> <span class="kw">rbern</span>(N, <span class="dt">prob =</span> <span class="fl">.85</span>))</span></code></pre></div>
<p>We build the following model:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] rt;
  array[N] int correct;
}
parameters {
  real&lt;lower = 0&gt; sigma;
  real theta;
}
model {
  target += normal_lpdf(mu | 0, 20);
  target += lognormal_lpdf(sigma | 3, 1)
  for(n in 1:N)
    target += lognormal_lpdf(rt[n] | mu, sigma);
    target += bernoulli_lpdf(correct[n] | theta);
}</code></pre>
<p>Why does this model not work?</p>
<div class="sourceCode" id="cb1228"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1228-1"><a href="exercises.html#cb1228-1" aria-hidden="true"></a>ls_sim &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">rt =</span> df_sim<span class="op">$</span>rt,</span>
<span id="cb1228-2"><a href="exercises.html#cb1228-2" aria-hidden="true"></a>               <span class="dt">correct =</span> df_sim<span class="op">$</span>correct)</span>
<span id="cb1228-3"><a href="exercises.html#cb1228-3" aria-hidden="true"></a>incorrect &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb1228-4"><a href="exercises.html#cb1228-4" aria-hidden="true"></a>                         <span class="st">&quot;incorrect.stan&quot;</span>,</span>
<span id="cb1228-5"><a href="exercises.html#cb1228-5" aria-hidden="true"></a>                         <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb1228-6"><a href="exercises.html#cb1228-6" aria-hidden="true"></a>fit_sim &lt;-<span class="st"> </span><span class="kw">stan</span>(incorrect, <span class="dt">data =</span> ls_sim)</span></code></pre></div>
<pre><code>## Error in stanc(file = file, model_code = model_code, model_name = model_name, : 0
## Syntax error in &#39;string&#39;, line 13, column 2 to column 5, parsing error:
##    -------------------------------------------------
##     11:    target += normal_lpdf(mu | 0, 20);
##     12:    target += lognormal_lpdf(sigma | 3, 1)
##     13:    for(n in 1:N)
##            ^
##     14:      target += lognormal_lpdf(rt[n] | mu, sigma);
##     15:      target += bernoulli_lpdf(correct[n] | theta);
##    -------------------------------------------------
## 
## Unexpected input after the conclusion of a valid expression.
## You may be missing a &quot;,&quot; between expressions, an operator, or a terminating &quot;}&quot;, &quot;)&quot;, &quot;]&quot;, or &quot;;&quot;.</code></pre>
<p>Try to make it run. (Hint: There are several problems.)</p>
</div>
<div id="exr:skewstan" class="section level3 unlisted hasAnchor" number="26.8.3">
<h3><span class="header-section-number">G.8.3</span> Using Stan documentation.<a href="exercises.html#exr:skewstan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Edit the simple example with Stan from section <a href="ch-introstan.html#sec-firststan">8.2</a>, and replace the normal distribution with a skew normal distribution. (Don’t forget to add a prior to the new parameter, and check the Stan documentation or a statistics textbook for more information about the distribution).</p>
<p>Fit the following data:</p>
<div class="sourceCode" id="cb1230"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1230-1"><a href="exercises.html#cb1230-1" aria-hidden="true"></a>Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dt">mean =</span> <span class="dv">3</span>, <span class="dt">sd =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>Does the estimate of the new parameter make sense?</p>
</div>
<div id="exr:linkfunction" class="section level3 unlisted hasAnchor" number="26.8.4">
<h3><span class="header-section-number">G.8.4</span> The probit link function as an alternative to the logit function.<a href="exercises.html#exr:linkfunction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The probit link function is the inverse of the CDF of the standard normal distribution (<span class="math inline">\(Normal(0,1)\)</span>). Since the CDF of the standard normal is usually written using the Greek letter <span class="math inline">\(\Phi\)</span> (Phi), the probit function is written as its inverse, <span class="math inline">\(\Phi^{-1}\)</span>. Refit the model presented in <a href="ch-introstan.html#sec-logisticstan">8.4.3</a> changing the logit link function for the probit link (that is transforming the regression to a constrained space using <code>Phi()</code> in Stan).</p>
<p>You will probably see the following as the model runs; this is because the probit link is less numerically stable (i.e., under- and overflows) than the logit link in Stan. Don’t worry, it is good enough for this exercise.</p>
<pre><code>   Rejecting initial value:
   Log probability evaluates to log(0), i.e. negative infinity.
   Stan can&#39;t start sampling from this initial value.</code></pre>
<ol style="list-style-type: lower-alpha">
<li>Do the results of the coefficients <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> change?</li>
<li>Do the results in probability space change?</li>
</ol>
</div>
<div id="exr:logisticstan" class="section level3 unlisted hasAnchor" number="26.8.5">
<h3><span class="header-section-number">G.8.5</span> Examining the position of the queued word on recall.<a href="exercises.html#exr:logisticstan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Refit the model presented in section <a href="ch-introstan.html#sec-logisticstan">8.4.3</a> and examine whether set size, trial effects, the position of the queued word (<code>tested</code> in the data set), and their interaction affect free recall. (Tip: You can do this exercise without changing the Stan code.).</p>
<p>How does the accuracy change from position one to position two?</p>
</div>
<div id="exr:fallacy" class="section level3 unlisted hasAnchor" number="26.8.6">
<h3><span class="header-section-number">G.8.6</span> The conjunction fallacy.<a href="exercises.html#exr:fallacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="citation">Paolacci, Chandler, and Ipeirotis (<a href="#ref-Paolaccietal" role="doc-biblioref">2010</a>)</span> examined whether the results of some classic experiments differ between a university pool population and subjects recruited from Mechanical Turk. We’ll examine whether the results of the conjunction fallacy experiment <span class="citation">(or Linda problem: Tversky and Kahneman <a href="#ref-TverskyKahneman1983" role="doc-biblioref">1983</a>)</span> are replicated for both groups.</p>
<div class="sourceCode" id="cb1232"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1232-1"><a href="exercises.html#cb1232-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_fallacy&quot;</span>)</span>
<span id="cb1232-2"><a href="exercises.html#cb1232-2" aria-hidden="true"></a>df_fallacy</span></code></pre></div>
<pre><code>## # A tibble: 268 × 2
##   source answer
##   &lt;chr&gt;   &lt;int&gt;
## 1 mturk       1
## 2 mturk       1
## 3 mturk       1
## # ℹ 265 more rows</code></pre>
<p>The conjunction fallacy shows that people often fail to regard a combination of events as less probable than a single event in the combination <span class="citation">(Tversky and Kahneman <a href="#ref-TverskyKahneman1983" role="doc-biblioref">1983</a>)</span>:</p>
<p><em>Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.</em></p>
<p><em>Which is more probable?</em></p>
<ol style="list-style-type: lower-alpha">
<li><em>Linda is a bank teller.</em></li>
<li><em>Linda is a bank teller and is active in the feminist movement.</em></li>
</ol>
<p>The majority of those asked chose option b even though it’s less probable (<span class="math inline">\(\Pr(a \land b)\leq \Pr(b)\)</span>. The data set is named <code>df_fallacy</code> and it indicates with <code>0</code> option “a” and with <code>1</code> option <code>b</code>.
Fit a logistic regression in Stan and report:</p>
<ol style="list-style-type: lower-alpha">
<li>The estimated overall probability of answering (b) ignoring the group.</li>
<li>The estimated overall probability of answering (b) for each group.</li>
</ol>
</div>
</div>
<div id="hierarchical-models-and-reparameterization" class="section level2 hasAnchor" number="26.9">
<h2><span class="header-section-number">G.9</span> Hierarchical models and reparameterization<a href="exercises.html#hierarchical-models-and-reparameterization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:stroop" class="section level3 unlisted hasAnchor" number="26.9.1">
<h3><span class="header-section-number">G.9.1</span> A log-normal model in Stan.<a href="exercises.html#exr:stroop" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Refit the Stroop example from section <a href="ch-hierarchical.html#sec-stroop">5.3</a> in Stan (<code>df_stroop</code>).</p>
<p>Assume the following likelihood and priors:</p>
<p><span class="math display">\[\begin{equation}
  rt_n \sim \mathit{LogNormal}(\alpha + u_{subj[n],1}  + c\_cond_n \cdot  (\beta + u_{subj[n],2}), \sigma)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
   \alpha &amp; \sim \mathit{Normal}(6, 1.5) \\
   \beta  &amp; \sim \mathit{Normal}(0, .1) \\
    \sigma  &amp;\sim \mathit{Normal}_+(0, 1)
 \end{aligned}
 \end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim \mathit{Normal}_+(0,1)\\
\tau_{u_2} &amp;\sim \mathit{Normal}_+(0,1)\\
\begin{bmatrix}
1 &amp; \rho_u \\
\rho_u &amp; 1
\end{bmatrix} &amp;\sim \mathit{LKJcorr}(2) 
\end{aligned}
\end{equation}\]</span></p>
</div>
<div id="exr:hierarchical-logn-stan" class="section level3 unlisted hasAnchor" number="26.9.2">
<h3><span class="header-section-number">G.9.2</span> A by-subjects and by-items hierarchical model with a log-normal likelihood.<a href="exercises.html#exr:hierarchical-logn-stan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Revisit the question “Are subject relatives easier to process than object relatives?” Fit the model from the exercise <a href="exercises.html#exr:hierarchical-logn">G.5.2</a> using Stan.</p>
</div>
<div id="exr:strooplogis" class="section level3 unlisted hasAnchor" number="26.9.3">
<h3><span class="header-section-number">G.9.3</span> A hierarchical logistic regression with Stan.<a href="exercises.html#exr:strooplogis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Revisit the question “Is there a Stroop effect in accuracy?” Fit the model the exercise <a href="exercises.html#exr:strooplogis-brms">G.5.6</a> using Stan.</p>
</div>
<div id="exr:distr-stan" class="section level3 unlisted hasAnchor" number="26.9.4">
<h3><span class="header-section-number">G.9.4</span> A distributional regression model of the effect of cloze probability on the N400.<a href="exercises.html#exr:distr-stan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In section <a href="ch-hierarchical.html#sec-distrmodel">5.2.6</a>, we saw how to fit a distributional regression model. We might want to extend this approach to Stan. Fit the EEG data to a hierarchical model with by-subject and by-items varying intercept and slopes, and in addition assume that the residual standard deviation (the scale of the normal likelihood) can vary by subject.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  signal_n &amp;\sim \mathit{Normal}(\alpha + u_{subj[n],1} + w_{item[n],1} + \\
  &amp; c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2}), \sigma_n)\\
  \sigma_n &amp;= \exp(\alpha_{\sigma} + u_{\sigma_{subj[n]}})
\end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  \alpha_\alpha &amp;\sim \mathit{Normal}(0,log(50))\\
  u_\sigma &amp;\sim \mathit{Normal}(0, \tau_{u_\sigma}) \\
  \tau_{u_\sigma} &amp;\sim \mathit{Normal}_+(0, 5)
\end{aligned}
\end{equation}\]</span></p>
<p>To fit this model, take into account that <code>sigma</code> is now a vector, and it is a transformed parameter which depends on two parameters: <code>alpha_sigma</code> and the vector with <code>N_subj</code> elements <code>u_sigma</code>. In addition, <code>u_sigma</code> depends on the hyperparameter <code>tau_u_sigma</code> (<span class="math inline">\(\tau_{u_\sigma}\)</span>). (Using the non-centered parameterization for <code>u_sigma</code> speeds up the model fit considerably).</p>
</div>
</div>
<div id="sec-customexercises" class="section level2 hasAnchor" number="26.10">
<h2><span class="header-section-number">G.10</span> Custom distributions in Stan<a href="exercises.html#sec-customexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:shiftedlogn" class="section level3 unlisted hasAnchor" number="26.10.1">
<h3><span class="header-section-number">G.10.1</span> Fitting a  shifted log-normal distribution.<a href="exercises.html#exr:shiftedlogn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable <span class="math inline">\(Y\)</span> has a shifted log-normal distribution with shift <span class="math inline">\(\psi\)</span>, location <span class="math inline">\(\mu\)</span>, and scale <span class="math inline">\(\sigma\)</span>, if <span class="math inline">\(Z = Y-\psi\)</span> and <span class="math inline">\(Z \sim \mathit{LogNormal}(\mu,\sigma)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Implement a <code>shifted_lognormal_ldpf</code> function in Stan with three parameters, <code>mu</code>, <code>sigma</code>, and <code>psi</code>. Tip: One can use the regular log-normal distribution and apply a change of variable. In this case the adjustment of the Jacobian would be
<span class="math inline">\(|\frac{d}{dY} Y - \psi|=1\)</span>, which in log-space is conveniently zero.</p></li>
<li><p>Verify the correctness of the model by recovering the true values of (your choice) of the parameters of the model and by using simulation-based calibration. In order to use simulation-based calibration, you will need to decide on sensible priors; assume that <span class="math inline">\(\psi \sim \mathit{Normal}_+(100,50)\)</span>, and choose priors for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> so that the prior predictive distributions are adequate for response times.</p></li>
</ol>
</div>
<div id="exr:wald" class="section level3 unlisted hasAnchor" number="26.10.2">
<h3><span class="header-section-number">G.10.2</span> Fitting a Wald distribution.<a href="exercises.html#exr:wald" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The  Wald distribution (or inverse Gaussian distribution) and its variants have been proposed as another useful distribution for  response times <span class="citation">(see for example Heathcote <a href="#ref-heathcote2004fitting" role="doc-biblioref">2004</a>)</span>.</p>
<p>The probability density function of the Wald distribution is the following.</p>
<p><span class="math display">\[\begin{equation}
f(x;\mu ,\lambda )={\sqrt {\frac {\lambda }{2\pi x^{3}}}}\exp {\biggl (}-{\frac {\lambda (x-\mu )^{2}}{2\mu ^{2}x}}{\biggr )}
\end{equation}\]</span></p>
<ol style="list-style-type: decimal">
<li>Implement this distribution in Stan as <code>wald_lpdf</code>. In order to do this, you will need to derive the logarithm of the PDF presented above. You can adapt the code of the following R function.</li>
</ol>
<div class="sourceCode" id="cb1234"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1234-1"><a href="exercises.html#cb1234-1" aria-hidden="true"></a>dwald &lt;-<span class="st"> </span><span class="cf">function</span>(x, lambda, mu, <span class="dt">log =</span> <span class="ot">FALSE</span>) {</span>
<span id="cb1234-2"><a href="exercises.html#cb1234-2" aria-hidden="true"></a>  log_density &lt;-<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(lambda <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pi</span>())) <span class="op">-</span></span>
<span id="cb1234-3"><a href="exercises.html#cb1234-3" aria-hidden="true"></a><span class="st">    </span><span class="fl">1.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(x) <span class="op">-</span></span>
<span id="cb1234-4"><a href="exercises.html#cb1234-4" aria-hidden="true"></a><span class="st">    </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span>((x <span class="op">-</span><span class="st"> </span>mu) <span class="op">/</span><span class="st"> </span>(mu <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(x)))<span class="op">^</span><span class="dv">2</span></span>
<span id="cb1234-5"><a href="exercises.html#cb1234-5" aria-hidden="true"></a>  <span class="cf">if</span> (log <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) {</span>
<span id="cb1234-6"><a href="exercises.html#cb1234-6" aria-hidden="true"></a>    <span class="kw">exp</span>(log_density)</span>
<span id="cb1234-7"><a href="exercises.html#cb1234-7" aria-hidden="true"></a>  } <span class="cf">else</span> {</span>
<span id="cb1234-8"><a href="exercises.html#cb1234-8" aria-hidden="true"></a>    log_density</span>
<span id="cb1234-9"><a href="exercises.html#cb1234-9" aria-hidden="true"></a>  }</span>
<span id="cb1234-10"><a href="exercises.html#cb1234-10" aria-hidden="true"></a>}</span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Verify the correctness of the model by recovering the true values of (your choice) of the parameters of the model and by using simulation-based calibration. As with the previous exercise, you will need to decide on sensible priors by deriving prior predictive distributions that are adequate for response times.</li>
</ol>
</div>
</div>
<div id="sec-REMAMEexercises" class="section level2 hasAnchor" number="26.11">
<h2><span class="header-section-number">G.11</span> Meta-analysis and measurement error models<a href="exercises.html#sec-REMAMEexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:REMAMEExtracting" class="section level3 unlisted hasAnchor" number="26.11.1">
<h3><span class="header-section-number">G.11.1</span> Extracting estimates from published papers<a href="exercises.html#exr:REMAMEExtracting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Researchers often do not release the data that lie behind the published paper. This creates the problem that one has to figure out the estimated effect size and standard error from published statistics. This exercise gives some practice on how to do this by considering some typical cases that are encountered in repeated measures designs.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose that in a repeated measures reading study, the observed t-value from a paired t-test is 2.3 with degrees of freedom 23. Suppose also that the estimated standard deviation is 150 ms. What is the estimated standard error and the estimated effect size?</p></li>
<li><p>A repeated measures reading study reports an F-score from an analysis of variance as <span class="math inline">\(F(1,34)=6.1\)</span>, with an effect size of 25 ms. What is the estimated standard error?</p></li>
</ol>
</div>
<div id="exr:REMAMEBuerki" class="section level3 unlisted hasAnchor" number="26.11.2">
<h3><span class="header-section-number">G.11.2</span> A meta-analysis of picture-word interference data<a href="exercises.html#exr:REMAMEBuerki" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Load the following data set:</p>
<div class="sourceCode" id="cb1235"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1235-1"><a href="exercises.html#cb1235-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_buerki&quot;</span>)</span>
<span id="cb1235-2"><a href="exercises.html#cb1235-2" aria-hidden="true"></a><span class="kw">head</span>(df_buerki)</span></code></pre></div>
<pre><code>##                  study   d    se study_id
## 1 Collina 2013 Exp.1 a  24 13.09        1
## 2 Collina 2013 Exp.1 b -25 17.00        2
## 3   Collina 2013 Exp.2  46 22.79        3
## 4     Mahon 2007 Exp.1  17 12.24        4
## 5     Mahon 2007 Exp.2  57 13.96        5
## 6    Mahon 2007 Exp. 4  17  8.01        6</code></pre>
<div class="sourceCode" id="cb1237"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1237-1"><a href="exercises.html#cb1237-1" aria-hidden="true"></a>df_buerki &lt;-<span class="st"> </span><span class="kw">subset</span>(df_buerki, se <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.60</span>)</span></code></pre></div>
<p>The data are from <span class="citation">Bürki et al. (<a href="#ref-BuerkiEtAl2020" role="doc-biblioref">2020</a>)</span>. We have a summary of the effect estimates (d) and standard errors (se) of the estimates from 162 published experiments on a phenomenon called <em>semantic picture-word interference</em>. We removed an implausibly low SE in the code above, but the results don’t change regardless of whether we keep them or not, because we have data from a lot of studies.</p>
<p>In this experimental paradigm, subjects are asked to name a picture while ignoring a distractor word (which is either related or unrelated to the picture). The word can be printed on the picture itself, or presented auditorily. The dependent measure is the response latency, or time interval between the presentation of the picture and the onset of the vocal response. Theory says that distractors that come from the same semantic category as the picture to be named lead to a slower response then when the distractor comes from a different semantic category.</p>
<p>Carry out a random effects meta-analysis using <code>brms</code> and display the posterior distribution of the effect, along with the posterior of the between study standard deviation.</p>
<p>Choose <span class="math inline">\(\mathit{Normal}(0,100)\)</span> priors for the intercept and between study sd parameters. You can also try vague priors (sensitivity analysis). Examples would be:</p>
<ul>
<li><span class="math inline">\(\mathit{Normal}(0,200)\)</span></li>
<li><span class="math inline">\(\mathit{Normal}(0,400)\)</span></li>
</ul>
</div>
<div id="exr:REMAMELiEnglish" class="section level3 unlisted hasAnchor" number="26.11.3">
<h3><span class="header-section-number">G.11.3</span> Measurement error model for English VOT data<a href="exercises.html#exr:REMAMELiEnglish" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Load the following data:</p>
<div class="sourceCode" id="cb1238"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1238-1"><a href="exercises.html#cb1238-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_VOTenglish&quot;</span>)</span>
<span id="cb1238-2"><a href="exercises.html#cb1238-2" aria-hidden="true"></a><span class="kw">head</span>(df_VOTenglish)</span></code></pre></div>
<pre><code>##   subject meanVOT seVOT meanvdur sevdur
## 1     F01   108.1  4.56      171   11.7
## 2     F02    92.5  4.62      189   12.7
## 3     F03    82.6  3.13      171   10.0
## 4     F04    88.3  3.21      168   11.8
## 5     F05    94.6  3.67      166   15.0
## 6     F06    75.9  3.70      176   12.9</code></pre>
<p>You are given mean voice onset time (VOT) data (with SEs) in milliseconds for English, along with mean vowel durations (with SEs) in milliseconds. Fit a measurement-error model investigating the effect of mean vowel duration on mean VOT duration. First plot the relationship between the two variables; does it look like there is an association between the two?</p>
<p>Then use <code>brms</code> with measurement error included in both the dependent and independent variables. Do a sensitivity analysis to check the influence of the priors on the posteriors of the relevant parameters.</p>
</div>
</div>
<div id="introduction-to-model-comparison" class="section level2 hasAnchor" number="26.12">
<h2><span class="header-section-number">G.12</span> Introduction to model comparison<a href="exercises.html#introduction-to-model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="bayes-factors" class="section level2 hasAnchor" number="26.13">
<h2><span class="header-section-number">G.13</span> Bayes factors<a href="exercises.html#bayes-factors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:bysubjects" class="section level3 unlisted hasAnchor" number="26.13.1">
<h3><span class="header-section-number">G.13.1</span> Is there evidence for differences in the effect of cloze probability among the subjects?<a href="exercises.html#exr:bysubjects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use Bayes factor to compare the log cloze probability model that we examined in section <a href="ch-bf.html#sec-BFnonnested">13.2.2</a> with a similar model but that incorporates the strong assumption of no difference between subjects for the effect of cloze (<span class="math inline">\(\tau_{u_2}=0\)</span>).</p>
</div>
<div id="exr:bf-logn" class="section level3 unlisted hasAnchor" number="26.13.2">
<h3><span class="header-section-number">G.13.2</span> Is there evidence for the claim that English subject relative clauses are easier to process than object relative clauses?<a href="exercises.html#exr:bf-logn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider again the reading time data coming from Experiment 1 of <span class="citation">Grodner and Gibson (<a href="#ref-grodner" role="doc-biblioref">2005</a>)</span> presented in exercise <a href="exercises.html#exr:hierarchical-logn">G.5.2</a>:</p>
<div class="sourceCode" id="cb1240"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1240-1"><a href="exercises.html#cb1240-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_gg05_rc&quot;</span>)</span>
<span id="cb1240-2"><a href="exercises.html#cb1240-2" aria-hidden="true"></a>df_gg05_rc</span></code></pre></div>
<pre><code>## # A tibble: 672 × 7
##    subj  item condition    RT residRT qcorrect experiment
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;     
## 1     1     1 objgap      320   -21.4        0 tedrg3    
## 2     1     2 subjgap     424    74.7        1 tedrg2    
## 3     1     3 objgap      309   -40.3        0 tedrg3    
## # ℹ 669 more rows</code></pre>
<p>As in exercise <a href="exercises.html#exr:hierarchical-logn">G.5.2</a>, you should use a sum coding for the predictors. Here, object relative clauses (<code>"objgaps"</code>) are coded <span class="math inline">\(+1/2\)</span>, and subject relative clauses as <span class="math inline">\(-1/2\)</span>.</p>
<div class="sourceCode" id="cb1242"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1242-1"><a href="exercises.html#cb1242-1" aria-hidden="true"></a>df_gg05_rc &lt;-<span class="st"> </span>df_gg05_rc <span class="op">%&gt;%</span></span>
<span id="cb1242-2"><a href="exercises.html#cb1242-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cond =</span> <span class="kw">if_else</span>(condition <span class="op">==</span><span class="st"> &quot;objgap&quot;</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">-1</span><span class="op">/</span><span class="dv">2</span>))</span></code></pre></div>
<p>Using the Bayes factors function shown in this chapter, quantify the evidence against the null model (no population-level reading time difference between SRC and ORC) relative to the following alternative models:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, 1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, 0.1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, 0.01)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}_+(0, 1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}_+(0, 0.1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}_+(0, 0.01)\)</span></li>
</ol>
<p>(A <span class="math inline">\(\mathit{Normal}_+(.)\)</span> prior can be set in <code>brms</code> by defining a lower boundary as <span class="math inline">\(0\)</span>, with the argument <code>lb = 0</code>.)</p>
<p>What are the Bayes factors in favor of the alternative models a-f, compared to the null model?</p>
<p>Now carry out a standard frequentist likelihood ratio test using the <code>anova()</code> function that is used with the <code>lmer()</code> function. The commands
for doing this comparison would be:</p>
<div class="sourceCode" id="cb1243"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1243-1"><a href="exercises.html#cb1243-1" aria-hidden="true"></a>m_full &lt;-<span class="st"> </span><span class="kw">lmer</span>(<span class="kw">log</span>(RT) <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span></span>
<span id="cb1243-2"><a href="exercises.html#cb1243-2" aria-hidden="true"></a><span class="st">                 </span>(c_cond <span class="op">||</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">||</span><span class="st"> </span>item),</span>
<span id="cb1243-3"><a href="exercises.html#cb1243-3" aria-hidden="true"></a>               df_gg05_rc)</span>
<span id="cb1243-4"><a href="exercises.html#cb1243-4" aria-hidden="true"></a>m_null &lt;-<span class="st"> </span><span class="kw">lmer</span>(<span class="kw">log</span>(RT) <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(c_cond<span class="op">||</span>subj) <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">||</span><span class="st"> </span>item),</span>
<span id="cb1243-5"><a href="exercises.html#cb1243-5" aria-hidden="true"></a>               df_gg05_rc)</span>
<span id="cb1243-6"><a href="exercises.html#cb1243-6" aria-hidden="true"></a><span class="kw">anova</span>(m_null, m_full)</span></code></pre></div>
<p>How do the conclusions from the Bayes factor analyses compare with the conclusion we obtain from the frequentist model comparison?</p>
</div>
<div id="exr:bf-logistic" class="section level3 unlisted hasAnchor" number="26.13.3">
<h3><span class="header-section-number">G.13.3</span> In the Grodner and Gibson 2005 data, in question-response accuracies, is there evidence for the claim that sentences with subject relative clauses are easier to comprehend?<a href="exercises.html#exr:bf-logistic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the question response accuracy of the data of Experiment 1 of <span class="citation">Grodner and Gibson (<a href="#ref-grodner" role="doc-biblioref">2005</a>)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Compare a model that assumes that RC type affects question accuracy on the population-level and with the effect varying by-subjects and by-items with <em>a null model</em> that assumes that there is no population-level effect present.</li>
<li>Compare a model that assumes that RC type affects question accuracy on the population level and with the effect varying by-subjects and by-items with <em>another null model</em> that assumes that there is no population-level or group-level effect present, that is no by-subject or by-item effects. What’s the meaning of the results of the Bayes factor analysis?</li>
</ol>
<p>Assume that for the effect of RC on question accuracy, <span class="math inline">\(\beta \sim \mathit{Normal}(0, 0.1)\)</span> is a reasonable prior, and that for all the variance components, the same prior, <span class="math inline">\(\tau \sim \mathit{Normal}_{+}(0, 1)\)</span>, is a reasonable prior.</p>
</div>
<div id="exr:lognstan" class="section level3 unlisted hasAnchor" number="26.13.4">
<h3><span class="header-section-number">G.13.4</span> Bayes factor and bounded parameters using Stan.<a href="exercises.html#exr:lognstan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Re-fit the data of a single subject pressing a button repeatedly from <a href="ch-reg.html#sec-trial">4.2</a> from <code>data("df_spacebar")</code>, coding the model in Stan.</p>
<p>Start by assuming the following likelihood and priors:</p>
<p><span class="math display">\[\begin{equation}
rt_n \sim \mathit{LogNormal}(\alpha + c\_trial_n \cdot \beta,\sigma)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim \mathit{Normal}(6, 1.5) \\
\beta &amp;\sim \mathit{Normal}_+(0, 0.1)\\
\sigma &amp;\sim \mathit{Normal}_+(0, 1)
\end{aligned}
\end{equation}\]</span></p>
<p>Use the Bayes factor to answer the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li>Is there evidence for any effect of trial number in comparison with no effect?</li>
<li>Is there evidence for a positive effect of trial number (as the subject reads further, they slowdown) in comparison with no effect?</li>
<li>Is there evidence for a negative effect of trial number (as the subject reads further, they speedup) in comparison with no effect?</li>
<li>Is there evidence for a positive effect of trial number in comparison with a negative effect?</li>
</ol>
<p>(Expect very large Bayes factors in this exercise.)</p>
</div>
</div>
<div id="cross-validation" class="section level2 hasAnchor" number="26.14">
<h2><span class="header-section-number">G.14</span> Cross-validation<a href="exercises.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:logcv" class="section level3 unlisted hasAnchor" number="26.14.1">
<h3><span class="header-section-number">G.14.1</span> Predictive accuracy of the linear and the logarithm effect of cloze probability.<a href="exercises.html#exr:logcv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Is there a difference in predictive accuracy between the model that incorporates a linear effect of cloze probability and one that incorporates log-transformed cloze probabilities?</p>
</div>
<div id="exr:stroopcv" class="section level3 unlisted hasAnchor" number="26.14.2">
<h3><span class="header-section-number">G.14.2</span> Log-normal model<a href="exercises.html#exr:stroopcv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use PSIS-LOO to compare a model of Stroop as the one in <a href="exercises.html#exr:stroop">G.9.1</a> with a model that assumes no population-level effect</p>
<ol style="list-style-type: lower-alpha">
<li>in <code>brms</code>.</li>
<li>in Stan.</li>
</ol>
</div>
<div id="exr:logrec" class="section level3 unlisted hasAnchor" number="26.14.3">
<h3><span class="header-section-number">G.14.3</span> Log-normal vs rec-normal model in Stan<a href="exercises.html#exr:logrec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In section <a href="ch-custom.html#sec-change">10.1</a>, we proposed a reciprocal truncated normal distribution (rec-normal) to response times data, as an alternative to the log-normal distribution. The log-likelihood (of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>) of an individual observation, <span class="math inline">\(\mathit{RT}_{n}\)</span>, for the rec-normal distribution would be the following one.</p>
<p><span class="math display">\[\begin{equation}
\log \mathcal{L} = \log(\mathit{Normal}(1/\mathit{RT}_n | \mu, \sigma)) - 2 \cdot \log(\mathit{RT}_n)
\end{equation}\]</span></p>
<p>As explained in <a href="ch-custom.html#sec-change">10.1</a>, we obtain the log-likelihood based on all the <span class="math inline">\(N\)</span> observations by summing the log-likelihood of individual observations.</p>
<p><span class="math display">\[\begin{equation}
\log \mathcal{L} = \sum_n^N \log(\mathit{Normal}(1/\mathit{RT}_n | \mu, \sigma))  - \sum_n^N 2 \cdot \log(\mathit{RT}_n)
\end{equation}\]</span></p>
<p>Since these two models assume right-skewed data with only positive values, the question that we are interested in here is if we can really distinguish between them. Investigate this in the following way:</p>
<ol style="list-style-type: lower-alpha">
<li>Generate data (N = 100 and N = 1000) with a rec-normal distribution (e.g., <code>rt = 1 / rtnorm(N, mu, sigma, a = 0)</code>).</li>
<li>Generate data (N = 100 and N = 1000) with a log-normal distribution</li>
</ol>
<p>Fit a rec-normal and a log-normal model using Stan to each of the four data sets, and use PSIS-LOO to compare the models.</p>
<p>What do you conclude?</p>
</div>
</div>
<div id="introduction-to-cognitive-modeling" class="section level2 hasAnchor" number="26.15">
<h2><span class="header-section-number">G.15</span> Introduction to cognitive modeling<a href="exercises.html#introduction-to-cognitive-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="multinomial-processing-trees" class="section level2 hasAnchor" number="26.16">
<h2><span class="header-section-number">G.16</span> Multinomial processing trees<a href="exercises.html#multinomial-processing-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:mult" class="section level3 unlisted hasAnchor" number="26.16.1">
<h3><span class="header-section-number">G.16.1</span> Modeling multiple categorical responses.<a href="exercises.html#exr:mult" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: lower-alpha">
<li>Re-fit the model presented in section <a href="ch-MPT.html#sec-cat">16.1.2</a>, adding the assumption that you have more information about the probability of giving a correct response in the task. Assume that you know that subjects’ answers have around 60% accuracy. Encode this information in the priors with two different degrees of certainty. (Hint: 1. As with the Beta distribution, you can increase the pseudo-counts to increase the amount of information and reduce the “width” of the distribution; compare <span class="math inline">\(Beta(9,1)\)</span> with <span class="math inline">\(Beta(900,100)\)</span>. 2. You’ll need to use a column vector for the Dirichlet concentration parameters. <code>[.., .., ]</code> is a <code>row_vector</code> that can be transposed and converted into a column vector by adding the transposition symbol <code>'</code> after the right bracket.)</li>
<li>What is the difference between the multinomial and categorical parameterizations?</li>
<li>What can we learn about impaired picture naming from the models in sections <a href="ch-MPT.html#sec-mult">16.1.1</a> and <a href="ch-MPT.html#sec-cat">16.1.2</a>?</li>
</ol>
</div>
<div id="exr:mpt-mnm" class="section level3 unlisted hasAnchor" number="26.16.2">
<h3><span class="header-section-number">G.16.2</span> An alternative MPT to model the picture recognition task.<a href="exercises.html#exr:mpt-mnm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Build <em>any</em> alternative tree with four parameters <span class="math inline">\(w\)</span>, <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, <span class="math inline">\(z\)</span> to fit the data generated in <a href="ch-MPT.html#sec-mpt-data">16.2.2</a>. Compare the posterior distribution of the auxiliary vector <code>theta</code> (that goes in the <code>multinomial_lpmf()</code>) with the one derived in section <a href="ch-MPT.html#sec-mpt-data">16.2.2</a>.</p>
</div>
<div id="exr:edit-mpt-cat" class="section level3 unlisted hasAnchor" number="26.16.3">
<h3><span class="header-section-number">G.16.3</span> A simple MPT model that incorporates phonological complexity in the picture recognition task.<a href="exercises.html#exr:edit-mpt-cat" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Edit the Stan code <code>mpt_cat.stan</code> from <code>bcogsci</code> presented in section <a href="ch-MPT.html#sec-MPT-reg">16.2.3</a> to incorporate the fact that <code>f</code> is now a transformed parameter that depends on the trial information and two new parameters, <span class="math inline">\(\alpha_f\)</span> and <span class="math inline">\(\beta_f\)</span>. The rest of the latent parameters do not need to vary by trial.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
f&#39;_j &amp;=\alpha_f + complexity_j\cdot \beta_f\\
f_j &amp;= logit^{-1}(f&#39;_j)
\end{aligned}
\end{equation}\]</span></p>
<p>The inverse logit or logistic function is called <code>inv_logit()</code> in Stan. Fit the model to the data of <a href="ch-MPT.html#sec-MPT-reg">16.2.3</a> and report the posterior distributions of the latent parameters.</p>
</div>
<div id="exr:mpt" class="section level3 unlisted hasAnchor" number="26.16.4">
<h3><span class="header-section-number">G.16.4</span> A more hierarchical MPT.<a href="exercises.html#exr:mpt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Modify the hierarchical MPT presented in section <a href="ch-MPT.html#sec-MPT-h">16.2.4</a> so that all the parameters are affected by individual differences. Simulate data and fit it. How well can you recover the parameters? You should use the non-centered parameterization for the by-subject adjustments. (Hint: Convergence will be reached much faster if you don’t assume that the adjustment parameters are correlated as in <a href="ch-complexstan.html#sec-uncorrstan">9.1.2</a>, but you could also assume a correlation between all (or some of) the adjustments by using the Cholesky factorization discussed in section <a href="ch-complexstan.html#sec-corrstan">9.1.3</a>.)</p>
</div>
<div id="exr:mpt-adv" class="section level3 unlisted hasAnchor" number="26.16.5">
<h3><span class="header-section-number">G.16.5</span> <strong>Advanced</strong>: Multinomial processing trees.<a href="exercises.html#exr:mpt-adv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The data set <code>df_source_monitoring</code> in <code>bcogsci</code> contains data from the package <code>psychotools</code> coming from a source-monitoring experiment <span class="citation">(Batchelder and Riefer <a href="#ref-batchelder1990multinomial" role="doc-biblioref">1990</a>)</span> performed by <span class="citation">Wickelmaier and Zeileis (<a href="#ref-wickelmaier2018using" role="doc-biblioref">2018</a>)</span>.</p>
<p>In this type of experiment, subjects study items from (at least) two different sources, A and B. After the presentation of the study items, subjects are required to classify each item as coming from source A, B, or as new: N (that is, a distractor). In their version of the experiment, Wickelmaier and Zeileis used two different A-B pairs: Half of the subjects had to read items either quietly (source A = think) or aloud (source B = say). The other half had to write items down (source A = write) or read them aloud (source B = say).</p>
<ul>
<li><code>experiment</code>: write-say or think-say</li>
<li><code>age</code>: Age of the respondent in years.</li>
<li><code>gender</code>: Gender of the respondent.</li>
<li><code>subj</code>: Subject id.</li>
<li><code>source</code>: Item source, a, b or n (new)</li>
<li><code>a</code>, <code>b</code>, <code>N</code>: Number of responses for each type of stimuli</li>
</ul>
<p>Fit a multinomial processing tree following Figures <a href="exercises.html#fig:smtikz">G.1</a> and <a href="exercises.html#fig:smtikz2">G.2</a> to investigate whether experiment type, age and/or gender affects the different processes assumed in the model.
As in <span class="citation">Batchelder and Riefer (<a href="#ref-batchelder1990multinomial" role="doc-biblioref">1990</a>)</span>, assume that <span class="math inline">\(a = g\)</span> (for identifiability) and that discriminability is equal for both sources (<span class="math inline">\(d_1 = d_2\)</span>).</p>

<div class="figure"><span style="display:block;" id="fig:smtikz"></span>
<img src="bayescogsci_files/figure-html/smtikz-1.svg" alt="Multinomial processing tree for the source A items from the source monitoring paradigm (Batchelder and Riefer, 1990). \(D_1\) stands for the detectability of source A, \(d_1\) stands for the source discriminabilities for source A items, \(b\) stands for the bias for responding “old” to a nondetected item, \(a\) stands for guessing that a detected but nondiscriminated item belongs to source A, and \(g\) stands for guessing that the item is a source A item." width="672" />
<p class="caption">
FIGURE G.1: Multinomial processing tree for the source A items from the source monitoring paradigm (Batchelder and Riefer, 1990). <span class="math inline">\(D_1\)</span> stands for the detectability of source A, <span class="math inline">\(d_1\)</span> stands for the source discriminabilities for source A items, <span class="math inline">\(b\)</span> stands for the bias for responding “old” to a nondetected item, <span class="math inline">\(a\)</span> stands for guessing that a detected but nondiscriminated item belongs to source A, and <span class="math inline">\(g\)</span> stands for guessing that the item is a source A item.
</p>
</div>

<div class="figure"><span style="display:block;" id="fig:smtikz2"></span>
<img src="bayescogsci_files/figure-html/smtikz2-1.svg" alt="Multinomial processing tree for the source B items from source monitoring paradigm (Batchelder and Riefer, 1990). \(D_2\) stand for the detectability of source B items, \(d_2\) stands for the source discriminabilities for source B, \(b\) stands for the bias for responding “old” to a nondetected item, \(a\) stands for guessing that a detected but nondiscriminated item belongs to Source A, and \(g\) stands for guessing that the item is a source A item." width="672" />
<p class="caption">
FIGURE G.2: Multinomial processing tree for the source B items from source monitoring paradigm (Batchelder and Riefer, 1990). <span class="math inline">\(D_2\)</span> stand for the detectability of source B items, <span class="math inline">\(d_2\)</span> stands for the source discriminabilities for source B, <span class="math inline">\(b\)</span> stands for the bias for responding “old” to a nondetected item, <span class="math inline">\(a\)</span> stands for guessing that a detected but nondiscriminated item belongs to Source A, and <span class="math inline">\(g\)</span> stands for guessing that the item is a source A item.
</p>
</div>

<div class="figure"><span style="display:block;" id="fig:sm-tikz3"></span>
<img src="bayescogsci_files/figure-html/sm-tikz3-1.svg" alt="Multinomial processing tree for the new items in the source monitoring paradigm (Batchelder and Riefer, 1990). \(b\) stands for the bias for responding “old” to a nondetected item, \(a\) stands for guessing that a detected but nondiscriminated item belongs to source A, and \(g\) stands for guessing that the item is a source A item." width="672" />
<p class="caption">
FIGURE G.3: Multinomial processing tree for the new items in the source monitoring paradigm (Batchelder and Riefer, 1990). <span class="math inline">\(b\)</span> stands for the bias for responding “old” to a nondetected item, <span class="math inline">\(a\)</span> stands for guessing that a detected but nondiscriminated item belongs to source A, and <span class="math inline">\(g\)</span> stands for guessing that the item is a source A item.
</p>
</div>
<p>Notice the following:</p>
<ul>
<li>The data are aggregated at the level of source, so you should use <code>multinomial_lpmf</code> for every row of the data set rather than <code>categorical_lpmf()</code>.</li>
<li>In contrast to the previous example, <code>source</code> determines three different trees, this means that the parameter <code>theta</code> has to be defined in relationship to the item source.</li>
<li>All the predictors are between subject, this means that only a by-intercept adjustment (for every latent process) is possible.</li>
</ul>
<p>If you want some basis to start with, you can have a look at the incomplete code in <code>source.stan</code>, by typing the following in R:</p>
<div class="sourceCode" id="cb1244"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1244-1"><a href="exercises.html#cb1244-1" aria-hidden="true"></a><span class="kw">cat</span>(<span class="kw">readLines</span>(<span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb1244-2"><a href="exercises.html#cb1244-2" aria-hidden="true"></a>                     <span class="st">&quot;source.stan&quot;</span>,</span>
<span id="cb1244-3"><a href="exercises.html#cb1244-3" aria-hidden="true"></a>                     <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)),</span>
<span id="cb1244-4"><a href="exercises.html#cb1244-4" aria-hidden="true"></a>    <span class="dt">sep =</span> <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
</div>
</div>
<div id="mixture-models" class="section level2 hasAnchor" number="26.17">
<h2><span class="header-section-number">G.17</span> Mixture models<a href="exercises.html#mixture-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:pcorrect" class="section level3 unlisted hasAnchor" number="26.17.1">
<h3><span class="header-section-number">G.17.1</span> Changes in the true point values.<a href="exercises.html#exr:pcorrect" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Change the true point value of <code>p_correct</code> to <span class="math inline">\(0.5\)</span> and <span class="math inline">\(0.1\)</span>, and generate data for the non-hierarchical model. Can you recover the value of this parameter without changing the model <code>mixture_rtacc2.stan</code>? Perform posterior predictive checks.</p>
</div>
<div id="exr:mixhier" class="section level3 unlisted hasAnchor" number="26.17.2">
<h3><span class="header-section-number">G.17.2</span> RTs in schizophrenic patients and control.<a href="exercises.html#exr:mixhier" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Response times for schizophrenic patients in a simple visual tracking experiment show more variability than for non-schizophrenic controls; see Figure <a href="exercises.html#fig:schiz">G.4</a>. It has been argued that at least some of this extra variability arises from an attentional lapse that delays some responses. We’ll use the data examined in <span class="citation">Belin and Rubin (<a href="#ref-belin1990" role="doc-biblioref">1990</a>)</span> (<code>df_schizophrenia</code> in the <code>bcogsci</code> package) analysis to investigate some potential models:</p>
<ul>
<li><span class="math inline">\(M_1\)</span>. Both schizophrenic and controls show attentional lapses, but the lapses are more common in schizophrenics. Other than that there is no difference in the latent response times and the lapses of attention.</li>
<li><span class="math inline">\(M_2\)</span>. Only schizophrenic patients show attentional lapses. Other than that there is no difference in the latent response times.</li>
<li><span class="math inline">\(M_3\)</span>. There are no (meaningful number of) lapses of attention in either group.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Fit the three models.</li>
<li>Carry out posterior predictive checks for each model; can they account for the data?</li>
<li>Carry out model comparison (with Bayes factor and cross-validation).</li>
</ol>

<div class="sourceCode" id="cb1245"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb1245-1"><a href="exercises.html#cb1245-1" aria-hidden="true"></a></span>
<span id="cb1245-2"><a href="exercises.html#cb1245-2" aria-hidden="true"></a><span class="kw">ggplot</span>(df_schizophrenia, <span class="kw">aes</span>(rt)) <span class="op">+</span></span>
<span id="cb1245-3"><a href="exercises.html#cb1245-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span></span>
<span id="cb1245-4"><a href="exercises.html#cb1245-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">facet_grid</span>(<span class="dt">rows =</span> <span class="kw">vars</span>(<span class="kw">factor</span>(patient, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;control&quot;</span>, <span class="st">&quot;schizophrenic&quot;</span>))))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:schiz"></span>
<img src="bayescogsci_files/figure-html/schiz-1.svg" alt="The distribution of response times for control and schizophrenic patients in df_schizophrenia." width="672" />
<p class="caption">
FIGURE G.4: The distribution of response times for control and schizophrenic patients in <code>df_schizophrenia</code>.
</p>
</div>
</div>
<div id="exr:mixbias" class="section level3 unlisted hasAnchor" number="26.17.3">
<h3><span class="header-section-number">G.17.3</span> <strong>Advanced:</strong> Guessing bias in the model.<a href="exercises.html#exr:mixbias" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the original model, it was assumed that subjects might have a bias (a preference) to one of the two answers when they were in the guessing mode. To fit this model we need to change the dependent variable and add more information; now we not only care if the participant answered correctly or not, but also which answer they gave (left or right).</p>
<ul>
<li>Implement a unique bias for all the subjects. Fit the new model to (a subset of) the data.</li>
<li>Implement a hierarchical bias, that is there is a common bias, but every subject has its adjustment. Fit the new model to (a subset of) the data.</li>
</ul>
</div>
</div>
<div id="a-simple-accumulator-model-to-account-for-choice-response-time" class="section level2 hasAnchor" number="26.18">
<h2><span class="header-section-number">G.18</span> A simple accumulator model to account for choice response time<a href="exercises.html#a-simple-accumulator-model-to-account-for-choice-response-time" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:recovery" class="section level3 unlisted hasAnchor" number="26.18.1">
<h3><span class="header-section-number">G.18.1</span> Can we recover the true point values of the parameters of a model when dealing with a contaminant distribution?<a href="exercises.html#exr:recovery" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Section <a href="ch-lognormalrace.html#sec-contaminant">18.1.5</a>, we fit a hierarchical model that assumed a contaminant distribution (<code>lnrace_h_cont.stan</code>) without first verifying that we can recover the true point values of its parameters if we simulate data. An important first step would be to work with a non-hierarchical version of this model.</p>
<ol style="list-style-type: decimal">
<li>Generate data of one subject as in section <a href="ch-lognormalrace.html#sec-genaccum">18.1.2</a>, but assume a contaminant distribution as in section <a href="ch-lognormalrace.html#sec-contaminant">18.1.5</a>.</li>
<li>Fit a non-hierarchical version of <code>lnrace_h_cont.stan</code> without restricting the parameter <code>theta_c</code> to be smaller than <code>0.1</code>.</li>
<li>Plot the posterior distributions of the model and verify that you can recover the true values of the parameters.</li>
</ol>
</div>
<div id="exr:lnracescale" class="section level3 unlisted hasAnchor" number="26.18.2">
<h3><span class="header-section-number">G.18.2</span> Can the log-normal race model account for fast errors?<a href="exercises.html#exr:lnracescale" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Subject 13 shows fast errors for incorrect responses. This can be seen in the left side of the quantile probability plot in Figure <a href="exercises.html#fig:qpp13">G.5</a>.</p>
<ol style="list-style-type: decimal">
<li>Fit a log-normal race model (with equal scales for the two accumulator) that accounts for contaminant responses.</li>
<li>Fit a variation of this model, where whether the lexicality of the string matches or not the accumulator affects its scale.</li>
<li>Visualize the fit of each model with quantile probability plots.</li>
<li>Use cross-validation to compare the models.</li>
</ol>
<p>Notice that the models should be fit to only one subject and they should not have a hierarchical structure.</p>

<div class="figure"><span style="display:block;" id="fig:qpp13"></span>
<img src="bayescogsci_files/figure-html/qpp13-1.svg" alt="Quantile probability plot showing 0.1, 0.3, 0.5, 0.7, and 0.9 response times quantiles plotted against proportion of incorrect responses (left) and proportion of correct responses (right) for only words of different frequency for subject number 13." width="672" />
<p class="caption">
FIGURE G.5: Quantile probability plot showing 0.1, 0.3, 0.5, 0.7, and 0.9 response times quantiles plotted against proportion of incorrect responses (left) and proportion of correct responses (right) for only words of different frequency for subject number 13.
</p>
</div>
</div>
<div id="exr:lnldt" class="section level3 unlisted hasAnchor" number="26.18.3">
<h3><span class="header-section-number">G.18.3</span> Accounting for response time and choice in the lexical decision task using the log-normal race model.<a href="exercises.html#exr:lnldt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Chapter <a href="ch-mixture.html#ch-mixture">17</a>, we modeled the data of the global motion detection task from <span class="citation">Dutilh et al. (<a href="#ref-DutilhEtAl2011" role="doc-biblioref">2011</a>)</span> (<code>df_dots</code>) using a mixture model. Now, we’ll investigate what happens if we fit a log-normal race model to the same data. As a reminder, in this type of task, subjects see a number of random dots on the screen from which a proportion of them move in a single direction (left or right) and the rest move in random directions. The goal of the task is to estimate the overall direction of the movement. In this data set, there are two difficulty levels (<code>diff</code>) and two types of instructions (<code>emphasis</code>) that focus on accuracy or speed. (More information about the data set can be found by loading the <code>bcogsci</code> package and typing <code>?df_dots</code> in the R console). For the sake of speed, we’ll fit only one subject from this data set.</p>
<ol style="list-style-type: decimal">
<li><p>Before modeling the data, show the relationship between response times and accuracy with a quantile probability plot that shows quantiles and accuracy of easy and hard difficulty conditions.</p></li>
<li><p>Fit a non-hierarchical log-normal race model to account for how both choice and response time are affected by task difficulty and emphasis. Assume no contaminant distribution of responses.</p></li>
</ol>
<p>Note that the direction of the dots is indicated with <code>stim</code>, when <code>stim</code> and <code>resp</code> match, both are <code>L</code>, left, or both are <code>R</code>, right, the accuracy, <code>acc</code>, is 1. For modeling this task with a log-normal race model, the difficulty of the task should be coded in a way that reflects that the stimuli will be harder to detect for the relevant accumulator. One way to do it is the following:</p>
<div class="sourceCode" id="cb1246"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1246-1"><a href="exercises.html#cb1246-1" aria-hidden="true"></a>df_dots_subset &lt;-<span class="st"> </span>df_dots <span class="op">%&gt;%</span></span>
<span id="cb1246-2"><a href="exercises.html#cb1246-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">filter</span>(subj <span class="op">==</span><span class="st"> </span><span class="dv">1</span>)</span>
<span id="cb1246-3"><a href="exercises.html#cb1246-3" aria-hidden="true"></a></span>
<span id="cb1246-4"><a href="exercises.html#cb1246-4" aria-hidden="true"></a>df_dots_subset &lt;-<span class="st"> </span>df_dots_subset <span class="op">%&gt;%</span></span>
<span id="cb1246-5"><a href="exercises.html#cb1246-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_diff =</span> <span class="kw">case_when</span>(stim <span class="op">==</span><span class="st"> &quot;L&quot;</span> <span class="op">&amp;</span><span class="st"> </span>diff <span class="op">==</span><span class="st"> &quot;easy&quot;</span> <span class="op">~</span><span class="st"> </span><span class="fl">.5</span>,</span>
<span id="cb1246-6"><a href="exercises.html#cb1246-6" aria-hidden="true"></a>                            stim <span class="op">==</span><span class="st"> &quot;L&quot;</span> <span class="op">&amp;</span><span class="st"> </span>diff <span class="op">==</span><span class="st"> &quot;hard&quot;</span> <span class="op">~</span><span class="st"> </span><span class="fl">-.5</span>,</span>
<span id="cb1246-7"><a href="exercises.html#cb1246-7" aria-hidden="true"></a>                            stim <span class="op">==</span><span class="st"> &quot;R&quot;</span> <span class="op">&amp;</span><span class="st"> </span>diff <span class="op">==</span><span class="st"> &quot;easy&quot;</span> <span class="op">~</span><span class="st"> </span><span class="fl">-.5</span>,</span>
<span id="cb1246-8"><a href="exercises.html#cb1246-8" aria-hidden="true"></a>                            stim <span class="op">==</span><span class="st"> &quot;R&quot;</span> <span class="op">&amp;</span><span class="st"> </span>diff <span class="op">==</span><span class="st"> &quot;hard&quot;</span> <span class="op">~</span><span class="st"> </span><span class="fl">.5</span>,</span>
<span id="cb1246-9"><a href="exercises.html#cb1246-9" aria-hidden="true"></a>                            ))</span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li><p>Expand the previous model including a contaminant distribution of responses.</p></li>
<li><p>Visualize the fit of the two previous models by doing posterior predictive checks using quantile probability plots.</p></li>
<li><p>Use cross-validation to compare the models.</p></li>
</ol>
</div>
</div>
<div id="sec-priorsexercises" class="section level2 hasAnchor" number="26.19">
<h2><span class="header-section-number">G.19</span> The Art and Science of  Prior Elicitation<a href="exercises.html#sec-priorsexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exr:PriorsRCs" class="section level3 unlisted hasAnchor" number="26.19.1">
<h3><span class="header-section-number">G.19.1</span> Develop a plausible informative prior for the difference between object and subject relative clause reading times<a href="exercises.html#exr:PriorsRCs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Do a literature search on reading studies involving subject-modifying relative clause processing, and use the estimates from these published studies to work out a plausible informative prior for reading time differences at the relative clause verb. Some examples of relevant studies are <span class="citation">Gibson et al. (<a href="#ref-gibson2005reading" role="doc-biblioref">2005</a>)</span>, experiment 1 of <span class="citation">Grodner and Gibson (<a href="#ref-grodner" role="doc-biblioref">2005</a>)</span>, and <span class="citation">Fedorenko, Gibson, and Rohde (<a href="#ref-fedorenko2006nature" role="doc-biblioref">2006</a>)</span>; there are many others. (Note: This is an open-ended exercise with no correct answer.)</p>
</div>
<div id="exr:Priorslocalcoherence" class="section level3 unlisted hasAnchor" number="26.19.2">
<h3><span class="header-section-number">G.19.2</span> Extracting an informative prior from a published paper for a future study<a href="exercises.html#exr:Priorslocalcoherence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In their experiment 1, <span class="citation">Tabor, Galantucci, and Richardson (<a href="#ref-taboretal04" role="doc-biblioref">2004</a>)</span> present a self-paced reading study with a repeated measures <span class="math inline">\(2\times 2\)</span> design. The details of the experiment are not important here, but a key estimate of interest in their reported results is the interaction of the two factors. The reported interaction estimate is <span class="math inline">\(73\)</span> ms, with a by-subjects repeated measures ANOVA F-score of <span class="math inline">\(6.32\)</span>. Given this information, work out the standard error (SE) of the estimate of the <span class="math inline">\(73\)</span> ms. Using the estimated interaction effect and the estimated SE, derive an informative prior for a planned study that attempts to directly replicate experiment 1 in <span class="citation">Tabor, Galantucci, and Richardson (<a href="#ref-taboretal04" role="doc-biblioref">2004</a>)</span>. (Hint: The F-score here is the square of the corresponding observed t-value, and we know that the t-value in a one-sample t-test is computed using the formula <span class="math inline">\(t=\frac{\bar{x}-0}{SE}\)</span>, where <span class="math inline">\(\bar{x}\)</span> is the estimate of the effect of interest, here, the interaction effect.)</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references hanging-indent">
<div id="ref-batchelder1990multinomial">
<p>Batchelder, William H., and David M. Riefer. 1990. “Multinomial Processing Models of Source Monitoring.” <em>Psychological Review</em> 97 (4): 548.</p>
</div>
<div id="ref-beall2013women">
<p>Beall, Alec T., and Jessica L. Tracy. 2013. “Women Are More Likely to Wear Red or Pink at Peak Fertility.” <em>Psychological Science</em> 24 (9): 1837–41.</p>
</div>
<div id="ref-belin1990">
<p>Belin, T. R., and Donald B. Rubin. 1990. “Analysis of a Finite Mixture Model with Variance Components.” In <em>Proceedings of the Social Statistics Section</em>, 211–15.</p>
</div>
<div id="ref-Broadbent1987">
<p>Broadbent, Donald E., and Margaret H. P. Broadbent. 1987. “From Detection to Identification: Response to Multiple Targets in Rapid Serial Visual Presentation.” <em>Perception &amp; Psychophysics</em> 42 (2): 105–13. <a href="https://doi.org/10.3758/BF03210498">https://doi.org/10.3758/BF03210498</a>.</p>
</div>
<div id="ref-BuerkiEtAl2020">
<p>Bürki, Audrey, Shereen Elbuy, Sylvain Madec, and Shravan Vasishth. 2020. “What Did We Learn from Forty Years of Research on Semantic Interference? A Bayesian Meta-Analysis.” <em>Journal of Memory and Language</em> 114. <a href="https://doi.org/10.1016/j.jml.2020.104125">https://doi.org/10.1016/j.jml.2020.104125</a>.</p>
</div>
<div id="ref-carney2010power">
<p>Carney, Dana R., Amy J. C. Cuddy, and Andy J. Yap. 2010. “Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance.” <em>Psychological Science</em> 21 (10): 1363–8.</p>
</div>
<div id="ref-Dillon-EtAl-2013">
<p>Dillon, Brian W., Alan Mishler, Shayne Sloggett, and Colin Phillips. 2013. “Contrasting Intrusion Profiles for Agreement and Anaphora: Experimental and Modeling Evidence.” <em>Journal of Memory and Language</em> 69 (2): 85–103. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2013.04.003">https://doi.org/https://doi.org/10.1016/j.jml.2013.04.003</a>.</p>
</div>
<div id="ref-DutilhEtAl2011">
<p>Dutilh, Gilles, Eric-Jan Wagenmakers, Ingmar Visser, and Han L. J. van der Maas. 2011. “A Phase Transition Model for the Speed-Accuracy Trade-Off in Response Time Experiments.” <em>Cognitive Science</em> 35 (2): 211–50. <a href="https://doi.org/10.1111/j.1551-6709.2010.01147.x">https://doi.org/10.1111/j.1551-6709.2010.01147.x</a>.</p>
</div>
<div id="ref-EngelmannJaegerVasishth2019">
<p>Engelmann, Felix, Lena A. Jäger, and Shravan Vasishth. 2020. “The Effect of Prominence and Cue Association in Retrieval Processes: A Computational Account.” <em>Cognitive Science</em> 43 (12): e12800. <a href="https://doi.org/10.1111/cogs.12800">https://doi.org/10.1111/cogs.12800</a>.</p>
</div>
<div id="ref-fedorenko2006nature">
<p>Fedorenko, Evelina, Edward Gibson, and Douglas Rohde. 2006. “The Nature of Working Memory Capacity in Sentence Comprehension: Evidence Against Domain-Specific Working Memory Resources.” <em>Journal of Memory and Language</em> 54 (4): 541–53. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2005.12.006">https://doi.org/https://doi.org/10.1016/j.jml.2005.12.006</a>.</p>
</div>
<div id="ref-FossePowerPose">
<p>Fosse, Nathan E. 2016. “Replication Data for "Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance" by Carney, Cuddy, Yap (2010).” Harvard Dataverse. <a href="https://doi.org/10.7910/DVN/FMEGS6">https://doi.org/10.7910/DVN/FMEGS6</a>.</p>
</div>
<div id="ref-FrankEtAl2015">
<p>Frank, Stefan L., Thijs Trompenaars, and Shravan Vasishth. 2015. “Cross-Linguistic Differences in Processing Double-Embedded Relative Clauses: Working-Memory Constraints or Language Statistics?” <em>Cognitive Science</em> 40: 554–78. <a href="https://doi.org/10.1111/cogs.12247">https://doi.org/10.1111/cogs.12247</a>.</p>
</div>
<div id="ref-gibson2005reading">
<p>Gibson, Edward, Timothy Desmet, Daniel Grodner, Duane Watson, and Kara Ko. 2005. “Reading Relative Clauses in English.” <em>Cognitive Linguistics</em> 16 (2): 313–53. <a href="https://doi.org/10.1515/cogl.2005.16.2.313">https://doi.org/10.1515/cogl.2005.16.2.313</a>.</p>
</div>
<div id="ref-gibsonthomas99">
<p>Gibson, Edward, and James Thomas. 1999. “Memory Limitations and Structural Forgetting: The Perception of Complex Ungrammatical Sentences as Grammatical.” <em>Language and Cognitive Processes</em> 14(3): 225–48. <a href="https://doi.org/https://doi.org/10.1080/016909699386293">https://doi.org/https://doi.org/10.1080/016909699386293</a>.</p>
</div>
<div id="ref-gibsonwu">
<p>Gibson, Edward, and H.-H. Iris Wu. 2013. “Processing Chinese Relative Clauses in Context.” <em>Language and Cognitive Processes</em> 28 (1-2): 125–55. <a href="https://doi.org/https://doi.org/10.1080/01690965.2010.536656">https://doi.org/https://doi.org/10.1080/01690965.2010.536656</a>.</p>
</div>
<div id="ref-grassi_two_2021">
<p>Grassi, Massimo, Camilla Crotti, David Giofrè, Ingrid Boedker, and Enrico Toffalini. 2021. “Two Replications of Raymond, Shapiro, and Arnell (1992), the Attentional Blink.” <em>Behavior Research Methods</em> 53 (2): 656–68. <a href="https://doi.org/10.3758/s13428-020-01457-6">https://doi.org/10.3758/s13428-020-01457-6</a>.</p>
</div>
<div id="ref-grodner">
<p>Grodner, Daniel, and Edward Gibson. 2005. “Consequences of the Serial Nature of Linguistic Input.” <em>Cognitive Science</em> 29: 261–90. <a href="https://doi.org/https://doi.org/10.1207/s15516709cog0000_7">https://doi.org/https://doi.org/10.1207/s15516709cog0000_7</a>.</p>
</div>
<div id="ref-heathcote2004fitting">
<p>Heathcote, Andrew. 2004. “Fitting Wald and ex-Wald Distributions to Response Time Data: An Example Using Functions for the S-Plus Package.” <em>Behavior Research Methods, Instruments, &amp; Computers</em> 36 (4): 678–94. <a href="https://doi.org/https://doi.org/10.3758/BF03206550">https://doi.org/https://doi.org/10.3758/BF03206550</a>.</p>
</div>
<div id="ref-hsiao03">
<p>Hsiao, Fanny Pai-Fang, and Edward Gibson. 2003. “Processing Relative Clauses in Chinese.” <em>Cognition</em> 90: 3–27. <a href="https://doi.org/https://doi.org/10.1016/S0010-0277(03)00124-0">https://doi.org/https://doi.org/10.1016/S0010-0277(03)00124-0</a>.</p>
</div>
<div id="ref-JaegerMertzenVanDykeVasishth2019">
<p>Jäger, Lena A., Daniela Mertzen, Julie A. Van Dyke, and Shravan Vasishth. 2020. “Interference Patterns in Subject-Verb Agreement and Reflexives Revisited: A Large-Sample Study.” <em>Journal of Memory and Language</em> 111. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2019.104063">https://doi.org/https://doi.org/10.1016/j.jml.2019.104063</a>.</p>
</div>
<div id="ref-lewisvasishth:cogsci05">
<p>Lewis, Richard L., and Shravan Vasishth. 2005. “An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval.” <em>Cognitive Science</em> 29: 1–45. <a href="https://doi.org/%2010.1207/s15516709cog0000_25">https://doi.org/ 10.1207/s15516709cog0000_25</a>.</p>
</div>
<div id="ref-Paolaccietal">
<p>Paolacci, Gabriele, Jesse Chandler, and Panagiotis G Ipeirotis. 2010. “Running Experiments on Amazon Mechanical Turk.” <em>Judgment and Decision Making</em> 5 (5): 411–19. <a href="https://doi.org/10.1017/S1930297500002205">https://doi.org/10.1017/S1930297500002205</a>.</p>
</div>
<div id="ref-raymond1992temporary">
<p>Raymond, Jane E., Kimron L. Shapiro, and Karen M. Arnell. 1992. “Temporary Suppression of Visual Processing in an RSVP Task: An Attentional Blink?” <em>Journal of Experimental Psychology: Human Perception and Performance</em> 18 (3): 849. <a href="https://doi.org/https://doi.org/10.1037/0096-1523.18.3.849">https://doi.org/https://doi.org/10.1037/0096-1523.18.3.849</a>.</p>
</div>
<div id="ref-SafaviEtAlFrontiers2016">
<p>Safavi, Molood Sadat, Samar Husain, and Shravan Vasishth. 2016. “Dependency Resolution Difficulty Increases with Distance in Persian Separable Complex Predicates: Implications for Expectation and Memory-Based Accounts.” <em>Frontiers in Psychology</em> 7 (403). <a href="https://doi.org/10.3389/fpsyg.2016.00403">https://doi.org/10.3389/fpsyg.2016.00403</a>.</p>
</div>
<div id="ref-taboretal04">
<p>Tabor, Whitney, Bruno Galantucci, and Daniel Richardson. 2004. “Effects of Merely Local Syntactic Coherence on Sentence Processing.” <em>Journal of Memory and Language</em> 50: 355–70.</p>
</div>
<div id="ref-TverskyKahneman1983">
<p>Tversky, Amos, and Daniel Kahneman. 1983. “Extensional Versus Intuitive Reasoning: The Conjunction Fallacy in Probability Judgment.” <em>Psychological Review</em> 90 (4): 293. <a href="https://doi.org/https://doi.org/10.1037/0033-295X.90.4.293">https://doi.org/https://doi.org/10.1037/0033-295X.90.4.293</a>.</p>
</div>
<div id="ref-VBLD07">
<p>Vasishth, Shravan, Sven Bruessow, Richard L. Lewis, and Heiner Drenhaus. 2008. “Processing Polarity: How the Ungrammatical Intrudes on the Grammatical.” <em>Cognitive Science</em> 32 (4, 4): 685–712. <a href="https://doi.org/https://doi.org/10.1080/03640210802066865">https://doi.org/https://doi.org/10.1080/03640210802066865</a>.</p>
</div>
<div id="ref-VasishthetalPLoSOne2013">
<p>Vasishth, Shravan, Zhong Chen, Qiang Li, and Gueilan Guo. 2013. “Processing Chinese Relative Clauses: Evidence for the Subject-Relative Advantage.” <em>PLoS ONE</em> 8 (10): 1–14. <a href="https://doi.org/https://doi.org/10.1371/journal.pone.0077006">https://doi.org/https://doi.org/10.1371/journal.pone.0077006</a>.</p>
</div>
<div id="ref-VSLK08">
<p>Vasishth, Shravan, Katja Suckow, Richard L. Lewis, and Sabine Kern. 2011. “Short-Term Forgetting in Sentence Comprehension: Crosslinguistic Evidence from Head-Final Structures.” <em>Language and Cognitive Processes</em> 25: 533–67. <a href="https://doi.org/https://doi.org/10.1080/01690960903310587">https://doi.org/https://doi.org/10.1080/01690960903310587</a>.</p>
</div>
<div id="ref-wahnPupilSizesScale2016">
<p>Wahn, Basil, Daniel P. Ferris, W. David Hairston, and Peter König. 2016. “Pupil Sizes Scale with Attentional Load and Task Experience in a Multiple Object Tracking Task.” <em>PLOS ONE</em> 11 (12): e0168087. <a href="https://doi.org/10.1371/journal.pone.0168087">https://doi.org/10.1371/journal.pone.0168087</a>.</p>
</div>
<div id="ref-wickelmaier2018using">
<p>Wickelmaier, Florian, and Achim Zeileis. 2018. “Using Recursive Partitioning to Account for Parameter Heterogeneity in Multinomial Processing Tree Models.” <em>Behavior Research Methods</em> 50 (3): 1217–33. <a href="https://doi.org/https://doi.org/10.3758/s13428-017-0937-z">https://doi.org/https://doi.org/10.3758/s13428-017-0937-z</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-workflow.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
