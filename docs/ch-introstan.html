<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Introduction to the probabilistic programming language Stan | Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Introduction to the probabilistic programming language Stan | Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://bruno.nicenboim.me/bayescogsci/images/temporarycover.jpg" />
  <meta property="og:description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/bnicenboim/bayescogsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Introduction to the probabilistic programming language Stan | Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bruno.nicenboim.me/bayescogsci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel J. Schad, and Shravan Vasishth" />


<meta name="date" content="2025-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-coding2x2.html"/>
<link rel="next" href="ch-complexstan.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<script data-goatcounter="https://bayescogsci.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b>  Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The  law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b>  Discrete random variables: An example using the  binomial distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b>  Continuous random variables: An example using the  normal distribution</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1:  Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (or multivariate) data</a></li>
<li class="chapter" data-level="1.6.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-decomposevcovmatrix"><i class="fa fa-check"></i><b>1.6.4</b> Decomposing a variance-covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-some-useful-r-functions"><i class="fa fa-check"></i><b>1.8</b> Summary of some useful R functions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b>  Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the  posterior using Bayes’ rule: An analytical example</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a  likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a  prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using  Bayes’ rule to compute the  posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The  posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the  posterior through  sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b>  Bayesian Regression Models using Stan:  brms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors:  sensitivity analysis</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b>  Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b>  Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b>  Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b>  Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b>  Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The  log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first  linear regression: Does attentional load affect pupil size?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b>  Likelihood and  priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b>  Log-normal model: Does trial affect response times?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b>  Logistic regression: Does  set size affect  free recall?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b>  Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b>  Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b>  No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b>  Varying intercepts and  varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A  hierarchical log-normal model: The  Stroop effect</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes  log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>6.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>6.1.1</b> Default contrast coding:  Treatment contrasts</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>6.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>6.1.3</b>  Sum contrasts</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>6.1.4</b>  Cell means parameterization and  posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>6.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>6.2.1</b>  Sum contrasts</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>6.2.2</b> The  hypothesis matrix</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>6.2.3</b> Generating contrasts: The  <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>6.3</b> Other types of contrasts: illustration with a factor of four levels</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>6.3.1</b>  Repeated contrasts</a></li>
<li class="chapter" data-level="6.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>6.3.2</b>  Helmert contrasts</a></li>
<li class="chapter" data-level="6.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>6.3.3</b> Contrasts in linear regression analysis: The design or  model matrix</a></li>
<li class="chapter" data-level="6.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>6.3.4</b>  Polynomial contrasts</a></li>
<li class="chapter" data-level="6.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>6.3.5</b> An alternative to contrasts:  Monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>6.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>6.4.1</b>  Centered contrasts</a></li>
<li class="chapter" data-level="6.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>6.4.2</b>  Orthogonal contrasts</a></li>
<li class="chapter" data-level="6.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> The role of the  intercept in  non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>6.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="6.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-5"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
<li class="chapter" data-level="6.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-3"><i class="fa fa-check"></i><b>6.7</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>7</b> Contrast coding with two predictor variables</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>7.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>7.1.1</b>  Nested effects</a></li>
<li class="chapter" data-level="7.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>7.1.2</b>  Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>7.2</b> One factor and one  covariate</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>7.2.1</b> Estimating a  group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>7.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>7.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="7.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="8" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>8</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>8.1</b> Stan syntax</a></li>
<li class="chapter" data-level="8.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>8.2</b> A first simple example with Stan:  Normal likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>8.3</b> Another simple example:  Cloze probability with Stan with the  binomial likelihood</a></li>
<li class="chapter" data-level="8.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>8.4</b>  Regression models in Stan</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>8.4.1</b> A first  linear regression in Stan: Does attentional load affect  pupil size?</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>8.4.2</b>  Interactions in Stan: Does attentional load interact with trial number affecting  pupil size?</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>8.4.3</b>  Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-7"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-5"><i class="fa fa-check"></i><b>8.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>9</b> Hierarchical models and reparameterization </a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>9.1</b> Hierarchical models with Stan</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>9.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>9.1.2</b> Uncorrelated  varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="9.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>9.1.3</b>  Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="9.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>9.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-8"><i class="fa fa-check"></i><b>9.2</b> Summary</a></li>
<li class="chapter" data-level="9.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-6"><i class="fa fa-check"></i><b>9.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>10</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>10.1</b> A change of variables with the reciprocal normal distribution</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>10.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>10.2</b>  Validation of a computed posterior distribution</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>10.2.1</b> The  simulation-based calibration procedure</a></li>
<li class="chapter" data-level="10.2.2" data-path="ch-custom.html"><a href="ch-custom.html#an-example-where-simulation-based-calibration-reveals-a-problem"><i class="fa fa-check"></i><b>10.2.2</b> An example where simulation-based calibration reveals a problem</a></li>
<li class="chapter" data-level="10.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-with-and-limitations-of-simulation-based-calibration"><i class="fa fa-check"></i><b>10.2.3</b> Issues with and limitations of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-the-exponential-distribution-implemented-manually"><i class="fa fa-check"></i><b>10.3</b> Another  custom distribution: The exponential distribution  implemented manually</a></li>
<li class="chapter" data-level="10.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-9"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
<li class="chapter" data-level="10.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-7"><i class="fa fa-check"></i><b>10.5</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="11" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>11</b>  Meta-analysis and  measurement error models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>11.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>11.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>11.2</b>  Measurement-error models</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>11.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-10"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-8"><i class="fa fa-check"></i><b>11.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison</b></span></li>
<li class="chapter" data-level="12" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>12</b> Introduction to model comparison</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-comparison.html"><a href="ch-comparison.html#prior-predictive-vs.-posterior-predictive-model-comparison"><i class="fa fa-check"></i><b>12.1</b> Prior predictive vs. posterior predictive model comparison</a></li>
<li class="chapter" data-level="12.2" data-path="ch-comparison.html"><a href="ch-comparison.html#some-important-points-to-consider-when-comparing-models"><i class="fa fa-check"></i><b>12.2</b> Some important points to consider when comparing models</a></li>
<li class="chapter" data-level="12.3" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-9"><i class="fa fa-check"></i><b>12.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>13</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>13.1</b> Hypothesis testing using the Bayes factor</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>13.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="13.1.2" data-path="ch-bf.html"><a href="ch-bf.html#the-bayes-factor"><i class="fa fa-check"></i><b>13.1.2</b> The Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>13.2</b> Examining the N400 effect with the Bayes factor</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>13.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="13.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>13.2.2</b>  Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>13.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="13.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>13.4</b>  The Bayes factor in Stan</a></li>
<li class="chapter" data-level="13.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>13.5</b> Bayes factors in theory and in practice</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>13.5.1</b> Bayes factors in theory: Stability and  accuracy</a></li>
<li class="chapter" data-level="13.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>13.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="13.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>13.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="ch-bf.html"><a href="ch-bf.html#sample-size-determination-using-bayes-factors"><i class="fa fa-check"></i><b>13.6</b> Sample size determination using Bayes factors</a></li>
<li class="chapter" data-level="13.7" data-path="ch-bf.html"><a href="ch-bf.html#summary-11"><i class="fa fa-check"></i><b>13.7</b> Summary</a></li>
<li class="chapter" data-level="13.8" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-10"><i class="fa fa-check"></i><b>13.8</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>14</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>14.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="14.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>14.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="14.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>14.3</b> Testing the N400 effect using cross-validation</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>14.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="14.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>14.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="14.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>14.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>14.4</b>  Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="14.5" data-path="ch-cv.html"><a href="ch-cv.html#sec-issuesCV"><i class="fa fa-check"></i><b>14.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="14.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>14.6</b> Cross-validation in Stan</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>14.6.1</b>  PSIS-LOO-CV in Stan</a></li>
<li class="chapter" data-level="14.6.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-cv-in-stan"><i class="fa fa-check"></i><b>14.6.2</b>  K-fold-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-12"><i class="fa fa-check"></i><b>14.7</b> Summary</a></li>
<li class="chapter" data-level="14.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-11"><i class="fa fa-check"></i><b>14.8</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>VI Cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="15" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>15</b> Introduction to cognitive modeling</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#what-characterizes-a-computational-cognitive-model"><i class="fa fa-check"></i><b>15.1</b> What characterizes a computational cognitive model?</a></li>
<li class="chapter" data-level="15.2" data-path="ch-cogmod.html"><a href="ch-cogmod.html#some-advantages-of-taking-the-latent-variable-modeling-approach"><i class="fa fa-check"></i><b>15.2</b> Some advantages of taking the latent-variable modeling approach</a></li>
<li class="chapter" data-level="15.3" data-path="ch-cogmod.html"><a href="ch-cogmod.html#types-of-computational-cognitive-model"><i class="fa fa-check"></i><b>15.3</b> Types of computational cognitive model</a></li>
<li class="chapter" data-level="15.4" data-path="ch-cogmod.html"><a href="ch-cogmod.html#summary-13"><i class="fa fa-check"></i><b>15.4</b> Summary</a></li>
<li class="chapter" data-level="15.5" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-12"><i class="fa fa-check"></i><b>15.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>16</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>16.1</b> Modeling  multiple categorical responses</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>16.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="16.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>16.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>16.2</b> Modeling picture naming abilities in aphasia with MPT models</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>16.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="16.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>16.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="16.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>16.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="16.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>16.2.4</b> A  hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-14"><i class="fa fa-check"></i><b>16.3</b> Summary</a></li>
<li class="chapter" data-level="16.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-13"><i class="fa fa-check"></i><b>16.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>17</b> Mixture models</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>17.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>17.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="17.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>17.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="17.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>17.1.3</b> A  multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="17.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>17.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="17.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>17.1.5</b> A  hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-15"><i class="fa fa-check"></i><b>17.2</b> Summary</a></li>
<li class="chapter" data-level="17.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-14"><i class="fa fa-check"></i><b>17.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>18</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>18.1</b> Modeling a lexical decision task</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>18.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>18.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="18.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>18.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="18.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>18.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="18.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>18.1.5</b> Dealing with  contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>18.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="18.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-16"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-closing.html"><a href="ch-closing.html"><i class="fa fa-check"></i><b>19</b> In closing</a></li>
<li class="appendix"><span><b>Online materials</b></span></li>
<li class="chapter" data-level="A" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html"><i class="fa fa-check"></i><b>A</b> Regression models with <code>brms</code> - Extended</a>
<ul>
<li class="chapter" data-level="A.1" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-efficientpriorpd"><i class="fa fa-check"></i><b>A.1</b> An efficient function for generating prior predictive distributions in R</a></li>
<li class="chapter" data-level="A.2" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-truncation"><i class="fa fa-check"></i><b>A.2</b> Truncated distributions</a></li>
<li class="chapter" data-level="A.3" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-intercept"><i class="fa fa-check"></i><b>A.3</b> Intercepts in <code>brms</code></a></li>
<li class="chapter" data-level="A.4" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-lognormal"><i class="fa fa-check"></i><b>A.4</b> Understanding the log-normal likelihood</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#log-normal-distributions-everywhere"><i class="fa fa-check"></i><b>A.4.1</b> Log-normal distributions everywhere</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-priorR"><i class="fa fa-check"></i><b>A.5</b> Prior predictive checks in R</a></li>
<li class="chapter" data-level="A.6" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-exch"><i class="fa fa-check"></i><b>A.6</b> Finitely exchangeable random variables</a></li>
<li class="chapter" data-level="A.7" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-matrixHierachicalModel"><i class="fa fa-check"></i><b>A.7</b> The Matrix Formulation of Hierarchical Models (the Laird-Ware form)</a></li>
<li class="chapter" data-level="A.8" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-cTreatGM"><i class="fa fa-check"></i><b>A.8</b> Treatment contrast with intercept as the grand mean</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html"><i class="fa fa-check"></i><b>B</b> Advanced models with Stan - Extended</a>
<ul>
<li class="chapter" data-level="B.1" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-target"><i class="fa fa-check"></i><b>B.1</b> What does <code>target</code> do in Stan models?</a></li>
<li class="chapter" data-level="B.2" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-tilde"><i class="fa fa-check"></i><b>B.2</b> Explicitly incrementing the log probability function (<code>target</code>) vs. using the sampling or distribution <code>~</code> notation</a></li>
<li class="chapter" data-level="B.3" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-cmdstanr"><i class="fa fa-check"></i><b>B.3</b> An alternative R interface to Stan: <code>cmdstanr</code></a></li>
<li class="chapter" data-level="B.4" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-stancontainers"><i class="fa fa-check"></i><b>B.4</b> Matrix, vector, or array in Stan?</a></li>
<li class="chapter" data-level="B.5" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-noncenterparam"><i class="fa fa-check"></i><b>B.5</b> A simple non-centered parameterization</a></li>
<li class="chapter" data-level="B.6" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-cholesky"><i class="fa fa-check"></i><b>B.6</b> Cholesky factorization for reparameterizing hierarchical models with correlations between adjustments to different parameters</a></li>
<li class="chapter" data-level="B.7" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-sbc"><i class="fa fa-check"></i><b>B.7</b> Different rank visualizations and the <code>SBC</code> package.</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="evidence-synthesis-and-measurements-with-error---extended.html"><a href="evidence-synthesis-and-measurements-with-error---extended.html"><i class="fa fa-check"></i><b>C</b> Evidence synthesis and measurements with error - Extended</a>
<ul>
<li class="chapter" data-level="C.1" data-path="evidence-synthesis-and-measurements-with-error---extended.html"><a href="evidence-synthesis-and-measurements-with-error---extended.html#app-sigmatrue"><i class="fa fa-check"></i><b>C.1</b> What happens if we set <code>sigma = TRUE</code> in <code>resp_se()</code> function in <code>brms</code>?</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html"><i class="fa fa-check"></i><b>D</b> Model comparison - Extended</a>
<ul>
<li class="chapter" data-level="D.1" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-null"><i class="fa fa-check"></i><b>D.1</b> Credible intervals should not be used to reject a null hypothesis</a></li>
<li class="chapter" data-level="D.2" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-likR"><i class="fa fa-check"></i><b>D.2</b> The likelihood ratio vs the Bayes factor</a></li>
<li class="chapter" data-level="D.3" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-integral"><i class="fa fa-check"></i><b>D.3</b> Approximation of the (expected) log predictive density of a model without integration</a></li>
<li class="chapter" data-level="D.4" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-CV-alg"><i class="fa fa-check"></i><b>D.4</b> The cross-validation algorithm for the expected log predictive density of a model</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>E</b> The Art and Science of Prior Elicitation</a>
<ul>
<li class="chapter" data-level="E.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>E.1</b> Eliciting priors from oneself for a self-paced reading study: An example</a>
<ul>
<li class="chapter" data-level="E.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>E.1.1</b> An example: English  relative clauses</a></li>
<li class="chapter" data-level="E.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>E.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="E.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>E.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="E.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>E.1.4</b> Eliciting priors for the  variance components</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>E.2</b>  Eliciting priors from experts</a></li>
<li class="chapter" data-level="E.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>E.3</b> Deriving priors from  meta-analyses</a></li>
<li class="chapter" data-level="E.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>E.4</b> Using previous experiments’  posteriors as priors for a new study</a></li>
<li class="chapter" data-level="E.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-17"><i class="fa fa-check"></i><b>E.5</b> Summary</a></li>
<li class="chapter" data-level="E.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-16"><i class="fa fa-check"></i><b>E.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>F</b> Workflow</a>
<ul>
<li class="chapter" data-level="F.1" data-path="ch-workflow.html"><a href="ch-workflow.html#building-a-model"><i class="fa fa-check"></i><b>F.1</b>  Building a model</a></li>
<li class="chapter" data-level="F.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-to-ask-on-a-model"><i class="fa fa-check"></i><b>F.2</b> Principled questions to ask on a model</a>
<ul>
<li class="chapter" data-level="F.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#checking-whether-assumptions-are-consistent-with-domain-expertise-prior-predictive-checks"><i class="fa fa-check"></i><b>F.2.1</b>  Checking whether assumptions are consistent with  domain expertise: Prior predictive checks</a></li>
<li class="chapter" data-level="F.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#testing-for-correct-posterior-approximations-checks-of-computational-faithfulness"><i class="fa fa-check"></i><b>F.2.2</b>  Testing for correct posterior approximations: Checks of computational faithfulness</a></li>
<li class="chapter" data-level="F.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#sensitivity-of-the-model"><i class="fa fa-check"></i><b>F.2.3</b>  Sensitivity of the model</a></li>
<li class="chapter" data-level="F.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#does-the-model-adequately-capture-the-dataposterior-predictive-checks"><i class="fa fa-check"></i><b>F.2.4</b>  Does the model adequately capture the data?–Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="F.3" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-17"><i class="fa fa-check"></i><b>F.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>G</b> Exercises</a>
<ul>
<li class="chapter" data-level="G.1" data-path="exercises.html"><a href="exercises.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>G.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="G.1.1" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart1"><i class="fa fa-check"></i><b>G.1.1</b> Practice using the <code>pnorm()</code> function–Part 1</a></li>
<li class="chapter" data-level="G.1.2" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart2"><i class="fa fa-check"></i><b>G.1.2</b> Practice using the <code>pnorm()</code> function–Part 2</a></li>
<li class="chapter" data-level="G.1.3" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart3"><i class="fa fa-check"></i><b>G.1.3</b> Practice using the <code>pnorm()</code> function–Part 3</a></li>
<li class="chapter" data-level="G.1.4" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisesqnormPart1"><i class="fa fa-check"></i><b>G.1.4</b> Practice using the <code>qnorm()</code> function–Part 1</a></li>
<li class="chapter" data-level="G.1.5" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisesqnormPart2"><i class="fa fa-check"></i><b>G.1.5</b> Practice using the <code>qnorm()</code> function–Part 2</a></li>
<li class="chapter" data-level="G.1.6" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisessamples1"><i class="fa fa-check"></i><b>G.1.6</b> Practice getting summaries from samples–Part 1</a></li>
<li class="chapter" data-level="G.1.7" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisessamples2"><i class="fa fa-check"></i><b>G.1.7</b> Practice getting summaries from samples–Part 2.</a></li>
<li class="chapter" data-level="G.1.8" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisesvcov1"><i class="fa fa-check"></i><b>G.1.8</b> Practice with a variance-covariance matrix for a bivariate distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.2" data-path="exercises.html"><a href="exercises.html#sec-BDAexercises"><i class="fa fa-check"></i><b>G.2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="G.2.1" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesDerivingBayes"><i class="fa fa-check"></i><b>G.2.1</b> Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="G.2.2" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj1"><i class="fa fa-check"></i><b>G.2.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="G.2.3" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj2"><i class="fa fa-check"></i><b>G.2.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="G.2.4" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj3"><i class="fa fa-check"></i><b>G.2.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="G.2.5" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj4"><i class="fa fa-check"></i><b>G.2.5</b> Conjugate forms 4</a></li>
<li class="chapter" data-level="G.2.6" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesWeightedMean"><i class="fa fa-check"></i><b>G.2.6</b> The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)</a></li>
</ul></li>
<li class="chapter" data-level="G.3" data-path="exercises.html"><a href="exercises.html#ex:compbda"><i class="fa fa-check"></i><b>G.3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="G.3.1" data-path="exercises.html"><a href="exercises.html#exr:simulatedlinearmod"><i class="fa fa-check"></i><b>G.3.1</b> Check for parameter recovery in a linear model using simulated data.</a></li>
<li class="chapter" data-level="G.3.2" data-path="exercises.html"><a href="exercises.html#exr:linearmod"><i class="fa fa-check"></i><b>G.3.2</b> A simple linear model.</a></li>
<li class="chapter" data-level="G.3.3" data-path="exercises.html"><a href="exercises.html#exr:compbda-biasedpost"><i class="fa fa-check"></i><b>G.3.3</b> Revisiting the button-pressing example with different priors.</a></li>
<li class="chapter" data-level="G.3.4" data-path="exercises.html"><a href="exercises.html#exr:ppd"><i class="fa fa-check"></i><b>G.3.4</b> Posterior predictive checks with a log-normal model.</a></li>
<li class="chapter" data-level="G.3.5" data-path="exercises.html"><a href="exercises.html#exr:skew"><i class="fa fa-check"></i><b>G.3.5</b> A skew normal distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.4" data-path="exercises.html"><a href="exercises.html#sec-LMexercises"><i class="fa fa-check"></i><b>G.4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="G.4.1" data-path="exercises.html"><a href="exercises.html#exr:powerposing"><i class="fa fa-check"></i><b>G.4.1</b> A simple linear regression: Power posing and testosterone.</a></li>
<li class="chapter" data-level="G.4.2" data-path="exercises.html"><a href="exercises.html#exr:pupils"><i class="fa fa-check"></i><b>G.4.2</b> Another linear regression model: Revisiting attentional load effect on pupil size.</a></li>
<li class="chapter" data-level="G.4.3" data-path="exercises.html"><a href="exercises.html#exr:lognormalm"><i class="fa fa-check"></i><b>G.4.3</b> Log-normal model: Revisiting the effect of trial on finger tapping times.</a></li>
<li class="chapter" data-level="G.4.4" data-path="exercises.html"><a href="exercises.html#exr:reg-logistic"><i class="fa fa-check"></i><b>G.4.4</b> Logistic regression: Revisiting the effect of set size on free recall.</a></li>
<li class="chapter" data-level="G.4.5" data-path="exercises.html"><a href="exercises.html#exr:red"><i class="fa fa-check"></i><b>G.4.5</b> Red is the sexiest color.</a></li>
</ul></li>
<li class="chapter" data-level="G.5" data-path="exercises.html"><a href="exercises.html#sec-HLMexercises"><i class="fa fa-check"></i><b>G.5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="G.5.1" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-normal"><i class="fa fa-check"></i><b>G.5.1</b> A hierarchical model (normal likelihood) of cognitive load on pupil size.</a></li>
<li class="chapter" data-level="G.5.2" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-logn"><i class="fa fa-check"></i><b>G.5.2</b> Are subject relatives easier to process than object relatives (log-normal likelihood)?</a></li>
<li class="chapter" data-level="G.5.3" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseMandarinRC"><i class="fa fa-check"></i><b>G.5.3</b> Relative clause processing in Mandarin Chinese</a></li>
<li class="chapter" data-level="G.5.4" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseEnglishAgrmt"><i class="fa fa-check"></i><b>G.5.4</b>  Agreement attraction in comprehension</a></li>
<li class="chapter" data-level="G.5.5" data-path="exercises.html"><a href="exercises.html#exr:ab"><i class="fa fa-check"></i><b>G.5.5</b>  Attentional blink (Bernoulli likelihood)</a></li>
<li class="chapter" data-level="G.5.6" data-path="exercises.html"><a href="exercises.html#exr:strooplogis-brms"><i class="fa fa-check"></i><b>G.5.6</b> Is there a Stroop effect in accuracy?</a></li>
<li class="chapter" data-level="G.5.7" data-path="exercises.html"><a href="exercises.html#exr:stroop-dist"><i class="fa fa-check"></i><b>G.5.7</b>  Distributional regression for the Stroop effect.</a></li>
<li class="chapter" data-level="G.5.8" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseGramCE"><i class="fa fa-check"></i><b>G.5.8</b> The  grammaticality illusion</a></li>
</ul></li>
<li class="chapter" data-level="G.6" data-path="exercises.html"><a href="exercises.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>G.6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="G.6.1" data-path="exercises.html"><a href="exercises.html#exr:ContrastsPersian"><i class="fa fa-check"></i><b>G.6.1</b> Contrast coding for a four-condition design</a></li>
<li class="chapter" data-level="G.6.2" data-path="exercises.html"><a href="exercises.html#exr:ContrastsNPIHelmert"><i class="fa fa-check"></i><b>G.6.2</b>  Helmert coding for a six-condition design.</a></li>
<li class="chapter" data-level="G.6.3" data-path="exercises.html"><a href="exercises.html#exr:ContrastsNcomparisons"><i class="fa fa-check"></i><b>G.6.3</b> Number of possible comparisons in a single model.</a></li>
</ul></li>
<li class="chapter" data-level="G.7" data-path="exercises.html"><a href="exercises.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>G.7</b> Contrast coding with two predictor variables</a>
<ul>
<li class="chapter" data-level="G.7.1" data-path="exercises.html"><a href="exercises.html#exr:ContrastsPersianANOVA"><i class="fa fa-check"></i><b>G.7.1</b> ANOVA coding for a four-condition design.</a></li>
<li class="chapter" data-level="G.7.2" data-path="exercises.html"><a href="exercises.html#exr:Contrasts2x2x2Dillon2013"><i class="fa fa-check"></i><b>G.7.2</b> ANOVA and nested comparisons in a <span class="math inline">\(2\times 2\times 2\)</span> design</a></li>
</ul></li>
<li class="chapter" data-level="G.8" data-path="exercises.html"><a href="exercises.html#introduction-to-the-probabilistic-programming-language-stan"><i class="fa fa-check"></i><b>G.8</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="G.8.1" data-path="exercises.html"><a href="exercises.html#exr:first"><i class="fa fa-check"></i><b>G.8.1</b> A very simple model.</a></li>
<li class="chapter" data-level="G.8.2" data-path="exercises.html"><a href="exercises.html#exr:badstan"><i class="fa fa-check"></i><b>G.8.2</b> Incorrect Stan model.</a></li>
<li class="chapter" data-level="G.8.3" data-path="exercises.html"><a href="exercises.html#exr:skewstan"><i class="fa fa-check"></i><b>G.8.3</b> Using Stan documentation.</a></li>
<li class="chapter" data-level="G.8.4" data-path="exercises.html"><a href="exercises.html#exr:linkfunction"><i class="fa fa-check"></i><b>G.8.4</b> The probit link function as an alternative to the logit function.</a></li>
<li class="chapter" data-level="G.8.5" data-path="exercises.html"><a href="exercises.html#exr:logisticstan"><i class="fa fa-check"></i><b>G.8.5</b> Examining the position of the queued word on recall.</a></li>
<li class="chapter" data-level="G.8.6" data-path="exercises.html"><a href="exercises.html#exr:fallacy"><i class="fa fa-check"></i><b>G.8.6</b> The conjunction fallacy.</a></li>
</ul></li>
<li class="chapter" data-level="G.9" data-path="exercises.html"><a href="exercises.html#hierarchical-models-and-reparameterization"><i class="fa fa-check"></i><b>G.9</b> Hierarchical models and reparameterization</a>
<ul>
<li class="chapter" data-level="G.9.1" data-path="exercises.html"><a href="exercises.html#exr:stroop"><i class="fa fa-check"></i><b>G.9.1</b> A log-normal model in Stan.</a></li>
<li class="chapter" data-level="G.9.2" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-logn-stan"><i class="fa fa-check"></i><b>G.9.2</b> A by-subjects and by-items hierarchical model with a log-normal likelihood.</a></li>
<li class="chapter" data-level="G.9.3" data-path="exercises.html"><a href="exercises.html#exr:strooplogis"><i class="fa fa-check"></i><b>G.9.3</b> A hierarchical logistic regression with Stan.</a></li>
<li class="chapter" data-level="G.9.4" data-path="exercises.html"><a href="exercises.html#exr:distr-stan"><i class="fa fa-check"></i><b>G.9.4</b> A distributional regression model of the effect of cloze probability on the N400.</a></li>
</ul></li>
<li class="chapter" data-level="G.10" data-path="exercises.html"><a href="exercises.html#sec-customexercises"><i class="fa fa-check"></i><b>G.10</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="G.10.1" data-path="exercises.html"><a href="exercises.html#exr:shiftedlogn"><i class="fa fa-check"></i><b>G.10.1</b> Fitting a  shifted log-normal distribution.</a></li>
<li class="chapter" data-level="G.10.2" data-path="exercises.html"><a href="exercises.html#exr:wald"><i class="fa fa-check"></i><b>G.10.2</b> Fitting a Wald distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.11" data-path="exercises.html"><a href="exercises.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>G.11</b> Meta-analysis and measurement error models</a>
<ul>
<li class="chapter" data-level="G.11.1" data-path="exercises.html"><a href="exercises.html#exr:REMAMEExtracting"><i class="fa fa-check"></i><b>G.11.1</b> Extracting estimates from published papers</a></li>
<li class="chapter" data-level="G.11.2" data-path="exercises.html"><a href="exercises.html#exr:REMAMEBuerki"><i class="fa fa-check"></i><b>G.11.2</b> A meta-analysis of picture-word interference data</a></li>
<li class="chapter" data-level="G.11.3" data-path="exercises.html"><a href="exercises.html#exr:REMAMELiEnglish"><i class="fa fa-check"></i><b>G.11.3</b> Measurement error model for English VOT data</a></li>
</ul></li>
<li class="chapter" data-level="G.12" data-path="exercises.html"><a href="exercises.html#introduction-to-model-comparison"><i class="fa fa-check"></i><b>G.12</b> Introduction to model comparison</a></li>
<li class="chapter" data-level="G.13" data-path="exercises.html"><a href="exercises.html#bayes-factors"><i class="fa fa-check"></i><b>G.13</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="G.13.1" data-path="exercises.html"><a href="exercises.html#exr:bysubjects"><i class="fa fa-check"></i><b>G.13.1</b> Is there evidence for differences in the effect of cloze probability among the subjects?</a></li>
<li class="chapter" data-level="G.13.2" data-path="exercises.html"><a href="exercises.html#exr:bf-logn"><i class="fa fa-check"></i><b>G.13.2</b> Is there evidence for the claim that English subject relative clauses are easier to process than object relative clauses?</a></li>
<li class="chapter" data-level="G.13.3" data-path="exercises.html"><a href="exercises.html#exr:bf-logistic"><i class="fa fa-check"></i><b>G.13.3</b> In the Grodner and Gibson 2005 data, in question-response accuracies, is there evidence for the claim that sentences with subject relative clauses are easier to comprehend?</a></li>
<li class="chapter" data-level="G.13.4" data-path="exercises.html"><a href="exercises.html#exr:lognstan"><i class="fa fa-check"></i><b>G.13.4</b> Bayes factor and bounded parameters using Stan.</a></li>
</ul></li>
<li class="chapter" data-level="G.14" data-path="exercises.html"><a href="exercises.html#cross-validation"><i class="fa fa-check"></i><b>G.14</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="G.14.1" data-path="exercises.html"><a href="exercises.html#exr:logcv"><i class="fa fa-check"></i><b>G.14.1</b> Predictive accuracy of the linear and the logarithm effect of cloze probability.</a></li>
<li class="chapter" data-level="G.14.2" data-path="exercises.html"><a href="exercises.html#exr:stroopcv"><i class="fa fa-check"></i><b>G.14.2</b> Log-normal model</a></li>
<li class="chapter" data-level="G.14.3" data-path="exercises.html"><a href="exercises.html#exr:logrec"><i class="fa fa-check"></i><b>G.14.3</b> Log-normal vs rec-normal model in Stan</a></li>
</ul></li>
<li class="chapter" data-level="G.15" data-path="exercises.html"><a href="exercises.html#introduction-to-cognitive-modeling"><i class="fa fa-check"></i><b>G.15</b> Introduction to cognitive modeling</a></li>
<li class="chapter" data-level="G.16" data-path="exercises.html"><a href="exercises.html#multinomial-processing-trees"><i class="fa fa-check"></i><b>G.16</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="G.16.1" data-path="exercises.html"><a href="exercises.html#exr:mult"><i class="fa fa-check"></i><b>G.16.1</b> Modeling multiple categorical responses.</a></li>
<li class="chapter" data-level="G.16.2" data-path="exercises.html"><a href="exercises.html#exr:mpt-mnm"><i class="fa fa-check"></i><b>G.16.2</b> An alternative MPT to model the picture recognition task.</a></li>
<li class="chapter" data-level="G.16.3" data-path="exercises.html"><a href="exercises.html#exr:edit-mpt-cat"><i class="fa fa-check"></i><b>G.16.3</b> A simple MPT model that incorporates phonological complexity in the picture recognition task.</a></li>
<li class="chapter" data-level="G.16.4" data-path="exercises.html"><a href="exercises.html#exr:mpt"><i class="fa fa-check"></i><b>G.16.4</b> A more hierarchical MPT.</a></li>
<li class="chapter" data-level="G.16.5" data-path="exercises.html"><a href="exercises.html#exr:mpt-adv"><i class="fa fa-check"></i><b>G.16.5</b> <strong>Advanced</strong>: Multinomial processing trees.</a></li>
</ul></li>
<li class="chapter" data-level="G.17" data-path="exercises.html"><a href="exercises.html#mixture-models"><i class="fa fa-check"></i><b>G.17</b> Mixture models</a>
<ul>
<li class="chapter" data-level="G.17.1" data-path="exercises.html"><a href="exercises.html#exr:pcorrect"><i class="fa fa-check"></i><b>G.17.1</b> Changes in the true point values.</a></li>
<li class="chapter" data-level="G.17.2" data-path="exercises.html"><a href="exercises.html#exr:mixhier"><i class="fa fa-check"></i><b>G.17.2</b> RTs in schizophrenic patients and control.</a></li>
<li class="chapter" data-level="G.17.3" data-path="exercises.html"><a href="exercises.html#exr:mixbias"><i class="fa fa-check"></i><b>G.17.3</b> <strong>Advanced:</strong> Guessing bias in the model.</a></li>
</ul></li>
<li class="chapter" data-level="G.18" data-path="exercises.html"><a href="exercises.html#a-simple-accumulator-model-to-account-for-choice-response-time"><i class="fa fa-check"></i><b>G.18</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="G.18.1" data-path="exercises.html"><a href="exercises.html#exr:recovery"><i class="fa fa-check"></i><b>G.18.1</b> Can we recover the true point values of the parameters of a model when dealing with a contaminant distribution?</a></li>
<li class="chapter" data-level="G.18.2" data-path="exercises.html"><a href="exercises.html#exr:lnracescale"><i class="fa fa-check"></i><b>G.18.2</b> Can the log-normal race model account for fast errors?</a></li>
<li class="chapter" data-level="G.18.3" data-path="exercises.html"><a href="exercises.html#exr:lnldt"><i class="fa fa-check"></i><b>G.18.3</b> Accounting for response time and choice in the lexical decision task using the log-normal race model.</a></li>
</ul></li>
<li class="chapter" data-level="G.19" data-path="exercises.html"><a href="exercises.html#sec-priorsexercises"><i class="fa fa-check"></i><b>G.19</b> The Art and Science of  Prior Elicitation</a>
<ul>
<li class="chapter" data-level="G.19.1" data-path="exercises.html"><a href="exercises.html#exr:PriorsRCs"><i class="fa fa-check"></i><b>G.19.1</b> Develop a plausible informative prior for the difference between object and subject relative clause reading times</a></li>
<li class="chapter" data-level="G.19.2" data-path="exercises.html"><a href="exercises.html#exr:Priorslocalcoherence"><i class="fa fa-check"></i><b>G.19.2</b> Extracting an informative prior from a published paper for a future study</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-introstan" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Introduction to the probabilistic programming language Stan<a href="ch-introstan.html#ch-introstan" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Stan is a  probabilistic programming language for statistical inference written in C++ that can be accessed through several interfaces (e.g., R, Python, etc.). Stan uses an advanced dynamic  Hamiltonian Monte Carlo algorithm <span class="citation">(Betancourt <a href="#ref-betancourt2016identifying" role="doc-biblioref">2016</a>)</span> based on a variant of the  No-U-Turn sampler <span class="citation">(known as  NUTS: Hoffman and Gelman <a href="#ref-hoffmanNoUTurnSamplerAdaptively2014" role="doc-biblioref">2014</a>)</span>, which is, in general, more efficient than the traditional  Gibbs sampler used in other probabilistic languages such as  (Win)BUGS <span class="citation">(Lunn et al. <a href="#ref-lunn2000winbugs" role="doc-biblioref">2000</a>)</span> and  JAGS <span class="citation">(Plummer <a href="#ref-plummer2016jags" role="doc-biblioref">2016</a>)</span>. In this part of the book, we will focus on the package  <code>rstan</code> <span class="citation">(Guo et al. <a href="#ref-R-rstan" role="doc-biblioref">2024</a>)</span> that integrates Stan <span class="citation">(Carpenter et al. <a href="#ref-carpenter2017stan" role="doc-biblioref">2017</a>)</span> with R <span class="citation">(R Core Team <a href="#ref-R-base" role="doc-biblioref">2023</a>)</span>.</p>
<p>In order to understand how to fit a model in Stan and the difficulties one might face, a minimal understanding of the Stan  sampling algorithm is needed. Our descriptions of the technical details of HMC are oversimplifications that suffice for the purposes of this book. Stan takes advantage of the fact that the  shape of the posterior distribution is completely determined by the  priors and the  likelihood; in other words, the shape of the posterior is determined by the analytical form of the  unnormalized posterior, which is the upper part of the Bayes rule (abbreviated below as <span class="math inline">\(q(\Theta | y)\)</span>). This is because the denominator, or  marginal likelihood, “only” constitutes the  normalizing constant:</p>
<p><span class="math display" id="eq:bayesagain">\[\begin{equation}
p(\Theta|y) = \cfrac{ p(y|\Theta) \cdot p(\Theta) }{p(y)}
\tag{8.1}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:bayesagain2">\[\begin{equation}
q(\Theta|y) = p(y|\Theta) \cdot p(\Theta)
\tag{8.2}
\end{equation}\]</span></p>
<p>Thus, the unnormalized posterior is proportional (<span class="math inline">\(\propto\)</span>) to the posterior distribution:</p>
<p><span class="math display" id="eq:bayesagain3">\[\begin{equation}
q(\Theta|y) \propto p(\Theta|y)
\tag{8.3}
\end{equation}\]</span></p>
<p>The Stan sampler uses  Hamiltonian dynamics and treats the vector of parameters, <span class="math inline">\(\Theta\)</span> (that could range from a vector containing a couple of parameters, e.g., <span class="math inline">\(\langle \mu,\sigma \rangle\)</span>, to a vector of hundreds of parameters in hierarchical models), as the position of a  frictionless particle that glides on the  <em>negative logarithm of the unnormalized posterior</em>. That means that high probability places are valleys and low probability places are peaks in this space.<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a> Whenever the particle comes to a halt, that location constitutes a sample from the distribution.</p>
<p>However, Stan doesn’t just let the particle glide until it reaches the bottom of this space. If we let that happen, the particle would stop at the mode of the posterior distribution, and one would not be able to obtain samples from the rest of the distribution. Stan uses a complex algorithm to determine the weight of the particle and the  momentum applied to it, as well as when to stop the particle trajectory to take a sample. Because we need to know the velocity of this particle, Stan needs to be able to calculate the derivative of the log unnormalized posterior with respect to the parameters (recall that velocity is the first derivative with respect to the position). This means that if the parameter space is differentiable and relatively smooth (i.e., does not have any big break or sharp angle), and if the tuning parameters of the Stan algorithm are well adjusted–as should happen in the warm-up period–these samples are going to represent samples of the true  posterior distribution. Bear in mind that the  geometry of the posterior has a big influence on whether the algorithm will converge (quickly) or not: If the space is very flat, because there isn’t much data and the priors are not informative, then the particle may need to glide for a long time before it gets to a high probability area, that is a valley; if there are several valleys  (multimodality) the particle may never leave the vicinity of one of them; and if the space is  funnel shaped, the particle may never explore the funnel. One of the reasons for the difficulties in exploring complicated spaces is that the continuous path of the “particle” is discretized and divided into steps, and the  <em>step size</em> is optimized for the entire posterior space. In spaces that are too complex, such as a funnel, a step size might be too small to explore the wide part of the funnel, but too large to explore the narrow part; we will deal with this problem in section <a href="ch-complexstan.html#sec-uncorrstan">9.1.2</a>.</p>
<p>Although our following example assumes a vector of two parameters and thus a simple geometry, real world examples can easily have hundreds of parameters defining an unnormalized posterior space with hundreds of dimensions.</p>
<p>One question that might arise here is the following: Given that we already know the shape of the posterior, why do we need samples? After all, the posterior is just the unnormalized posterior multiplied by some number, the normalizing constant.</p>
<p>To make this discussion concrete, let’s say that we have a subject that participates in a memory test, and in each trial we get a noisy score from their true  working memory score. We assume that at each trial, the score is elicited with normally distributed noise. If we want to estimate the score and how much the noise makes it vary from trial to trial, we are assuming a normal likelihood and we want to estimate its mean and standard deviation.</p>
<p>We will use simulate data produced by a normal distribution with a true mean of <span class="math inline">\(3\)</span> and a true standard deviation of <span class="math inline">\(10\)</span> (the units are arbitrary):</p>
<div class="sourceCode" id="cb518"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb518-1"><a href="ch-introstan.html#cb518-1" aria-hidden="true"></a>y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">3</span>, <span class="dt">sd =</span> <span class="dv">10</span>)</span>
<span id="cb518-2"><a href="ch-introstan.html#cb518-2" aria-hidden="true"></a><span class="kw">head</span>(y)</span></code></pre></div>
<pre><code>## [1]  -4.41 -10.92  21.39   4.23   5.53   4.65</code></pre>
<p>As always, given our prior knowledge, we decide on priors. In this case, we use a log-normal prior for the standard deviation, <span class="math inline">\(\sigma\)</span>, since it can only be positive, but except for that, the prior distributions are quite arbitrary in this example.</p>
<p><span class="math display" id="eq:priorsdem">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim \mathit{Normal}(0, 20)\\
\sigma &amp;\sim \mathit{LogNormal}(3, 1)
\end{aligned}
\tag{8.4}
\end{equation}\]</span></p>
<p>The unnormalized posterior will be the product of the  likelihood of each data point times the  prior for each parameter:</p>
<p><span class="math display" id="eq:up">\[\begin{equation}
q(\mu, \sigma |y) = \prod_n^{100} \mathit{Normal}(y_n|\mu, \sigma) \cdot \mathit{Normal}(\mu | 0, 20) \cdot \mathit{LogNormal}(\sigma | 3, 1)
\tag{8.5}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(y = {-4.411, -10.918, \ldots}\)</span></p>
<p>We can also define the unnormalized posterior, <span class="math inline">\(q(\cdot)\)</span>, as a function in R:</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb520-1"><a href="ch-introstan.html#cb520-1" aria-hidden="true"></a>q &lt;-<span class="st"> </span><span class="cf">function</span>(mu, sigma, y) {</span>
<span id="cb520-2"><a href="ch-introstan.html#cb520-2" aria-hidden="true"></a>  <span class="kw">dnorm</span>(<span class="dt">x =</span> mu, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">20</span>) <span class="op">*</span></span>
<span id="cb520-3"><a href="ch-introstan.html#cb520-3" aria-hidden="true"></a><span class="st">    </span><span class="kw">dlnorm</span>(<span class="dt">x =</span> sigma, <span class="dt">mean =</span> <span class="dv">3</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="op">*</span></span>
<span id="cb520-4"><a href="ch-introstan.html#cb520-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">prod</span>(<span class="kw">dnorm</span>(<span class="dt">x =</span> y, <span class="dt">mean =</span> mu, <span class="dt">sd =</span> sigma))</span>
<span id="cb520-5"><a href="ch-introstan.html#cb520-5" aria-hidden="true"></a>}</span></code></pre></div>
<p>For example, if we want to know the unnormalized posterior density for the vector of parameters <span class="math inline">\(\langle \mu,\sigma \rangle = \langle 0, 5 \rangle\)</span>, we do the following:</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb521-1"><a href="ch-introstan.html#cb521-1" aria-hidden="true"></a><span class="kw">q</span>(<span class="dt">mu =</span> <span class="dv">0</span>, <span class="dt">sigma =</span> <span class="dv">5</span>, <span class="dt">y =</span> y)</span></code></pre></div>
<pre><code>## [1] 1.07e-217</code></pre>
<p>The shape of the unnormalized posterior density is completely defined and it will look like Figure <a href="ch-introstan.html#fig:up">8.1</a>.</p>

<div class="figure"><span style="display:block;" id="fig:up"></span>
<img src="bayescogsci_files/figure-html/up-1.svg" alt="The unnormalized posterior defined by Equation (8.5)." width="672" />
<p class="caption">
FIGURE 8.1: The unnormalized posterior defined by Equation <a href="ch-introstan.html#eq:up">(8.5)</a>.
</p>
</div>
<p>Why is the shape of the unnormalized posterior density not too useful for us? The main reason is that unless we already know which probability distribution we are dealing with (e.g., normal, Bernoulli, etc.) or we can easily  integrate it (which can only be done in simpler cases), we cannot do much with the analytical form of the unnormalized posterior: We cannot calculate credible intervals, or know how likely it is that the true score is above or below zero, and even the mean of the posterior is impossible to calculate.
This is because the unnormalized posterior distribution represents the shape of the posterior distribution. With just the shape of an unknown distribution, we can only answer the following question: What is the most (or least) likely value of the vector of parameters? We can answer this question by searching for the highest (or lowest) place in that shape. This leads us to the  maximum a posteriori  (MAP) estimate (i.e., the highest point in Figure <a href="ch-introstan.html#fig:up">8.1</a>, or the lowest point of the negative log unnormalized log posterior), which is in a sense the “Bayesian counterpart” of the penalized maximum likelihood estimate (MLE); see <span class="citation">Green (<a href="#ref-green1998penalized" role="doc-biblioref">1998</a>)</span> and <span class="citation">Scheipl, Kneib, and Fahrmeir (<a href="#ref-ScheiplEtAl2013PenalizedlikelihoodBayesian" role="doc-biblioref">2013</a>)</span> for a discussion of penalized likelihood and its connection to Bayesian inference.
However, the MAP is not truly Bayesian since it doesn’t take into account the uncertainty of the posterior. Even calculating the mode is not always trivial. In simple cases such as this one, one can calculate it analytically; but in more complex cases, relatively complicated algorithms are needed. (If we can recognize the shape as a distribution, we are in a different situation. In that case, we might know already the formulas for the expectation, variance, etc. This is what we did in chapter <a href="ch-introBDA.html#ch-introBDA">2</a>, but this is an unusual situation in realistic analyses.)
As we mentioned before, if we want to get  posterior density values, we need the denominator of the Bayes rule (or  marginal likelihood), <span class="math inline">\(p(y)\)</span>, which requires integrating the unnormalized posterior. Even this is not too useful if we want to communicate findings: almost every summary statistic requires us to solve more integrals, and except for a handful of cases, these integrals might not have an analytical solution.</p>
<p>If we want to be able to calculate  summary statistics of the posterior distribution (mean, quantiles, etc.), we are going to need  samples from this distribution. This is because with enough samples of a probability distribution, we can achieve very good approximations of summary statistics.</p>
<p>Stan will take care of returning samples from the posterior distribution, if the log unnormalized posterior distribution is differentiable and can be expressed as follows:<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a></p>
<p><span class="math display" id="eq:logup">\[\begin{equation}
\log(q(\Theta|y)) = \sum_n \log(p(y_n|\Theta)) + \sum_{par} \log(p(\Theta_{par}))
\tag{8.6}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(n\)</span> indicates each data point and <span class="math inline">\(par\)</span> each parameter. In our case, this corresponds to the following:</p>
<p><span class="math display" id="eq:up-applied">\[\begin{equation}
\begin{aligned}
\log(q(\mu, \sigma |y)) =&amp; \sum_n^{100} \log(Normal(y_n|\mu, \sigma)) + \log(Normal(\mu | 0, 20)) \\
&amp;+ \log(LogNormal(\sigma | 3, 1))
\end{aligned}
\tag{8.7}
\end{equation}\]</span></p>
<p>In the following sections, we’ll see how we can implement this model and many others in Stan.</p>
<div id="stan-syntax" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Stan syntax<a href="ch-introstan.html#stan-syntax" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A Stan program is usually saved as a <code>.stan</code> file and accessed through R (or other interfaces) and it is organized into a sequence of optional and obligatory  blocks, which must be written in order. The Stan language is different from R and it is loosely based on C++; one important aspect to pay attention to is that every statement ends in a semi-colon, <code>;</code>. Blocks (<code>{}</code>) do not end in semi-colons. Some functions in Stan are written in the same way as in R (e.g., <code>mean</code>, <code>sum</code>, <code>max</code>, <code>min</code>). But some are different; when in doubt, <a href="https://mc-stan.org/users/documentation/">Stan documentation</a> can be extremely helpful. In addition, the package  <code>rstan</code> provides the function  <code>lookup()</code> to look up for translations of functions. For example, in section <a href="ch-reg.html#sec-logistic">4.3</a>, we saw that the R function <code>plogis()</code> is needed to convert from log-odds to probability space. If we need it in a Stan program, we can look for it in the following way:</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb523-1"><a href="ch-introstan.html#cb523-1" aria-hidden="true"></a><span class="kw">lookup</span>(plogis)</span></code></pre></div>
<pre><code>##          StanFunction                                Arguments
## 262         inv_logit                                (T);(int)
## 307     log_inv_logit                                (T);(int)
## 323 logistic_ccdf_log (real, real, T);(vector, vector, vector)
## 324      logistic_cdf (real, real, T);(vector, vector, vector)
## 325  logistic_cdf_log (real, real, T);(vector, vector, vector)
## 326    logistic_lccdf (real, real, T);(vector, vector, vector)
## 327     logistic_lcdf (real, real, T);(vector, vector, vector)
##     ReturnType
## 262     T;real
## 307     T;real
## 323     T;real
## 324     T;real
## 325     T;real
## 326     T;real
## 327     T;real</code></pre>
<p>There are three columns in the output of this call. The first one indicates Stan function names, the second one their arguments with their type, and the third one the type they return. Unlike R, Stan is strict with the type of the variables.<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a> In order to decide which function to use, it is necessary to look at the Stan documentation and find the function that matches our specific needs (for <code>plogis</code>, the corresponding function would be <code>inv_logit()</code>).</p>
<p>Another important difference from R is that every variable needs to be declared with its type (real, integer, vector, matrix, etc.). The next two sections exemplify these details through basic Stan programs.</p>
</div>
<div id="sec-firststan" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> A first simple example with Stan:  Normal likelihood<a href="ch-introstan.html#sec-firststan" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s fit a Stan model to estimate the simple example given at the introduction of this chapter, where we simulate data in R from a normal distribution with a true mean of <code>3</code> and a true standard deviation of <code>10</code>:</p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb525-1"><a href="ch-introstan.html#cb525-1" aria-hidden="true"></a>y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">3</span>, <span class="dt">sd =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>As mentioned earlier,  Stan code is organized in  blocks. The first block indicates what constitutes data for the model:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;  // Total number of trials
  vector[N] y;  // Score in each trial
}</code></pre>
<p>The variable of type  <code>int</code> (integer) represents the number of trials. In addition to the type, some constraints can be indicated with  <code>lower</code> and  <code>upper</code>. In this case, <code>N</code> can’t be smaller than <code>1</code>. These constraints serve as a sanity check; if they are not satisfied, we get an error and the model won’t run. The data are stored in a vector of length <code>N</code>, unlike R, vectors (and matrices and arrays) need to be defined with their dimensions. Comments are indicated with <code>//</code> rather than <code>#</code>.</p>
<p>The next block indicates the parameters of the model:</p>
<pre class="stan fold-show"><code>parameters {
  real mu;
  real&lt;lower = 0&gt; sigma;
}</code></pre>
<p>The two parameters are real numbers, and <code>sigma</code> is constrained to be positive.</p>
<p>Finally, we indicate the prior distributions and likelihood functions in the  model block:
</p>
<pre class="stan fold-show"><code>model {
  // Priors:
  target += normal_lpdf(mu | 0, 20);
  target += lognormal_lpdf(sigma | 3, 1);
  // Likelihood:
  for(i in 1:N)
    target += normal_lpdf(y[i] | mu, sigma);
}</code></pre>
<p>The variable <code>target</code> is a reserved word in Stan; every statement with <code>target +=</code> adds terms to the  unnormalized <em>log</em> posterior density. We do this because adding to the unnormalized log posterior amounts to multiplying a term in the numerator of the unnormalized posterior. As explained earlier, Stan uses the shape of the unnormalized posterior to sample from the actual posterior distribution. See online section <a href="advanced-models-with-stan---extended.html#app-target">B.1</a> for a more detailed explanation, and see online section <a href="advanced-models-with-stan---extended.html#app-tilde">B.2</a> for alternative notations.</p>
<p>We didn’t use curly brackets with the for-loop; this is a common practice if the for-loop has only one line, but brackets can be added and are obligatory if the for-loop spans several lines.</p>
<p>It’s also possible to avoid the  for-loop since many functions are vectorized in Stan:
</p>
<pre class="stan fold-show"><code>model {
  // Priors:
  target += normal_lpdf(mu | 0, 20);
  target += lognormal_lpdf(sigma | 3, 1);
  // Likelihood:
  target += normal_lpdf(y | mu, sigma);
}</code></pre>
<p>The for-loop and vectorized versions give us the same output: The for-loop version evaluated the log-likelihood at each value of <code>y</code> and added it to <code>target</code>. The vectorized version does not create a vector of log-likelihoods; instead, it sums up the log-likelihood evaluated at each element of <code>y</code> and then it adds that to <code>target</code>.</p>
<p>The complete model looks like this:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;  // Total number of trials
  vector[N] y;  // Score in each trial
}
parameters {
  real mu;
  real&lt;lower = 0&gt; sigma;
}
model {
  // Priors:
  target += normal_lpdf(mu | 0, 20);
  target += lognormal_lpdf(sigma | 3, 1);
  // Likelihood:
  target += normal_lpdf(y | mu, sigma);
}</code></pre>
<p>You can save the above code as <code>normal.stan</code>. Alternatively, you can use the version stored in the package <code>bcogsci</code>. (Typing <code>?stan-normal</code> in the R console provides some documentation for the model.) You can access the code of the models of this book by using  <code>system.file("stan_models", "name_of_the_model.stan", package = "bcogsci")</code>.</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb531-1"><a href="ch-introstan.html#cb531-1" aria-hidden="true"></a>normal &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb531-2"><a href="ch-introstan.html#cb531-2" aria-hidden="true"></a>                      <span class="st">&quot;normal.stan&quot;</span>,</span>
<span id="cb531-3"><a href="ch-introstan.html#cb531-3" aria-hidden="true"></a>                      <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span></code></pre></div>
<p>This command just points to a text file that the package <code>bcogsci</code> stores on your computer. You can open it to read the code (with any text editor, or <code>readLines()</code> in R). You’ll need to compile this code and run it with <code>stan()</code>.</p>
<p>Stan requires the data to be in a list object in R. Below, we fit the model with the default number of  chains and  iterations.</p>
<div class="sourceCode" id="cb532"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb532-1"><a href="ch-introstan.html#cb532-1" aria-hidden="true"></a>y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">3</span>, <span class="dt">sd =</span> <span class="dv">10</span>)</span>
<span id="cb532-2"><a href="ch-introstan.html#cb532-2" aria-hidden="true"></a>lst_score_data &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">y =</span> y, <span class="dt">N =</span> <span class="kw">length</span>(y))</span>
<span id="cb532-3"><a href="ch-introstan.html#cb532-3" aria-hidden="true"></a><span class="co"># Fit the model with the default values of number of</span></span>
<span id="cb532-4"><a href="ch-introstan.html#cb532-4" aria-hidden="true"></a><span class="co"># chains and iterations: chains = 4, iter = 2000</span></span>
<span id="cb532-5"><a href="ch-introstan.html#cb532-5" aria-hidden="true"></a>fit_score &lt;-<span class="st"> </span><span class="kw">stan</span>(normal, <span class="dt">data =</span> lst_score_data)</span>
<span id="cb532-6"><a href="ch-introstan.html#cb532-6" aria-hidden="true"></a><span class="co"># alternatively:</span></span>
<span id="cb532-7"><a href="ch-introstan.html#cb532-7" aria-hidden="true"></a><span class="co"># stan(&quot;normal.stan&quot;, data = lst_score_data)</span></span></code></pre></div>
<p>Inspect how well the chains mixed in Figure <a href="ch-introstan.html#fig:traceplotmusigma">8.2</a>. The chains for each parameter should look like a  “fat hairy caterpillar” <span class="citation">(Lunn et al. <a href="#ref-lunn2012bugs" role="doc-biblioref">2012</a>)</span>; see section <a href="ch-compbda.html#sec-convergencenut">3.2.1.2</a> for a brief discussion about  convergence.</p>

<div class="sourceCode" id="cb533"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb533-1"><a href="ch-introstan.html#cb533-1" aria-hidden="true"></a><span class="kw">traceplot</span>(fit_score, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;mu&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:traceplotmusigma"></span>
<img src="bayescogsci_files/figure-html/traceplotmusigma-1.svg" alt="Traceplots of mu and sigma from the model fit_score." width="672" />
<p class="caption">
FIGURE 8.2: Traceplots of <code>mu</code> and <code>sigma</code> from the model <code>fit_score</code>.
</p>
</div>
<p>We can see a summary of the posterior by either printing out the model fit, or by plotting it. The summary displayed by the function  <code>print</code> includes means, standard deviations (<code>sd</code>), quantiles,  Monte Carlo standard errors for the mean of the posterior (<code>se_mean</code>), split Rhats, and effective sample sizes (<code>n_eff</code>). The summaries are computed after removing the warmup and merging together all chains. The  <code>se_mean</code> is unrelated to the <code>se</code> of an estimate in the parallel frequentist model. Similarly to a large effective sample size, small Monte Carlo standard errors indicate a successful sampling procedure: with a large value of  <code>n_eff</code> and a small value for <code>se_mean</code> we can be relatively sure of the reliability of the mean of the posterior. However, what constitutes a large or small <code>se_mean</code> is harder to define <span class="citation">(see Vehtari et al. <a href="#ref-vehtari2019ranknormalization" role="doc-biblioref">2021</a> for a more extensive discussion)</span>.<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a></p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb534-1"><a href="ch-introstan.html#cb534-1" aria-hidden="true"></a><span class="kw">print</span>(fit_score, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;mu&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</span></code></pre></div>
<pre><code>## Inference for Stan model: anon_model.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##       mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat
## mu    3.91    0.02 0.92 2.06 3.29 3.91 4.53   5.7  3707    1
## sigma 9.23    0.01 0.66 8.07 8.75 9.19 9.65  10.7  4026    1
## 
## Samples were drawn using NUTS(diag_e) at Mon Jan 13 08:27:33 2025.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>After transforming the <code>stanfit</code> object into a data frame, it’s possible to provide summary plots as the one shown in Figure <a href="ch-introstan.html#fig:mcmchist">8.3</a>. The package  <code>bayesplot</code> <span class="citation">(Gabry and Mahr <a href="#ref-R-bayesplot" role="doc-biblioref">2024</a>)</span> is a wrapper around <code>ggplot2</code> <span class="citation">(Wickham, Chang, et al. <a href="#ref-R-ggplot2" role="doc-biblioref">2024</a>)</span> and has several convenient functions to plot the samples. Bayesplot functions for posterior summaries start with <code>mcmc_</code>:</p>

<div class="sourceCode" id="cb536"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb536-1"><a href="ch-introstan.html#cb536-1" aria-hidden="true"></a>df_fit_score &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_score)</span>
<span id="cb536-2"><a href="ch-introstan.html#cb536-2" aria-hidden="true"></a><span class="kw">mcmc_hist</span>(df_fit_score, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;mu&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:mcmchist"></span>
<img src="bayescogsci_files/figure-html/mcmchist-1.svg" alt="Histograms of the samples of the posterior distributions of mu and sigma from the model fit_score." width="672" />
<p class="caption">
FIGURE 8.3: Histograms of the samples of the posterior distributions of <code>mu</code> and <code>sigma</code> from the model <code>fit_score</code>.
</p>
</div>
<p>There are also several ways to get the samples for other summaries or customized plots, depending on whether we want a list, a data frame, or an array.<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a></p>
<div class="sourceCode" id="cb537"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb537-1"><a href="ch-introstan.html#cb537-1" aria-hidden="true"></a><span class="co"># The function extract from rstan is sometimes overwritten by</span></span>
<span id="cb537-2"><a href="ch-introstan.html#cb537-2" aria-hidden="true"></a><span class="co"># a tidyverse version with the same name, so make sure that </span></span>
<span id="cb537-3"><a href="ch-introstan.html#cb537-3" aria-hidden="true"></a><span class="co"># you are using the right function:</span></span>
<span id="cb537-4"><a href="ch-introstan.html#cb537-4" aria-hidden="true"></a>rstan<span class="op">::</span><span class="kw">extract</span>(fit_score) <span class="op">%&gt;%</span></span>
<span id="cb537-5"><a href="ch-introstan.html#cb537-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">str</span>()</span></code></pre></div>
<pre><code>## List of 3
##  $ mu   : num [1:4000(1d)] 3.89 5.04 3.8 3.91 5.55 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 1
##   .. ..$ iterations: NULL
##  $ sigma: num [1:4000(1d)] 8.01 9.37 8.47 8.39 9.66 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 1
##   .. ..$ iterations: NULL
##  $ lp__ : num [1:4000(1d)] -370 -369 -368 -368 -369 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 1
##   .. ..$ iterations: NULL</code></pre>
<div class="sourceCode" id="cb539"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb539-1"><a href="ch-introstan.html#cb539-1" aria-hidden="true"></a><span class="kw">as.data.frame</span>(fit_score) <span class="op">%&gt;%</span></span>
<span id="cb539-2"><a href="ch-introstan.html#cb539-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">str</span>(<span class="dt">list.len =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    4000 obs. of  3 variables:
##  $ mu   : num  4.92 2.6 4.91 3.07 4.02 ...
##  $ sigma: num  10.04 8.51 9.97 9.39 9.04 ...
##  $ lp__ : num  -369 -369 -369 -368 -368 ...</code></pre>
<div class="sourceCode" id="cb541"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb541-1"><a href="ch-introstan.html#cb541-1" aria-hidden="true"></a><span class="kw">as.array</span>(fit_score) <span class="op">%&gt;%</span></span>
<span id="cb541-2"><a href="ch-introstan.html#cb541-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">str</span>()</span></code></pre></div>
<pre><code>##  num [1:1000, 1:4, 1:3] 4.92 2.6 4.91 3.07 4.02 ...
##  - attr(*, &quot;dimnames&quot;)=List of 3
##   ..$ iterations: NULL
##   ..$ chains    : chr [1:4] &quot;chain:1&quot; &quot;chain:2&quot; &quot;chain:3&quot; &quot;chain:4&quot;
##   ..$ parameters: chr [1:3] &quot;mu&quot; &quot;sigma&quot; &quot;lp__&quot;</code></pre>
</div>
<div id="sec-clozestan" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Another simple example:  Cloze probability with Stan with the  binomial likelihood<a href="ch-introstan.html#sec-clozestan" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s fit a Stan model (<code>binomial_cloze.stan</code>) to estimate the cloze probability of a word given its context: that is, what is the probability of an upcoming word given its previous context; the model that was detailed in section <a href="ch-introBDA.html#sec-analytical">2.2</a> and was fit in section <a href="ch-compbda.html#sec-sampling">3.1</a>. We want to estimate <span class="math inline">\(\theta\)</span>, the cloze probability of producing “<em>umbrella</em>,” given the following data: “<em>umbrella</em>” was produced in <span class="math inline">\(80\)</span> out of <span class="math inline">\(100\)</span> trials. We assume a binomial distribution as the likelihood function, and <span class="math inline">\(\mathit{Beta}(a=4,b=4)\)</span> as a prior distribution for the cloze probability.</p>
<pre class="stan fold-show"><code>data {
  // Total number of answers
  int&lt;lower = 1&gt; N;
  // Number of times umbrella was answered:
  int&lt;lower = 0, upper = N&gt; k;
}
parameters {
  // theta is a probability, must be constrained between 0 and 1
  real&lt;lower = 0, upper = 1&gt; theta;
}
model {
  // Prior on theta:
  target += beta_lpdf(theta | 4, 4);
  // Likelihood:
  target += binomial_lpmf(k | N, theta);
}
</code></pre>
<p>There is only one parameter in this model, cloze probability represented with the parameter <code>theta</code>, which is a real number constrained between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Another difference between this and the previous example is that the likelihood function ends with  <code>_lpmf</code> rather than with  <code>_lpdf</code>. This is because Stan differentiates between distributions of continuous variables, i.e,  probability density functions  (PDFs), and distributions of discrete variables, i.e.,  probability mass functions  (PMFs).</p>
<div class="sourceCode" id="cb544"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb544-1"><a href="ch-introstan.html#cb544-1" aria-hidden="true"></a>lst_cloze_data &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">k =</span> <span class="dv">80</span>, <span class="dt">N =</span> <span class="dv">100</span>)</span>
<span id="cb544-2"><a href="ch-introstan.html#cb544-2" aria-hidden="true"></a>binomial_cloze &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb544-3"><a href="ch-introstan.html#cb544-3" aria-hidden="true"></a>                              <span class="st">&quot;binomial_cloze.stan&quot;</span>,</span>
<span id="cb544-4"><a href="ch-introstan.html#cb544-4" aria-hidden="true"></a>                              <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb544-5"><a href="ch-introstan.html#cb544-5" aria-hidden="true"></a>fit_cloze &lt;-<span class="st"> </span><span class="kw">stan</span>(binomial_cloze, <span class="dt">data =</span> lst_cloze_data)</span></code></pre></div>
<p>Print the summary of the posterior distribution of <span class="math inline">\(\theta\)</span> below, and show its posterior distribution graphically (see Figure <a href="ch-introstan.html#fig:posttheta">8.4</a>):</p>

<div class="sourceCode" id="cb545"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb545-1"><a href="ch-introstan.html#cb545-1" aria-hidden="true"></a><span class="kw">print</span>(fit_cloze, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;theta&quot;</span>))</span></code></pre></div>
<pre><code>##       mean 2.5% 97.5% n_eff Rhat
## theta 0.78  0.7  0.85  1371    1</code></pre>
<div class="sourceCode" id="cb547"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb547-1"><a href="ch-introstan.html#cb547-1" aria-hidden="true"></a>df_fit_cloze &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_cloze)</span>
<span id="cb547-2"><a href="ch-introstan.html#cb547-2" aria-hidden="true"></a><span class="kw">mcmc_dens</span>(df_fit_cloze, <span class="dt">pars =</span> <span class="st">&quot;theta&quot;</span>) <span class="op">+</span></span>
<span id="cb547-3"><a href="ch-introstan.html#cb547-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(df_fit_cloze<span class="op">$</span>theta))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:posttheta"></span>
<img src="bayescogsci_files/figure-html/posttheta-1.svg" alt="The posterior distribution of the cloze probability of umbrella (the parameter \(\theta\))." width="672" />
<p class="caption">
FIGURE 8.4: The posterior distribution of the cloze probability of umbrella (the parameter <span class="math inline">\(\theta\)</span>).
</p>
</div>
</div>
<div id="regression-models-in-stan" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span>  Regression models in Stan<a href="ch-introstan.html#regression-models-in-stan" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the following sections, we will revisit and expand on some of the examples that were fit with <code>brms</code> in chapter <a href="ch-reg.html#ch-reg">4</a>.</p>
<div id="sec-pupilstan" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> A first  linear regression in Stan: Does attentional load affect  pupil size?<a href="ch-introstan.html#sec-pupilstan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As in section <a href="ch-reg.html#sec-pupil">4.1</a>, we focus on the effect of cognitive load on one subject’s pupil size with a subset of the data from <span class="citation">Wahn et al. (<a href="#ref-wahnPupilSizesScale2016" role="doc-biblioref">2016</a>)</span>. We use the following likelihood and priors. For details about our decision on priors and likelihood, see section <a href="ch-reg.html#sec-pupil">4.1</a>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p\_size_n &amp;\sim \mathit{Normal}(\alpha + c\_load_n \cdot \beta,\sigma) \\
\alpha &amp;\sim \mathit{Normal}(1000, 500) \\
\beta &amp;\sim \mathit{Normal}(0, 100) \\
\sigma &amp;\sim \mathit{Normal}_+(0, 1000)
\end{aligned}
\end{equation}\]</span></p>
<p>The Stan model <code>pupil.stan</code> is as follows:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] p_size;
  vector[N] c_load;
}
parameters {
  real alpha;
  real beta;
  real&lt;lower = 0&gt; sigma;
}
model {
  // priors:
  target += normal_lpdf(alpha | 1000, 500);
  target += normal_lpdf(beta | 0, 100);
  target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);
  // likelihood
  target += normal_lpdf(p_size | alpha + c_load * beta, sigma);
}</code></pre>
<p>Because we are fitting a regression, we use the  location (<span class="math inline">\(\mu\)</span>) of the likelihood function to regress <code>p_size</code> with the following equation <code>alpha + c_load * beta</code>, where both <code>p_size</code> and <code>c_load</code> are vectors defined in the data block. The following line accumulates the log-likelihood of every observation:</p>
<p><code>target += normal_lpdf(p_size | alpha + c_load * beta, sigma);</code></p>
<p>This is equivalent to and slightly faster than the following lines:</p>
<pre><code>for(n in 1:N)
    target += normal_lpdf(p_size[n] | alpha + c_load[n] * beta, sigma);</code></pre>
<p>A statement that requires some explanation is the following:</p>
<pre><code>target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);</code></pre>
<p>As in our original example in section <a href="ch-reg.html#sec-pupil">4.1</a>, we are assuming a truncated normal distribution as a prior for <span class="math inline">\(\sigma\)</span>. Not only are we setting a lower boundary to the parameter with <code>lower = 0</code>, but we are also “correcting” its prior distribution by subtracting <code>normal_lccdf(0 | 0, 1000)</code>, where  <code>lccdf</code> stands for  log complement of a cumulative distribution function. Once we add a  lower boundary, the probability mass under half of the “regular” normal distribution should be one, that is, when we integrate from zero (rather than from minus infinity) to infinity. As discussed in online section <a href="regression-models-with-brms---extended.html#app-truncation">A.2</a>, we need to normalize the PDF by dividing it by the difference of its CDF evaluated in the new boundaries (<span class="math inline">\(a = 0\)</span> and <span class="math inline">\(b = \infty\)</span> in our case):</p>
<p><span class="math display" id="eq:truncPDF2">\[\begin{equation}
f_{[a,b]}(x) = \frac{f(x)}{F(b) - F(a)}
\tag{8.8}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(f\)</span> is the PDF and <span class="math inline">\(F\)</span> the CDF.</p>
<p>This equation in log-space is:</p>
<p><span class="math display" id="eq:truncPDF3">\[\begin{equation}
log(f_{[a,b]}(x)) = log(f(x)) - log(F(b) - F(a))
\tag{8.9}
\end{equation}\]</span></p>
<p>In Stan <span class="math inline">\(\log(f(x))\)</span> corresponds to  <code>normal_lpdf(x |...)</code>, and <code>log(F(x))</code> to  <code>normal_lcdf(x|...)</code>. Because in our example <span class="math inline">\(b=\infty\)</span>, <span class="math inline">\(F(b) = 1\)</span>, we are dealing with the complement of the log CDF evaluated at <span class="math inline">\(a =0\)</span>, <span class="math inline">\(\log(1 - F(0))\)</span>, that is why we use <code>normal_lccdf(0 | ...)</code> (notice the double <code>c</code>; this symbol represents the complement of the CDF).</p>
<p>To be able to fit the model, Stan requires the data to be input as a list: First, load the data and center the dependent variable in a data frame, then create a list, and finally fit the model.</p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb551-1"><a href="ch-introstan.html#cb551-1" aria-hidden="true"></a>df_pupil &lt;-<span class="st"> </span>df_pupil <span class="op">%&gt;%</span></span>
<span id="cb551-2"><a href="ch-introstan.html#cb551-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_load =</span> load <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(load))</span></code></pre></div>
<div class="sourceCode" id="cb552"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb552-1"><a href="ch-introstan.html#cb552-1" aria-hidden="true"></a>ls_pupil &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">p_size =</span> df_pupil<span class="op">$</span>p_size,</span>
<span id="cb552-2"><a href="ch-introstan.html#cb552-2" aria-hidden="true"></a>                 <span class="dt">c_load =</span> df_pupil<span class="op">$</span>c_load,</span>
<span id="cb552-3"><a href="ch-introstan.html#cb552-3" aria-hidden="true"></a>                 <span class="dt">N =</span> <span class="kw">nrow</span>(df_pupil))</span>
<span id="cb552-4"><a href="ch-introstan.html#cb552-4" aria-hidden="true"></a>pupil &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb552-5"><a href="ch-introstan.html#cb552-5" aria-hidden="true"></a>                     <span class="st">&quot;pupil.stan&quot;</span>,</span>
<span id="cb552-6"><a href="ch-introstan.html#cb552-6" aria-hidden="true"></a>                     <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb552-7"><a href="ch-introstan.html#cb552-7" aria-hidden="true"></a>fit_pupil &lt;-<span class="st"> </span><span class="kw">stan</span>(pupil, <span class="dt">data =</span> ls_pupil)</span></code></pre></div>
<p>Check the  traceplots (Figure <a href="ch-introstan.html#fig:traceplotreg">8.5</a>).</p>

<div class="sourceCode" id="cb553"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb553-1"><a href="ch-introstan.html#cb553-1" aria-hidden="true"></a><span class="kw">traceplot</span>(fit_pupil, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:traceplotreg"></span>
<img src="bayescogsci_files/figure-html/traceplotreg-1.svg" alt="Traceplots of alpha, beta, and sigma from the model fit_pupil." width="672" />
<p class="caption">
FIGURE 8.5: Traceplots of <code>alpha</code>, <code>beta</code>, and <code>sigma</code> from the model <code>fit_pupil</code>.
</p>
</div>
<p>Examine some summaries of the marginal posterior distributions of the parameters of interest:</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb554-1"><a href="ch-introstan.html#cb554-1" aria-hidden="true"></a><span class="kw">print</span>(fit_pupil, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</span></code></pre></div>
<pre><code>##        mean  2.5% 97.5% n_eff Rhat
## alpha 701.6 661.5 743.1  3299    1
## beta   33.6  10.4  56.8  3207    1
## sigma 128.8 103.4 161.7  2908    1</code></pre>
<p>Plot the posterior distributions (Figure <a href="ch-introstan.html#fig:postreg">8.6</a>).</p>

<div class="sourceCode" id="cb556"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb556-1"><a href="ch-introstan.html#cb556-1" aria-hidden="true"></a>df_fit_pupil &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_pupil)</span>
<span id="cb556-2"><a href="ch-introstan.html#cb556-2" aria-hidden="true"></a><span class="kw">mcmc_hist</span>(fit_pupil, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:postreg"></span>
<img src="bayescogsci_files/figure-html/postreg-1.svg" alt="Histograms of the posterior samples of alpha, beta, and sigma from the model fit_pupil." width="672" />
<p class="caption">
FIGURE 8.6: Histograms of the posterior samples of <code>alpha</code>, <code>beta</code>, and <code>sigma</code> from the model <code>fit_pupil</code>.
</p>
</div>
<p>To determine the probability that the posterior for beta is larger than zero given the model and data, examine the proportion of samples above zero:</p>
<div class="sourceCode" id="cb557"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb557-1"><a href="ch-introstan.html#cb557-1" aria-hidden="true"></a><span class="co"># We are using df_fit_pupil and not the &quot;raw&quot; Stanfit object.</span></span>
<span id="cb557-2"><a href="ch-introstan.html#cb557-2" aria-hidden="true"></a><span class="kw">mean</span>(df_fit_pupil<span class="op">$</span>beta <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 0.997</code></pre>
<p>To generate prior or  posterior predictive distributions, we can create our own functions in R with the <code>purrr</code> function <code>map_dfr()</code> (or a for-loop) as we did in section <a href="ch-reg.html#sec-trial">4.2</a> with the function <code>lognormal_model_pred()</code>. Alternatively, we can use the  <code>generated quantities</code> block in our model:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] c_load;
  int&lt;lower= 0, upper = 1&gt; onlyprior;
  vector[N] p_size;
}
parameters {
  real alpha;
  real beta;
  real&lt;lower = 0&gt; sigma;
}
model {
  // priors including all constants
  target += normal_lpdf(alpha | 1000, 500);
  target += normal_lpdf(beta | 0, 100);
  target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);
  if (!onlyprior)
    target += normal_lpdf(p_size | alpha + c_load * beta, sigma);
}
generated quantities {
  array[N] real p_size_pred;
  p_size_pred = normal_rng(alpha + c_load * beta, sigma);
}</code></pre>
<p>For most of the probability functions, there is a matching  pseudorandom number generator  (PRNG) with the suffix  <code>_rng</code>. Here we are using the vectorized function  <code>normal_rng</code>. Once <code>p_size_pred</code> is declared as an array of size <code>N</code>, the following statement generates <span class="math inline">\(N\)</span> predictions (for each iteration of the sampler):</p>
<pre><code>p_size_pred = normal_rng(alpha + c_load * beta, sigma);</code></pre>
<p>At the moment not all the PRNG are vectorized, but the ones that are only allow for  arrays and, confusingly enough, not vectors. We define an array by indicating <code>array</code>, between brackets, the length of each dimension, then the type, and finally the name of the variable. For example, to define an array of real numbers with three dimensions of length 6, 7, and 10, we write <code>array[6, 7, 10] real var</code>.<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a> Vectors and matrices are also valid types for an array. See online section <a href="advanced-models-with-stan---extended.html#app-stancontainers">B.4</a> for more about the difference between arrays and vectors, and other algebra types. We also included a data variable called <code>onlyprior</code>, this is an integer that can only be set to <span class="math inline">\(1\)</span> (TRUE) or <span class="math inline">\(0\)</span> (FALSE). When <code>onlyprior == 1</code>, the likelihood is omitted from the model, <code>p_size</code> is ignored, and <code>p_size_pred</code> is the  prior predictive distribution. When <code>onlyprior == 0</code>, the likelihood is incorporated in the model (as it is in the original code <code>pupil.stan</code>) using <code>p_size</code>, and <code>p_size_pred</code> is the posterior predictive distribution.</p>
<p>If we want posterior predictive distributions, we fit the model to the data and set <code>onlyprior = 0</code>, if we want prior predictive distributions, we sample from the priors and set <code>onlyprior = 1</code>. Then we use <code>bayesplot</code> functions to visualize predictive checks.</p>
<p>For posterior predictive checks, we would do the following:</p>
<div class="sourceCode" id="cb561"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb561-1"><a href="ch-introstan.html#cb561-1" aria-hidden="true"></a>ls_pupil &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">onlyprior =</span> <span class="dv">0</span>,</span>
<span id="cb561-2"><a href="ch-introstan.html#cb561-2" aria-hidden="true"></a>                 <span class="dt">p_size =</span> df_pupil<span class="op">$</span>p_size,</span>
<span id="cb561-3"><a href="ch-introstan.html#cb561-3" aria-hidden="true"></a>                 <span class="dt">c_load =</span> df_pupil<span class="op">$</span>c_load,</span>
<span id="cb561-4"><a href="ch-introstan.html#cb561-4" aria-hidden="true"></a>                 <span class="dt">N =</span> <span class="kw">nrow</span>(df_pupil))</span>
<span id="cb561-5"><a href="ch-introstan.html#cb561-5" aria-hidden="true"></a>pupil_gen &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb561-6"><a href="ch-introstan.html#cb561-6" aria-hidden="true"></a>                         <span class="st">&quot;pupil_gen.stan&quot;</span>,</span>
<span id="cb561-7"><a href="ch-introstan.html#cb561-7" aria-hidden="true"></a>                         <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb561-8"><a href="ch-introstan.html#cb561-8" aria-hidden="true"></a>fit_pupil &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> pupil_gen, <span class="dt">data =</span> ls_pupil)</span></code></pre></div>
<p>Store the predicted pupil sizes in <code>yrep_pupil</code>. This variable contains an <span class="math inline">\(N_{samples} \times N_{observations}\)</span> matrix, that is, each row of the matrix is a draw from the posterior predictive distribution, i.e., a vector with one element for each of the data points in y.</p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb562-1"><a href="ch-introstan.html#cb562-1" aria-hidden="true"></a>yrep_pupil &lt;-<span class="st"> </span><span class="kw">extract</span>(fit_pupil)<span class="op">$</span>p_size_pred</span>
<span id="cb562-2"><a href="ch-introstan.html#cb562-2" aria-hidden="true"></a><span class="kw">dim</span>(yrep_pupil)</span></code></pre></div>
<pre><code>## [1] 4000   41</code></pre>
<p> Predictive checks functions in  <code>bayesplot</code> (starting with  <code>ppc_</code>) require a vector with the observations in the first argument and a matrix with the predictive distribution as its second argument. As an example, in Figure <a href="ch-introstan.html#fig:densoverlay">8.7</a> we use an overlay of densities and we draw only <span class="math inline">\(50\)</span> elements (that is <span class="math inline">\(50\)</span> predicted data sets).</p>

<div class="sourceCode" id="cb564"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb564-1"><a href="ch-introstan.html#cb564-1" aria-hidden="true"></a><span class="kw">ppc_dens_overlay</span>(df_pupil<span class="op">$</span>p_size, <span class="dt">yrep =</span> yrep_pupil[<span class="dv">1</span><span class="op">:</span><span class="dv">50</span>, ])</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:densoverlay"></span>
<img src="bayescogsci_files/figure-html/densoverlay-1.svg" alt="A posterior predictive check showing 50 predicted density plots from the model fit_pupil against the observed data." width="672" />
<p class="caption">
FIGURE 8.7: A posterior predictive check showing 50 predicted density plots from the model <code>fit_pupil</code> against the observed data.
</p>
</div>
<p>For prior predictive distributions, we simply set <code>onlyprior = 1</code>. The observations (<code>p_size</code>) are ignored by the model, but are required by the data block in Stan. If we haven’t collected data yet, we could include a vector of zeros.</p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb565-1"><a href="ch-introstan.html#cb565-1" aria-hidden="true"></a>ls_pupil_prior &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">onlyprior =</span> <span class="dv">1</span>,</span>
<span id="cb565-2"><a href="ch-introstan.html#cb565-2" aria-hidden="true"></a>                       <span class="dt">p_size =</span> df_pupil<span class="op">$</span>p_size,</span>
<span id="cb565-3"><a href="ch-introstan.html#cb565-3" aria-hidden="true"></a>                       <span class="co"># or: p_size = rep(0, nrow(df_pupil)),</span></span>
<span id="cb565-4"><a href="ch-introstan.html#cb565-4" aria-hidden="true"></a>                       <span class="dt">c_load =</span> df_pupil<span class="op">$</span>c_load,</span>
<span id="cb565-5"><a href="ch-introstan.html#cb565-5" aria-hidden="true"></a>                       <span class="dt">N =</span> <span class="kw">nrow</span>(df_pupil))</span>
<span id="cb565-6"><a href="ch-introstan.html#cb565-6" aria-hidden="true"></a>prior_pupil &lt;-<span class="st"> </span><span class="kw">stan</span>(pupil_gen, <span class="dt">data =</span> ls_pupil_prior,</span>
<span id="cb565-7"><a href="ch-introstan.html#cb565-7" aria-hidden="true"></a>                    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.99</span>))</span></code></pre></div>
<p>To avoid divergent transitions, increase the <code>adapt_delta</code> parameter’s default value from <span class="math inline">\(0.8\)</span> to <span class="math inline">\(0.99\)</span>. It is important to highlight that we cannot safely ignore the warnings of the above model, even if we are not fitting data. This is so because in practice one is still sampling a density using Hamiltonian Monte Carlo, and thus the prior sampling process can break in the same ways as the posterior sampling process. Prior predictive distributions as the one shown in Figure <a href="ch-introstan.html#fig:priordensoverlay">8.8</a> can be plot with  <code>ppd_dens_overlay()</code> (and in general with functions starting with  <code>ppd_</code> which don’t require an argument with data).</p>

<div class="sourceCode" id="cb566"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb566-1"><a href="ch-introstan.html#cb566-1" aria-hidden="true"></a>yrep_prior_pupil &lt;-<span class="st"> </span><span class="kw">extract</span>(prior_pupil)<span class="op">$</span>p_size_pred</span>
<span id="cb566-2"><a href="ch-introstan.html#cb566-2" aria-hidden="true"></a><span class="kw">ppd_dens_overlay</span>(yrep_prior_pupil[<span class="dv">1</span><span class="op">:</span><span class="dv">50</span>, ])</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:priordensoverlay"></span>
<img src="bayescogsci_files/figure-html/priordensoverlay-1.svg" alt="Prior predictive distribution showing \(50\) predicted density plots from the model fit_pupil." width="672"  />
<p class="caption">
FIGURE 8.8: Prior predictive distribution showing <span class="math inline">\(50\)</span> predicted density plots from the model <code>fit_pupil</code>.
</p>
</div>
</div>
<div id="sec-interstan" class="section level3 hasAnchor" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span>  Interactions in Stan: Does attentional load interact with trial number affecting  pupil size?<a href="ch-introstan.html#sec-interstan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ll expand the previous model to also include the effect of (centered) trial and its interaction with cognitive load on one subject’s pupil size. Our new likelihood is as follows:</p>
<p><span class="math display">\[\begin{equation}
p\_size_n \sim \mathit{Normal}(\alpha + c\_load_n \cdot \beta_1 + c\_trial \cdot \beta_2 + c\_load \cdot c\_trial \cdot \beta_3, \sigma)
\end{equation}\]</span></p>
<p>Define priors for all the new <span class="math inline">\(\beta\)</span>s. Since we don’t have more information about the new predictors, they are sampled from identical prior distributions:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim \mathit{Normal}(1000, 500) \\
\beta_1 &amp;\sim \mathit{Normal}(0, 100) \\
\beta_2 &amp;\sim \mathit{Normal}(0, 100) \\
\beta_3 &amp;\sim \mathit{Normal}(0, 100) \\
\sigma &amp;\sim \mathit{Normal}_+(0, 1000)
\end{aligned}
\end{equation}\]</span></p>
<p>The following Stan model, <code>pupil_int1.stan</code>, is the direct translation of the new priors and likelihood.</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] c_load;
  vector[N] c_trial;
  vector[N] p_size;
}
parameters {
  real alpha;
  real beta1;
  real beta2;
  real beta3;
  real&lt;lower = 0&gt; sigma;
}
model {
  // priors including all constants
  target += normal_lpdf(alpha | 1000, 500);
  target += normal_lpdf(beta1 | 0, 100);
  target += normal_lpdf(beta2 | 0, 100);
  target += normal_lpdf(beta3 | 0, 100);
  target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);
  target += normal_lpdf(p_size | alpha + c_load * beta1 +
                                 c_trial * beta2 +
                                 c_load .* c_trial * beta3, sigma);
}
</code></pre>
<p>When there are matrices or vectors involved, <code>*</code> indicates matrix multiplication whereas <code>.*</code> indicates  element-wise multiplication; in R, <code>%*%</code> indicates matrix multiplication whereas <code>*</code> indicates element-wise multiplication.</p>
<p>There is, however, an alternative notation that can simplify our code. In the following likelihood, <span class="math inline">\(p\_size\)</span> is a vector of N observations (in this case 41), <span class="math inline">\(X\)</span> is the model matrix with a dimension of <span class="math inline">\(N \times N_{pred}\)</span> (in this case <span class="math inline">\(41 \times 3\)</span>), and <span class="math inline">\(\beta\)</span> a vector of <span class="math inline">\(N_{pred}\)</span> (in this case, 3) rows. Assuming that <span class="math inline">\(\beta\)</span> is a vector, we indicate with one line that each parameter <span class="math inline">\(\beta_n\)</span> is sampled from identical prior distributions.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p\_size &amp;\sim \mathit{Normal}(\alpha + X \cdot \beta,\sigma)\\
\beta &amp;\sim \mathit{Normal}(0, 100) \\
\sigma &amp;\sim \mathit{Normal}_+(0, 1000)
\end{aligned}
\end{equation}\]</span></p>
<p>The translation into Stan code is the following:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  int&lt;lower = 0&gt; K;   // number of predictors
  matrix[N, K] X;   // model matrix
  vector[N] p_size;
}
parameters {
  real alpha;
  vector[K] beta;
  real&lt;lower = 0&gt; sigma;
}
model {
  // priors including all constants
  target += normal_lpdf(alpha | 1000, 500);
  target += normal_lpdf(beta | 0, 100);
  target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);
  target += normal_lpdf(p_size | alpha + X * beta, sigma);
}</code></pre>
<p>For some likelihood functions, Stan provides a more  efficient implementation of the linear regression than the one manually written in the previous code. It’s critical to understand that, in general, a more efficient implementation should not only be faster, but should also achieve the same number of effective samples (or more) than a less efficient implementation (and should also show convergence). In this case, we can achieve that using <code>_glm</code> functions. We can replace the last line with the following statement (the order of the arguments is important):</p>
<pre><code>target += normal_id_glm_lpdf(p_size | X, alpha, beta, sigma);</code></pre>
<p>The most optimized model, <code>pupil_int.stan</code>, includes this last statement. We prepare the data as follows: First create a centered version of trial, <code>c_trial</code> and load <code>c_load</code>, then use the function <code>model.matrix</code> to create the <code>X</code> matrix that contains in each column our predictors and omits the intercept with <code>0 +</code>.</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb570-1"><a href="ch-introstan.html#cb570-1" aria-hidden="true"></a>df_pupil &lt;-<span class="st"> </span>df_pupil <span class="op">%&gt;%</span></span>
<span id="cb570-2"><a href="ch-introstan.html#cb570-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_trial =</span> trial <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(trial),</span>
<span id="cb570-3"><a href="ch-introstan.html#cb570-3" aria-hidden="true"></a>         <span class="dt">c_load =</span> load <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(load))</span>
<span id="cb570-4"><a href="ch-introstan.html#cb570-4" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>c_load <span class="op">*</span><span class="st"> </span>c_trial, df_pupil)</span>
<span id="cb570-5"><a href="ch-introstan.html#cb570-5" aria-hidden="true"></a>ls_pupil_X &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">p_size =</span> df_pupil<span class="op">$</span>p_size,</span>
<span id="cb570-6"><a href="ch-introstan.html#cb570-6" aria-hidden="true"></a>                   <span class="dt">X =</span> X,</span>
<span id="cb570-7"><a href="ch-introstan.html#cb570-7" aria-hidden="true"></a>                   <span class="dt">K =</span> <span class="kw">ncol</span>(X),</span>
<span id="cb570-8"><a href="ch-introstan.html#cb570-8" aria-hidden="true"></a>                   <span class="dt">N =</span> <span class="kw">nrow</span>(df_pupil))</span></code></pre></div>
<div class="sourceCode" id="cb571"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb571-1"><a href="ch-introstan.html#cb571-1" aria-hidden="true"></a>pupil_int &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb571-2"><a href="ch-introstan.html#cb571-2" aria-hidden="true"></a>                         <span class="st">&quot;pupil_int.stan&quot;</span>,</span>
<span id="cb571-3"><a href="ch-introstan.html#cb571-3" aria-hidden="true"></a>                         <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb571-4"><a href="ch-introstan.html#cb571-4" aria-hidden="true"></a>fit_pupil_int &lt;-<span class="st"> </span><span class="kw">stan</span>(pupil_int, <span class="dt">data =</span> ls_pupil_X)</span></code></pre></div>
<div class="sourceCode" id="cb572"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb572-1"><a href="ch-introstan.html#cb572-1" aria-hidden="true"></a><span class="kw">print</span>(fit_pupil_int, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</span></code></pre></div>
<pre><code>##           mean   2.5%  97.5% n_eff Rhat
## alpha   699.63 668.03 731.54  4941    1
## beta[1]  31.26  11.88  49.85  4605    1
## beta[2]  -5.79  -8.55  -2.87  5410    1
## beta[3]  -1.82  -3.51  -0.20  4329    1
## sigma   104.48  82.88 133.55  3732    1</code></pre>
<p>In Figure <a href="ch-introstan.html#fig:pupilintbeta">8.9</a>, we plot the 95% CrI of the parameters of interest. We use  <code>regex_pars</code>, rather than <code>pars</code>, because we want to capture <code>beta[1]</code>, <code>beta[2]</code>, and <code>beta[3]</code>; <code>regex_pars</code> use  regular expressions to select the parameters (for information about regular expressions in R, see <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html" class="uri">https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html</a>).</p>

<div class="sourceCode" id="cb574"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb574-1"><a href="ch-introstan.html#cb574-1" aria-hidden="true"></a>df_fit_pupil_int &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_pupil_int)</span>
<span id="cb574-2"><a href="ch-introstan.html#cb574-2" aria-hidden="true"></a><span class="kw">mcmc_intervals</span>(fit_pupil_int,</span>
<span id="cb574-3"><a href="ch-introstan.html#cb574-3" aria-hidden="true"></a>               <span class="dt">regex_pars =</span> <span class="st">&quot;beta&quot;</span>,</span>
<span id="cb574-4"><a href="ch-introstan.html#cb574-4" aria-hidden="true"></a>               <span class="dt">prob_outer =</span> <span class="fl">.95</span>,</span>
<span id="cb574-5"><a href="ch-introstan.html#cb574-5" aria-hidden="true"></a>               <span class="dt">prob =</span> <span class="fl">.8</span>,</span>
<span id="cb574-6"><a href="ch-introstan.html#cb574-6" aria-hidden="true"></a>               <span class="dt">point_est =</span> <span class="st">&quot;mean&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pupilintbeta"></span>
<img src="bayescogsci_files/figure-html/pupilintbeta-1.svg" alt="The means and 95% CrIs of the effect of load, beta[1], the effect of trial, beta[2], and their interaction, beta[3]." width="672" />
<p class="caption">
FIGURE 8.9: The means and 95% CrIs of the effect of load, <code>beta[1]</code>, the effect of trial, <code>beta[2]</code>, and their interaction, <code>beta[3]</code>.
</p>
</div>
</div>
<div id="sec-logisticstan" class="section level3 hasAnchor" number="8.4.3">
<h3><span class="header-section-number">8.4.3</span>  Logistic regression in Stan: Does set size and trial affect free recall?<a href="ch-introstan.html#sec-logisticstan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We revisit and expand on the analysis presented in section <a href="ch-reg.html#sec-logistic">4.3</a> of a subset of the data from <span class="citation">Oberauer (<a href="#ref-oberauerWorkingMemoryCapacity2019" role="doc-biblioref">2019</a>)</span>. In this example, we will investigate whether the length of a list and trial number affect the probability of correctly recalling a word.</p>
<p>As in section <a href="ch-reg.html#sec-logistic">4.3</a>, we assume a  Bernoulli likelihood with a  logit link function, and the following priors (recall that the  logistic function is the inverse of the logit).</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
correct_n &amp;\sim \mathit{Bernoulli}( \mathit{logistic}(\alpha + X \cdot \beta))\\
\alpha &amp;\sim \mathit{Normal}(0, 1.5) \\
\beta &amp;\sim \mathit{Normal}(0, 0.1)
\end{aligned}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(\beta\)</span> is a vector of size <span class="math inline">\(K = 2\)</span>, <span class="math inline">\(\langle \beta_0, \beta_1 \rangle\)</span>. Below in <code>recall.stan</code> we present the most efficient way to code this in Stan.</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  int&lt;lower=0&gt; K;   // number of predictors
  matrix[N, K] X;   // model matrix
  array[N] int correct;
}
parameters {
  real alpha;
  vector[K] beta;
}
model {
  // priors including all constants
  target += normal_lpdf(alpha | 0, 1.5);
  target += normal_lpdf(beta | 0, .1);
  target += bernoulli_logit_glm_lpmf(correct | X, alpha, beta);
}</code></pre>
<p>The dependent variable, <code>correct</code>, is an array of integers rather than a vector; this is because vectors are always composed of real numbers, but the Bernoulli likelihood only accepts the integers <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span>. As in the previous example, we are taking advantage of the <code>_glm</code> functions. A less efficient but more transparent option would be to replace the last statement with:</p>
<pre><code>target += bernoulli_logit_lpmf(correct | alpha + X * beta);</code></pre>
<p>We might want to use  <code>bernoulli_logit_lpmf()</code> if we want to define a non-linear relationship between the predictors that are outside the generalized linear model framework. One example would be the following:</p>
<pre><code>target += bernoulli_logit_lpmf(correct| alpha + exp(X * beta));</code></pre>
<p>Another more flexible possibility when we want to indicate a Bernoulli likelihood is to use  <code>bernoulli_lpmf()</code> and add the link manually. The last statement of <code>recall.stan</code> would become the following:</p>
<pre><code>target += bernoulli_lpmf(correct| inv_logit(alpha + X * beta));</code></pre>
<p>The function <code>bernoulli_lpmf()</code> can be useful if one wants to try other link functions; see online exercise <a href="exercises.html#exr:linkfunction">G.8.4</a>.</p>
<p>Finally, the most transparent form (but less efficient) would be the following for-loop:</p>
<pre><code>for (n in 1:N)
  target += bernoulli_lpmf(correct[n] | inv_logit(alpha + X[n] * beta));</code></pre>
<p>To fit the model as <code>recall.stan</code>, prepare the data by centering the trial number first:</p>
<div class="sourceCode" id="cb580"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb580-1"><a href="ch-introstan.html#cb580-1" aria-hidden="true"></a>df_recall &lt;-<span class="st"> </span>df_recall <span class="op">%&gt;%</span></span>
<span id="cb580-2"><a href="ch-introstan.html#cb580-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_set_size =</span> set_size <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(set_size),</span>
<span id="cb580-3"><a href="ch-introstan.html#cb580-3" aria-hidden="true"></a>         <span class="dt">c_trial =</span> trial <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(trial))</span></code></pre></div>
<p>Next, create the model matrix, <span class="math inline">\(X\)</span>, and prepare the data as a list. As in section <a href="ch-introstan.html#sec-interstan">8.4.2</a>, exclude the intercept from the matrix <code>X</code> using <code>0 +...</code>. This is because in the Stan code that we are using, the intercept is kept separate from the model matrix <span class="math inline">\(X\)</span>, which only has the contrast coding for the slope parameters. The approach we take here is equivalent to defining a model matrix whose first column is a vector of ones.</p>
<div class="sourceCode" id="cb581"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb581-1"><a href="ch-introstan.html#cb581-1" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>c_set_size <span class="op">*</span><span class="st"> </span>c_trial, df_recall)</span>
<span id="cb581-2"><a href="ch-introstan.html#cb581-2" aria-hidden="true"></a>ls_recall &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">correct =</span> df_recall<span class="op">$</span>correct,</span>
<span id="cb581-3"><a href="ch-introstan.html#cb581-3" aria-hidden="true"></a>                  <span class="dt">X =</span> X,</span>
<span id="cb581-4"><a href="ch-introstan.html#cb581-4" aria-hidden="true"></a>                  <span class="dt">K =</span> <span class="kw">ncol</span>(X),</span>
<span id="cb581-5"><a href="ch-introstan.html#cb581-5" aria-hidden="true"></a>                  <span class="dt">N =</span> <span class="kw">nrow</span>(df_recall))</span></code></pre></div>
<div class="sourceCode" id="cb582"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb582-1"><a href="ch-introstan.html#cb582-1" aria-hidden="true"></a>recall &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb582-2"><a href="ch-introstan.html#cb582-2" aria-hidden="true"></a>                      <span class="st">&quot;recall.stan&quot;</span>,</span>
<span id="cb582-3"><a href="ch-introstan.html#cb582-3" aria-hidden="true"></a>                      <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb582-4"><a href="ch-introstan.html#cb582-4" aria-hidden="true"></a>fit_recall &lt;-<span class="st"> </span><span class="kw">stan</span>(recall, <span class="dt">data =</span> ls_recall)</span></code></pre></div>
<p>After fitting the model, we can print and plot summaries of the posterior distribution.</p>
<div class="sourceCode" id="cb583"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb583-1"><a href="ch-introstan.html#cb583-1" aria-hidden="true"></a><span class="kw">print</span>(fit_recall, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>))</span></code></pre></div>
<pre><code>##          mean  2.5% 97.5% n_eff Rhat
## alpha    1.99  1.40  2.62  3289    1
## beta[1] -0.19 -0.35 -0.02  4021    1
## beta[2] -0.02 -0.09  0.05  3122    1
## beta[3]  0.00 -0.03  0.04  3533    1</code></pre>
<p>In Figure <a href="ch-introstan.html#fig:cri-vaccdet">8.10</a>(a), we plot the 95% CrI of the parameters of interest.</p>
<div class="sourceCode" id="cb585"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb585-1"><a href="ch-introstan.html#cb585-1" aria-hidden="true"></a>df_fit_recall &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_recall)</span>
<span id="cb585-2"><a href="ch-introstan.html#cb585-2" aria-hidden="true"></a><span class="kw">mcmc_intervals</span>(df_fit_recall,</span>
<span id="cb585-3"><a href="ch-introstan.html#cb585-3" aria-hidden="true"></a>               <span class="dt">regex_pars =</span> <span class="st">&quot;beta&quot;</span>,</span>
<span id="cb585-4"><a href="ch-introstan.html#cb585-4" aria-hidden="true"></a>               <span class="dt">prob_outer =</span> <span class="fl">.95</span>,</span>
<span id="cb585-5"><a href="ch-introstan.html#cb585-5" aria-hidden="true"></a>               <span class="dt">prob =</span> <span class="fl">.8</span>,</span>
<span id="cb585-6"><a href="ch-introstan.html#cb585-6" aria-hidden="true"></a>               <span class="dt">point_est =</span> <span class="st">&quot;mean&quot;</span>) <span class="op">+</span></span>
<span id="cb585-7"><a href="ch-introstan.html#cb585-7" aria-hidden="true"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;log-odds&quot;</span>) </span></code></pre></div>
<p>As shown in section <a href="ch-reg.html#sec-comlogis">4.3.4</a>, we might want to communicate the posterior in proportions rather than in log-odds (as seen in the parameters <code>beta</code>). This can be done in R by manipulating the data frame <code>df_fit_recall</code>, or by extracting the samples with <code>extract(fit_recall)</code>. Another alternative presented here is by using the generated quantities block. To make the code more compact, we declare the type of each variable and store its content in the same line in <code>recall_prop.stan</code>.</p>
<pre class="stan fold-show"><code>generated quantities {
  real average_accuracy = inv_logit(alpha);
  vector[K] change_acc = inv_logit(alpha) - inv_logit(alpha - beta);
}</code></pre>
<p>Recall that due to the non-linearity of the scale, the effects depend on the average accuracy, and the set size and trial that we start from (in this case we are examining the change of one unit from the average set size and the average trial).</p>
<div class="sourceCode" id="cb587"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb587-1"><a href="ch-introstan.html#cb587-1" aria-hidden="true"></a>recall_prop &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb587-2"><a href="ch-introstan.html#cb587-2" aria-hidden="true"></a>                           <span class="st">&quot;recall_prop.stan&quot;</span>,</span>
<span id="cb587-3"><a href="ch-introstan.html#cb587-3" aria-hidden="true"></a>                           <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb587-4"><a href="ch-introstan.html#cb587-4" aria-hidden="true"></a>fit_recall &lt;-<span class="st"> </span><span class="kw">stan</span>(recall_prop, <span class="dt">data =</span> ls_recall)</span></code></pre></div>
<p>The plot in Figure <a href="ch-introstan.html#fig:cri-vaccdet">8.10</a>(b) now shows how the average accuracy deteriorates when the subject is exposed to a set size larger than the average by one, a trial further than the middle one, and the interaction of both.</p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb588-1"><a href="ch-introstan.html#cb588-1" aria-hidden="true"></a>df_fit_recall &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_recall) <span class="op">%&gt;%</span></span>
<span id="cb588-2"><a href="ch-introstan.html#cb588-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">rename</span>(<span class="dt">set_size =</span> <span class="st">`</span><span class="dt">change_acc[1]</span><span class="st">`</span>,</span>
<span id="cb588-3"><a href="ch-introstan.html#cb588-3" aria-hidden="true"></a>         <span class="dt">trial =</span> <span class="st">`</span><span class="dt">change_acc[2]</span><span class="st">`</span>,</span>
<span id="cb588-4"><a href="ch-introstan.html#cb588-4" aria-hidden="true"></a>         <span class="dt">interaction =</span> <span class="st">`</span><span class="dt">change_acc[3]</span><span class="st">`</span>)</span>
<span id="cb588-5"><a href="ch-introstan.html#cb588-5" aria-hidden="true"></a><span class="kw">mcmc_intervals</span>(df_fit_recall,</span>
<span id="cb588-6"><a href="ch-introstan.html#cb588-6" aria-hidden="true"></a>               <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;set_size&quot;</span>, <span class="st">&quot;trial&quot;</span>, <span class="st">&quot;interaction&quot;</span>),</span>
<span id="cb588-7"><a href="ch-introstan.html#cb588-7" aria-hidden="true"></a>               <span class="dt">prob_outer =</span> <span class="fl">.95</span>,</span>
<span id="cb588-8"><a href="ch-introstan.html#cb588-8" aria-hidden="true"></a>               <span class="dt">prob =</span> <span class="fl">.8</span>,</span>
<span id="cb588-9"><a href="ch-introstan.html#cb588-9" aria-hidden="true"></a>               <span class="dt">point_est =</span> <span class="st">&quot;mean&quot;</span>) <span class="op">+</span></span>
<span id="cb588-10"><a href="ch-introstan.html#cb588-10" aria-hidden="true"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Change in accuracy&quot;</span>)</span></code></pre></div>

<div class="figure"><span style="display:block;" id="fig:cri-vaccdet"></span>
<img src="bayescogsci_files/figure-html/cri-vaccdet-1.svg" alt="(a) 95% credible intervals for the beta parameters of the fit_recall model, representing the log-odds effects of set size, trial, and their interaction. (b) Impact of set size, trial, and their interaction on average recall accuracy." width="48%" /><img src="bayescogsci_files/figure-html/cri-vaccdet-2.svg" alt="(a) 95% credible intervals for the beta parameters of the fit_recall model, representing the log-odds effects of set size, trial, and their interaction. (b) Impact of set size, trial, and their interaction on average recall accuracy." width="48%" />
<p class="caption">
FIGURE 8.10: (a) 95% credible intervals for the <code>beta</code> parameters of the <code>fit_recall</code> model, representing the log-odds effects of set size, trial, and their interaction. (b) Impact of set size, trial, and their interaction on average recall accuracy.
</p>
</div>
<p>The plot in Figure <a href="ch-introstan.html#fig:cri-vaccdet">8.10</a>(b) shows that our model is estimating that by increasing the set size by one unit, the recall accuracy of the single subject deteriorates by 2%. In contrast, there is hardly any trial effect or interaction between trial and set size.</p>
</div>
</div>
<div id="summary-7" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Summary<a href="ch-introstan.html#summary-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter introduced basic Stan syntax for fitting some standard linear models. Example code covered the normal, binomial, Bernoulli, and log-normal likelihoods. We also saw how to express regression models using the matrix model in Stan syntax.</p>
</div>
<div id="further-reading-5" class="section level2 hasAnchor" number="8.6">
<h2><span class="header-section-number">8.6</span> Further reading<a href="ch-introstan.html#further-reading-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For further reading on the Hamiltonian Monte Carlo algorithm, see the rather technical review of <span class="citation">Betancourt (<a href="#ref-betancourt2017conceptual" role="doc-biblioref">2017</a>)</span>, or the more conceptual introduction provided by <span class="citation">Monnahan, Thorson, and Branch (<a href="#ref-monnahanFasterEstimationBayesian2017" role="doc-biblioref">2017</a>)</span> or chapter 17 of <span class="citation">Lambert (<a href="#ref-lambert2018student" role="doc-biblioref">2018</a>)</span>. A useful article with example R code is <span class="citation">Neal (<a href="#ref-nealMCMCUsingHamiltonian2011" role="doc-biblioref">2011</a>)</span>. A detailed walk-through on its implementation is also provided in Chapter 41 of <span class="citation">MacKay (<a href="#ref-mackay" role="doc-biblioref">2003</a>)</span>. The Stan documentation <span class="citation">(Stan Development Team <a href="#ref-Stan2023" role="doc-biblioref">2024</a>)</span>, consisting of a User’s Guide and the Language Reference Manual are important starting points for going deeper into Stan programming: <a href="https://mc-stan.org/users/documentation/" class="uri">https://mc-stan.org/users/documentation/</a>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references hanging-indent">
<div id="ref-betancourt2016identifying">
<p>Betancourt, Michael J. 2016. “Identifying the Optimal Integration Time in Hamiltonian Monte Carlo.” <em>arXiv Preprint arXiv:1601.00225</em>. <a href="http://arxiv.org/abs/1601.00225">http://arxiv.org/abs/1601.00225</a>.</p>
</div>
<div id="ref-betancourt2017conceptual">
<p>Betancourt, Michael J. 2017. “A Conceptual Introduction to Hamiltonian Monte Carlo.” <em>arXiv Preprin: arXiv:1701.02434</em>. <a href="http://arxiv.org/abs/1701.02434">http://arxiv.org/abs/1701.02434</a>.</p>
</div>
<div id="ref-carpenter2017stan">
<p>Carpenter, Bob, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael J. Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. “Stan: A Probabilistic Programming Language.” <em>Journal of Statistical Software</em> 76 (1).</p>
</div>
<div id="ref-R-bayesplot">
<p>Gabry, Jonah, and Tristan Mahr. 2024. <em>bayesplot: Plotting for Bayesian Models</em>. <a href="https://mc-stan.org/bayesplot/">https://mc-stan.org/bayesplot/</a>.</p>
</div>
<div id="ref-green1998penalized">
<p>Green, Peter J. 1998. “Penalized Likelihood.” In <em>Encyclopedia of Statistical Sciences</em>, 2:578–86. Wiley.</p>
</div>
<div id="ref-R-rstan">
<p>Guo, Jiqiang, Jonah Gabry, Ben Goodrich, Andrew Johnson, Sebastian Weber, and Hamada S. Badr. 2024. <em>rstan: R Interface to Stan</em>. <a href="https://mc-stan.org/rstan/">https://mc-stan.org/rstan/</a>.</p>
</div>
<div id="ref-hoffmanNoUTurnSamplerAdaptively2014">
<p>Hoffman, Matthew D., and Andrew Gelman. 2014. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” <em>Journal of Machine Learning Research</em> 15 (1): 1593–1623. <a href="http://dl.acm.org/citation.cfm?id=2627435.2638586">http://dl.acm.org/citation.cfm?id=2627435.2638586</a>.</p>
</div>
<div id="ref-lambert2018student">
<p>Lambert, Ben. 2018. <em>A Student’s Guide to Bayesian Statistics</em>. London, UK: Sage.</p>
</div>
<div id="ref-lunn2012bugs">
<p>Lunn, David J., Chris Jackson, David J. Spiegelhalter, Nichola G. Best, and Andrew Thomas. 2012. <em>The BUGS Book: A Practical Introduction to Bayesian Analysis</em>. Vol. 98. CRC Press.</p>
</div>
<div id="ref-lunn2000winbugs">
<p>Lunn, David J., Andrew Thomas, Nichola G. Best, and David J. Spiegelhalter. 2000. “WinBUGS-A Bayesian Modelling Framework: Concepts, Structure, and Extensibility.” <em>Statistics and Computing</em> 10 (4): 325–37.</p>
</div>
<div id="ref-mackay">
<p>MacKay, David J. C. 2003. <em>Information Theory, Inference and Learning Algorithms</em>. Cambridge, UK: Cambridge University Press.</p>
</div>
<div id="ref-monnahanFasterEstimationBayesian2017">
<p>Monnahan, Cole C., James T. Thorson, and Trevor A. Branch. 2017. “Faster Estimation of Bayesian Models in Ecology Using Hamiltonian Monte Carlo.” Edited by Robert B. O’Hara. <em>Methods in Ecology and Evolution</em> 8 (3): 339–48. <a href="https://doi.org/10.1111/2041-210X.12681">https://doi.org/10.1111/2041-210X.12681</a>.</p>
</div>
<div id="ref-nealMCMCUsingHamiltonian2011">
<p>Neal, Radford M. 2011. “MCMC Using Hamiltonian Dynamics.” In <em>Handbook of Markov Chain Monte Carlo</em>, edited by Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Taylor &amp; Francis. <a href="https://doi.org/10.1201/b10905-10">https://doi.org/10.1201/b10905-10</a>.</p>
</div>
<div id="ref-oberauerWorkingMemoryCapacity2019">
<p>Oberauer, Klaus. 2019. “Working Memory Capacity Limits Memory for Bindings.” <em>Journal of Cognition</em> 2 (1): 40. <a href="https://doi.org/10.5334/joc.86">https://doi.org/10.5334/joc.86</a>.</p>
</div>
<div id="ref-plummer2016jags">
<p>Plummer, Martin. 2016. “JAGS Version 4.2.0 User Manual.”</p>
</div>
<div id="ref-R-base">
<p>R Core Team. 2023. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-ScheiplEtAl2013PenalizedlikelihoodBayesian">
<p>Scheipl, Fabian, Thomas Kneib, and Ludwig Fahrmeir. 2013. “Penalized Likelihood and Bayesian Function Selection in Regression Models.” <em>AStA Advances in Statistical Analysis</em> 97 (4): 349–85. <a href="https://doi.org/10.1007/s10182-013-0211-3">https://doi.org/10.1007/s10182-013-0211-3</a>.</p>
</div>
<div id="ref-Stan2023">
<p>Stan Development Team. 2024. “Stan Modeling Language Users Guide and Reference Manual, Version 2.32.” <a href="https://mc-stan.org/docs/2_35/">https://mc-stan.org/docs/2_35/</a>.</p>
</div>
<div id="ref-vehtari2019ranknormalization">
<p>Vehtari, Aki, Andrew Gelman, Daniel P. Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An Improved <span class="math inline">\(\widehat{R}\)</span> for Assessing Convergence of MCMC.” <em>Bayesian Analysis</em> 16 (2): 667–718. <a href="https://doi.org/10.1214/20-BA1221">https://doi.org/10.1214/20-BA1221</a>.</p>
</div>
<div id="ref-wahnPupilSizesScale2016">
<p>Wahn, Basil, Daniel P. Ferris, W. David Hairston, and Peter König. 2016. “Pupil Sizes Scale with Attentional Load and Task Experience in a Multiple Object Tracking Task.” <em>PLOS ONE</em> 11 (12): e0168087. <a href="https://doi.org/10.1371/journal.pone.0168087">https://doi.org/10.1371/journal.pone.0168087</a>.</p>
</div>
<div id="ref-R-ggplot2">
<p>Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus O. Wilke, Kara Woo, Hiroaki Yutani, and Teun van den Brand. 2024. <em>ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics</em>. <a href="https://ggplot2.tidyverse.org">https://ggplot2.tidyverse.org</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="32">
<li id="fn32"><p>In the specific case of a model with two parameters, e.g., <span class="math inline">\(\langle \mu,\sigma \rangle\)</span>, the physical analogy works quite well: The <span class="math inline">\(\langle x,y \rangle\)</span> coordinates of the particle would be determined by <span class="math inline">\(\langle \mu,\sigma \rangle\)</span>, and its <span class="math inline">\(z\)</span> coordinate would be established by the height of the negative logarithm of the unnormalized posterior.<a href="ch-introstan.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p>Incidentally, this <span class="math inline">\(\log(q(\dot))\)</span> is the variable <code>target</code> in a Stan model and <code>lp__</code> in its output; see the aside below.<a href="ch-introstan.html#fnref33" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p>In this kind of output, there are some types that are new to the R user (but they are also used in C++): <code>reals</code> indicates that any of <code>real</code>, <code>real[]</code>, <code>vector</code>, or <code>row_vector</code>. A return type R with an input type <code>T</code> indicates that the type of the output of the function is the same as type of the argument.<a href="ch-introstan.html#fnref34" class="footnote-back">↩︎</a></p></li>
<li id="fn35"><p>We simplify the output of <code>print</code> in the text after this call, by actually calling <code>summary(fit, pars = pars, probs = c(0.025, 0.975))$summary</code>.<a href="ch-introstan.html#fnref35" class="footnote-back">↩︎</a></p></li>
<li id="fn36"><p>In addition, the new package <code>posterior</code> provides useful tools for working with output from Bayesian models. It allows for converting the output between many different formats, and offers consistent methods for operations commonly performed on samples, as well as summary functions.<a href="ch-introstan.html#fnref36" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p>The notation for arrays has changed in recent versions, the previous notation would have been <code>real[6, 7, 10] var</code>.<a href="ch-introstan.html#fnref37" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-coding2x2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-complexstan.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
