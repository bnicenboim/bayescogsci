<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction | Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction | Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://bruno.nicenboim.me/bayescogsci/images/temporarycover.jpg" />
  <meta property="og:description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/bnicenboim/bayescogsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction | Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="Introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bruno.nicenboim.me/bayescogsci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel J. Schad, and Shravan Vasishth" />


<meta name="date" content="2025-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="about-the-authors.html"/>
<link rel="next" href="ch-introBDA.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<script data-goatcounter="https://bayescogsci.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b>  Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The  law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b>  Discrete random variables: An example using the  binomial distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b>  Continuous random variables: An example using the  normal distribution</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1:  Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (or multivariate) data</a></li>
<li class="chapter" data-level="1.6.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-decomposevcovmatrix"><i class="fa fa-check"></i><b>1.6.4</b> Decomposing a variance-covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-some-useful-r-functions"><i class="fa fa-check"></i><b>1.8</b> Summary of some useful R functions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b>  Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the  posterior using Bayes’ rule: An analytical example</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a  likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a  prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using  Bayes’ rule to compute the  posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The  posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the  posterior through  sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b>  Bayesian Regression Models using Stan:  brms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors:  sensitivity analysis</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b>  Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b>  Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b>  Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b>  Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b>  Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The  log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first  linear regression: Does attentional load affect pupil size?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b>  Likelihood and  priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b>  Log-normal model: Does trial affect response times?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b>  Logistic regression: Does  set size affect  free recall?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b>  Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b>  Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b>  No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b>  Varying intercepts and  varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A  hierarchical log-normal model: The  Stroop effect</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes  log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>6.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>6.1.1</b> Default contrast coding:  Treatment contrasts</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>6.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>6.1.3</b>  Sum contrasts</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>6.1.4</b>  Cell means parameterization and  posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>6.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>6.2.1</b>  Sum contrasts</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>6.2.2</b> The  hypothesis matrix</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>6.2.3</b> Generating contrasts: The  <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>6.3</b> Other types of contrasts: illustration with a factor of four levels</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>6.3.1</b>  Repeated contrasts</a></li>
<li class="chapter" data-level="6.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>6.3.2</b>  Helmert contrasts</a></li>
<li class="chapter" data-level="6.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>6.3.3</b> Contrasts in linear regression analysis: The design or  model matrix</a></li>
<li class="chapter" data-level="6.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>6.3.4</b>  Polynomial contrasts</a></li>
<li class="chapter" data-level="6.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>6.3.5</b> An alternative to contrasts:  Monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>6.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>6.4.1</b>  Centered contrasts</a></li>
<li class="chapter" data-level="6.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>6.4.2</b>  Orthogonal contrasts</a></li>
<li class="chapter" data-level="6.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> The role of the  intercept in  non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>6.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="6.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-5"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
<li class="chapter" data-level="6.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-3"><i class="fa fa-check"></i><b>6.7</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>7</b> Contrast coding with two predictor variables</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>7.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>7.1.1</b>  Nested effects</a></li>
<li class="chapter" data-level="7.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>7.1.2</b>  Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>7.2</b> One factor and one  covariate</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>7.2.1</b> Estimating a  group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>7.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>7.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="7.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="8" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>8</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>8.1</b> Stan syntax</a></li>
<li class="chapter" data-level="8.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>8.2</b> A first simple example with Stan:  Normal likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>8.3</b> Another simple example:  Cloze probability with Stan with the  binomial likelihood</a></li>
<li class="chapter" data-level="8.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>8.4</b>  Regression models in Stan</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>8.4.1</b> A first  linear regression in Stan: Does attentional load affect  pupil size?</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>8.4.2</b>  Interactions in Stan: Does attentional load interact with trial number affecting  pupil size?</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>8.4.3</b>  Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-7"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-5"><i class="fa fa-check"></i><b>8.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>9</b> Hierarchical models and reparameterization </a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>9.1</b> Hierarchical models with Stan</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>9.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>9.1.2</b> Uncorrelated  varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="9.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>9.1.3</b>  Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="9.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>9.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-8"><i class="fa fa-check"></i><b>9.2</b> Summary</a></li>
<li class="chapter" data-level="9.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-6"><i class="fa fa-check"></i><b>9.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>10</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>10.1</b> A change of variables with the reciprocal normal distribution</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>10.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>10.2</b>  Validation of a computed posterior distribution</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>10.2.1</b> The  simulation-based calibration procedure</a></li>
<li class="chapter" data-level="10.2.2" data-path="ch-custom.html"><a href="ch-custom.html#an-example-where-simulation-based-calibration-reveals-a-problem"><i class="fa fa-check"></i><b>10.2.2</b> An example where simulation-based calibration reveals a problem</a></li>
<li class="chapter" data-level="10.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-with-and-limitations-of-simulation-based-calibration"><i class="fa fa-check"></i><b>10.2.3</b> Issues with and limitations of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-the-exponential-distribution-implemented-manually"><i class="fa fa-check"></i><b>10.3</b> Another  custom distribution: The exponential distribution  implemented manually</a></li>
<li class="chapter" data-level="10.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-9"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
<li class="chapter" data-level="10.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-7"><i class="fa fa-check"></i><b>10.5</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="11" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>11</b>  Meta-analysis and  measurement error models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>11.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>11.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>11.2</b>  Measurement-error models</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>11.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-10"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-8"><i class="fa fa-check"></i><b>11.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison</b></span></li>
<li class="chapter" data-level="12" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>12</b> Introduction to model comparison</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-comparison.html"><a href="ch-comparison.html#prior-predictive-vs.-posterior-predictive-model-comparison"><i class="fa fa-check"></i><b>12.1</b> Prior predictive vs. posterior predictive model comparison</a></li>
<li class="chapter" data-level="12.2" data-path="ch-comparison.html"><a href="ch-comparison.html#some-important-points-to-consider-when-comparing-models"><i class="fa fa-check"></i><b>12.2</b> Some important points to consider when comparing models</a></li>
<li class="chapter" data-level="12.3" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-9"><i class="fa fa-check"></i><b>12.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>13</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>13.1</b> Hypothesis testing using the Bayes factor</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>13.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="13.1.2" data-path="ch-bf.html"><a href="ch-bf.html#the-bayes-factor"><i class="fa fa-check"></i><b>13.1.2</b> The Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>13.2</b> Examining the N400 effect with the Bayes factor</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>13.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="13.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>13.2.2</b>  Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>13.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="13.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>13.4</b>  The Bayes factor in Stan</a></li>
<li class="chapter" data-level="13.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>13.5</b> Bayes factors in theory and in practice</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>13.5.1</b> Bayes factors in theory: Stability and  accuracy</a></li>
<li class="chapter" data-level="13.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>13.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="13.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>13.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="ch-bf.html"><a href="ch-bf.html#sample-size-determination-using-bayes-factors"><i class="fa fa-check"></i><b>13.6</b> Sample size determination using Bayes factors</a></li>
<li class="chapter" data-level="13.7" data-path="ch-bf.html"><a href="ch-bf.html#summary-11"><i class="fa fa-check"></i><b>13.7</b> Summary</a></li>
<li class="chapter" data-level="13.8" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-10"><i class="fa fa-check"></i><b>13.8</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>14</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>14.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="14.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>14.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="14.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>14.3</b> Testing the N400 effect using cross-validation</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>14.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="14.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>14.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="14.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>14.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>14.4</b>  Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="14.5" data-path="ch-cv.html"><a href="ch-cv.html#sec-issuesCV"><i class="fa fa-check"></i><b>14.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="14.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>14.6</b> Cross-validation in Stan</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>14.6.1</b>  PSIS-LOO-CV in Stan</a></li>
<li class="chapter" data-level="14.6.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-cv-in-stan"><i class="fa fa-check"></i><b>14.6.2</b>  K-fold-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-12"><i class="fa fa-check"></i><b>14.7</b> Summary</a></li>
<li class="chapter" data-level="14.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-11"><i class="fa fa-check"></i><b>14.8</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>VI Cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="15" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>15</b> Introduction to cognitive modeling</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#what-characterizes-a-computational-cognitive-model"><i class="fa fa-check"></i><b>15.1</b> What characterizes a computational cognitive model?</a></li>
<li class="chapter" data-level="15.2" data-path="ch-cogmod.html"><a href="ch-cogmod.html#some-advantages-of-taking-the-latent-variable-modeling-approach"><i class="fa fa-check"></i><b>15.2</b> Some advantages of taking the latent-variable modeling approach</a></li>
<li class="chapter" data-level="15.3" data-path="ch-cogmod.html"><a href="ch-cogmod.html#types-of-computational-cognitive-model"><i class="fa fa-check"></i><b>15.3</b> Types of computational cognitive model</a></li>
<li class="chapter" data-level="15.4" data-path="ch-cogmod.html"><a href="ch-cogmod.html#summary-13"><i class="fa fa-check"></i><b>15.4</b> Summary</a></li>
<li class="chapter" data-level="15.5" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-12"><i class="fa fa-check"></i><b>15.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>16</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>16.1</b> Modeling  multiple categorical responses</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>16.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="16.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>16.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>16.2</b> Modeling picture naming abilities in aphasia with MPT models</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>16.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="16.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>16.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="16.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>16.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="16.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>16.2.4</b> A  hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-14"><i class="fa fa-check"></i><b>16.3</b> Summary</a></li>
<li class="chapter" data-level="16.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-13"><i class="fa fa-check"></i><b>16.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>17</b> Mixture models</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>17.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>17.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="17.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>17.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="17.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>17.1.3</b> A  multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="17.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>17.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="17.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>17.1.5</b> A  hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-15"><i class="fa fa-check"></i><b>17.2</b> Summary</a></li>
<li class="chapter" data-level="17.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-14"><i class="fa fa-check"></i><b>17.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>18</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>18.1</b> Modeling a lexical decision task</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>18.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>18.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="18.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>18.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="18.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>18.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="18.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>18.1.5</b> Dealing with  contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>18.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="18.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-16"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-closing.html"><a href="ch-closing.html"><i class="fa fa-check"></i><b>19</b> In closing</a></li>
<li class="appendix"><span><b>Online materials</b></span></li>
<li class="chapter" data-level="A" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html"><i class="fa fa-check"></i><b>A</b> Regression models with <code>brms</code> - Extended</a>
<ul>
<li class="chapter" data-level="A.1" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-efficientpriorpd"><i class="fa fa-check"></i><b>A.1</b> An efficient function for generating prior predictive distributions in R</a></li>
<li class="chapter" data-level="A.2" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-truncation"><i class="fa fa-check"></i><b>A.2</b> Truncated distributions</a></li>
<li class="chapter" data-level="A.3" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-intercept"><i class="fa fa-check"></i><b>A.3</b> Intercepts in <code>brms</code></a></li>
<li class="chapter" data-level="A.4" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-lognormal"><i class="fa fa-check"></i><b>A.4</b> Understanding the log-normal likelihood</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#log-normal-distributions-everywhere"><i class="fa fa-check"></i><b>A.4.1</b> Log-normal distributions everywhere</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-priorR"><i class="fa fa-check"></i><b>A.5</b> Prior predictive checks in R</a></li>
<li class="chapter" data-level="A.6" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-exch"><i class="fa fa-check"></i><b>A.6</b> Finitely exchangeable random variables</a></li>
<li class="chapter" data-level="A.7" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-matrixHierachicalModel"><i class="fa fa-check"></i><b>A.7</b> The Matrix Formulation of Hierarchical Models (the Laird-Ware form)</a></li>
<li class="chapter" data-level="A.8" data-path="regression-models-with-brms---extended.html"><a href="regression-models-with-brms---extended.html#app-cTreatGM"><i class="fa fa-check"></i><b>A.8</b> Treatment contrast with intercept as the grand mean</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html"><i class="fa fa-check"></i><b>B</b> Advanced models with Stan - Extended</a>
<ul>
<li class="chapter" data-level="B.1" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-target"><i class="fa fa-check"></i><b>B.1</b> What does <code>target</code> do in Stan models?</a></li>
<li class="chapter" data-level="B.2" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-tilde"><i class="fa fa-check"></i><b>B.2</b> Explicitly incrementing the log probability function (<code>target</code>) vs. using the sampling or distribution <code>~</code> notation</a></li>
<li class="chapter" data-level="B.3" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-cmdstanr"><i class="fa fa-check"></i><b>B.3</b> An alternative R interface to Stan: <code>cmdstanr</code></a></li>
<li class="chapter" data-level="B.4" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-stancontainers"><i class="fa fa-check"></i><b>B.4</b> Matrix, vector, or array in Stan?</a></li>
<li class="chapter" data-level="B.5" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-noncenterparam"><i class="fa fa-check"></i><b>B.5</b> A simple non-centered parameterization</a></li>
<li class="chapter" data-level="B.6" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-cholesky"><i class="fa fa-check"></i><b>B.6</b> Cholesky factorization for reparameterizing hierarchical models with correlations between adjustments to different parameters</a></li>
<li class="chapter" data-level="B.7" data-path="advanced-models-with-stan---extended.html"><a href="advanced-models-with-stan---extended.html#app-sbc"><i class="fa fa-check"></i><b>B.7</b> Different rank visualizations and the <code>SBC</code> package.</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="evidence-synthesis-and-measurements-with-error---extended.html"><a href="evidence-synthesis-and-measurements-with-error---extended.html"><i class="fa fa-check"></i><b>C</b> Evidence synthesis and measurements with error - Extended</a>
<ul>
<li class="chapter" data-level="C.1" data-path="evidence-synthesis-and-measurements-with-error---extended.html"><a href="evidence-synthesis-and-measurements-with-error---extended.html#app-sigmatrue"><i class="fa fa-check"></i><b>C.1</b> What happens if we set <code>sigma = TRUE</code> in <code>resp_se()</code> function in <code>brms</code>?</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html"><i class="fa fa-check"></i><b>D</b> Model comparison - Extended</a>
<ul>
<li class="chapter" data-level="D.1" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-null"><i class="fa fa-check"></i><b>D.1</b> Credible intervals should not be used to reject a null hypothesis</a></li>
<li class="chapter" data-level="D.2" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-likR"><i class="fa fa-check"></i><b>D.2</b> The likelihood ratio vs the Bayes factor</a></li>
<li class="chapter" data-level="D.3" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-integral"><i class="fa fa-check"></i><b>D.3</b> Approximation of the (expected) log predictive density of a model without integration</a></li>
<li class="chapter" data-level="D.4" data-path="model-comparison---extended.html"><a href="model-comparison---extended.html#app-CV-alg"><i class="fa fa-check"></i><b>D.4</b> The cross-validation algorithm for the expected log predictive density of a model</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>E</b> The Art and Science of Prior Elicitation</a>
<ul>
<li class="chapter" data-level="E.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>E.1</b> Eliciting priors from oneself for a self-paced reading study: An example</a>
<ul>
<li class="chapter" data-level="E.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>E.1.1</b> An example: English  relative clauses</a></li>
<li class="chapter" data-level="E.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>E.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="E.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>E.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="E.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>E.1.4</b> Eliciting priors for the  variance components</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>E.2</b>  Eliciting priors from experts</a></li>
<li class="chapter" data-level="E.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>E.3</b> Deriving priors from  meta-analyses</a></li>
<li class="chapter" data-level="E.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>E.4</b> Using previous experiments’  posteriors as priors for a new study</a></li>
<li class="chapter" data-level="E.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-17"><i class="fa fa-check"></i><b>E.5</b> Summary</a></li>
<li class="chapter" data-level="E.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-16"><i class="fa fa-check"></i><b>E.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>F</b> Workflow</a>
<ul>
<li class="chapter" data-level="F.1" data-path="ch-workflow.html"><a href="ch-workflow.html#building-a-model"><i class="fa fa-check"></i><b>F.1</b>  Building a model</a></li>
<li class="chapter" data-level="F.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-to-ask-on-a-model"><i class="fa fa-check"></i><b>F.2</b> Principled questions to ask on a model</a>
<ul>
<li class="chapter" data-level="F.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#checking-whether-assumptions-are-consistent-with-domain-expertise-prior-predictive-checks"><i class="fa fa-check"></i><b>F.2.1</b>  Checking whether assumptions are consistent with  domain expertise: Prior predictive checks</a></li>
<li class="chapter" data-level="F.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#testing-for-correct-posterior-approximations-checks-of-computational-faithfulness"><i class="fa fa-check"></i><b>F.2.2</b>  Testing for correct posterior approximations: Checks of computational faithfulness</a></li>
<li class="chapter" data-level="F.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#sensitivity-of-the-model"><i class="fa fa-check"></i><b>F.2.3</b>  Sensitivity of the model</a></li>
<li class="chapter" data-level="F.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#does-the-model-adequately-capture-the-dataposterior-predictive-checks"><i class="fa fa-check"></i><b>F.2.4</b>  Does the model adequately capture the data?–Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="F.3" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-17"><i class="fa fa-check"></i><b>F.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>G</b> Exercises</a>
<ul>
<li class="chapter" data-level="G.1" data-path="exercises.html"><a href="exercises.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>G.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="G.1.1" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart1"><i class="fa fa-check"></i><b>G.1.1</b> Practice using the <code>pnorm()</code> function–Part 1</a></li>
<li class="chapter" data-level="G.1.2" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart2"><i class="fa fa-check"></i><b>G.1.2</b> Practice using the <code>pnorm()</code> function–Part 2</a></li>
<li class="chapter" data-level="G.1.3" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisespnormPart3"><i class="fa fa-check"></i><b>G.1.3</b> Practice using the <code>pnorm()</code> function–Part 3</a></li>
<li class="chapter" data-level="G.1.4" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisesqnormPart1"><i class="fa fa-check"></i><b>G.1.4</b> Practice using the <code>qnorm()</code> function–Part 1</a></li>
<li class="chapter" data-level="G.1.5" data-path="exercises.html"><a href="exercises.html#exr:FoundationsexercisesqnormPart2"><i class="fa fa-check"></i><b>G.1.5</b> Practice using the <code>qnorm()</code> function–Part 2</a></li>
<li class="chapter" data-level="G.1.6" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisessamples1"><i class="fa fa-check"></i><b>G.1.6</b> Practice getting summaries from samples–Part 1</a></li>
<li class="chapter" data-level="G.1.7" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisessamples2"><i class="fa fa-check"></i><b>G.1.7</b> Practice getting summaries from samples–Part 2.</a></li>
<li class="chapter" data-level="G.1.8" data-path="exercises.html"><a href="exercises.html#exr:Foundationsexercisesvcov1"><i class="fa fa-check"></i><b>G.1.8</b> Practice with a variance-covariance matrix for a bivariate distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.2" data-path="exercises.html"><a href="exercises.html#sec-BDAexercises"><i class="fa fa-check"></i><b>G.2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="G.2.1" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesDerivingBayes"><i class="fa fa-check"></i><b>G.2.1</b> Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="G.2.2" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj1"><i class="fa fa-check"></i><b>G.2.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="G.2.3" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj2"><i class="fa fa-check"></i><b>G.2.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="G.2.4" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj3"><i class="fa fa-check"></i><b>G.2.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="G.2.5" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesConj4"><i class="fa fa-check"></i><b>G.2.5</b> Conjugate forms 4</a></li>
<li class="chapter" data-level="G.2.6" data-path="exercises.html"><a href="exercises.html#exr:BDAexercisesWeightedMean"><i class="fa fa-check"></i><b>G.2.6</b> The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)</a></li>
</ul></li>
<li class="chapter" data-level="G.3" data-path="exercises.html"><a href="exercises.html#ex:compbda"><i class="fa fa-check"></i><b>G.3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="G.3.1" data-path="exercises.html"><a href="exercises.html#exr:simulatedlinearmod"><i class="fa fa-check"></i><b>G.3.1</b> Check for parameter recovery in a linear model using simulated data.</a></li>
<li class="chapter" data-level="G.3.2" data-path="exercises.html"><a href="exercises.html#exr:linearmod"><i class="fa fa-check"></i><b>G.3.2</b> A simple linear model.</a></li>
<li class="chapter" data-level="G.3.3" data-path="exercises.html"><a href="exercises.html#exr:compbda-biasedpost"><i class="fa fa-check"></i><b>G.3.3</b> Revisiting the button-pressing example with different priors.</a></li>
<li class="chapter" data-level="G.3.4" data-path="exercises.html"><a href="exercises.html#exr:ppd"><i class="fa fa-check"></i><b>G.3.4</b> Posterior predictive checks with a log-normal model.</a></li>
<li class="chapter" data-level="G.3.5" data-path="exercises.html"><a href="exercises.html#exr:skew"><i class="fa fa-check"></i><b>G.3.5</b> A skew normal distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.4" data-path="exercises.html"><a href="exercises.html#sec-LMexercises"><i class="fa fa-check"></i><b>G.4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="G.4.1" data-path="exercises.html"><a href="exercises.html#exr:powerposing"><i class="fa fa-check"></i><b>G.4.1</b> A simple linear regression: Power posing and testosterone.</a></li>
<li class="chapter" data-level="G.4.2" data-path="exercises.html"><a href="exercises.html#exr:pupils"><i class="fa fa-check"></i><b>G.4.2</b> Another linear regression model: Revisiting attentional load effect on pupil size.</a></li>
<li class="chapter" data-level="G.4.3" data-path="exercises.html"><a href="exercises.html#exr:lognormalm"><i class="fa fa-check"></i><b>G.4.3</b> Log-normal model: Revisiting the effect of trial on finger tapping times.</a></li>
<li class="chapter" data-level="G.4.4" data-path="exercises.html"><a href="exercises.html#exr:reg-logistic"><i class="fa fa-check"></i><b>G.4.4</b> Logistic regression: Revisiting the effect of set size on free recall.</a></li>
<li class="chapter" data-level="G.4.5" data-path="exercises.html"><a href="exercises.html#exr:red"><i class="fa fa-check"></i><b>G.4.5</b> Red is the sexiest color.</a></li>
</ul></li>
<li class="chapter" data-level="G.5" data-path="exercises.html"><a href="exercises.html#sec-HLMexercises"><i class="fa fa-check"></i><b>G.5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="G.5.1" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-normal"><i class="fa fa-check"></i><b>G.5.1</b> A hierarchical model (normal likelihood) of cognitive load on pupil size.</a></li>
<li class="chapter" data-level="G.5.2" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-logn"><i class="fa fa-check"></i><b>G.5.2</b> Are subject relatives easier to process than object relatives (log-normal likelihood)?</a></li>
<li class="chapter" data-level="G.5.3" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseMandarinRC"><i class="fa fa-check"></i><b>G.5.3</b> Relative clause processing in Mandarin Chinese</a></li>
<li class="chapter" data-level="G.5.4" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseEnglishAgrmt"><i class="fa fa-check"></i><b>G.5.4</b>  Agreement attraction in comprehension</a></li>
<li class="chapter" data-level="G.5.5" data-path="exercises.html"><a href="exercises.html#exr:ab"><i class="fa fa-check"></i><b>G.5.5</b>  Attentional blink (Bernoulli likelihood)</a></li>
<li class="chapter" data-level="G.5.6" data-path="exercises.html"><a href="exercises.html#exr:strooplogis-brms"><i class="fa fa-check"></i><b>G.5.6</b> Is there a Stroop effect in accuracy?</a></li>
<li class="chapter" data-level="G.5.7" data-path="exercises.html"><a href="exercises.html#exr:stroop-dist"><i class="fa fa-check"></i><b>G.5.7</b>  Distributional regression for the Stroop effect.</a></li>
<li class="chapter" data-level="G.5.8" data-path="exercises.html"><a href="exercises.html#exr:HLMExerciseGramCE"><i class="fa fa-check"></i><b>G.5.8</b> The  grammaticality illusion</a></li>
</ul></li>
<li class="chapter" data-level="G.6" data-path="exercises.html"><a href="exercises.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>G.6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="G.6.1" data-path="exercises.html"><a href="exercises.html#exr:ContrastsPersian"><i class="fa fa-check"></i><b>G.6.1</b> Contrast coding for a four-condition design</a></li>
<li class="chapter" data-level="G.6.2" data-path="exercises.html"><a href="exercises.html#exr:ContrastsNPIHelmert"><i class="fa fa-check"></i><b>G.6.2</b>  Helmert coding for a six-condition design.</a></li>
<li class="chapter" data-level="G.6.3" data-path="exercises.html"><a href="exercises.html#exr:ContrastsNcomparisons"><i class="fa fa-check"></i><b>G.6.3</b> Number of possible comparisons in a single model.</a></li>
</ul></li>
<li class="chapter" data-level="G.7" data-path="exercises.html"><a href="exercises.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>G.7</b> Contrast coding with two predictor variables</a>
<ul>
<li class="chapter" data-level="G.7.1" data-path="exercises.html"><a href="exercises.html#exr:ContrastsPersianANOVA"><i class="fa fa-check"></i><b>G.7.1</b> ANOVA coding for a four-condition design.</a></li>
<li class="chapter" data-level="G.7.2" data-path="exercises.html"><a href="exercises.html#exr:Contrasts2x2x2Dillon2013"><i class="fa fa-check"></i><b>G.7.2</b> ANOVA and nested comparisons in a <span class="math inline">\(2\times 2\times 2\)</span> design</a></li>
</ul></li>
<li class="chapter" data-level="G.8" data-path="exercises.html"><a href="exercises.html#introduction-to-the-probabilistic-programming-language-stan"><i class="fa fa-check"></i><b>G.8</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="G.8.1" data-path="exercises.html"><a href="exercises.html#exr:first"><i class="fa fa-check"></i><b>G.8.1</b> A very simple model.</a></li>
<li class="chapter" data-level="G.8.2" data-path="exercises.html"><a href="exercises.html#exr:badstan"><i class="fa fa-check"></i><b>G.8.2</b> Incorrect Stan model.</a></li>
<li class="chapter" data-level="G.8.3" data-path="exercises.html"><a href="exercises.html#exr:skewstan"><i class="fa fa-check"></i><b>G.8.3</b> Using Stan documentation.</a></li>
<li class="chapter" data-level="G.8.4" data-path="exercises.html"><a href="exercises.html#exr:linkfunction"><i class="fa fa-check"></i><b>G.8.4</b> The probit link function as an alternative to the logit function.</a></li>
<li class="chapter" data-level="G.8.5" data-path="exercises.html"><a href="exercises.html#exr:logisticstan"><i class="fa fa-check"></i><b>G.8.5</b> Examining the position of the queued word on recall.</a></li>
<li class="chapter" data-level="G.8.6" data-path="exercises.html"><a href="exercises.html#exr:fallacy"><i class="fa fa-check"></i><b>G.8.6</b> The conjunction fallacy.</a></li>
</ul></li>
<li class="chapter" data-level="G.9" data-path="exercises.html"><a href="exercises.html#hierarchical-models-and-reparameterization"><i class="fa fa-check"></i><b>G.9</b> Hierarchical models and reparameterization</a>
<ul>
<li class="chapter" data-level="G.9.1" data-path="exercises.html"><a href="exercises.html#exr:stroop"><i class="fa fa-check"></i><b>G.9.1</b> A log-normal model in Stan.</a></li>
<li class="chapter" data-level="G.9.2" data-path="exercises.html"><a href="exercises.html#exr:hierarchical-logn-stan"><i class="fa fa-check"></i><b>G.9.2</b> A by-subjects and by-items hierarchical model with a log-normal likelihood.</a></li>
<li class="chapter" data-level="G.9.3" data-path="exercises.html"><a href="exercises.html#exr:strooplogis"><i class="fa fa-check"></i><b>G.9.3</b> A hierarchical logistic regression with Stan.</a></li>
<li class="chapter" data-level="G.9.4" data-path="exercises.html"><a href="exercises.html#exr:distr-stan"><i class="fa fa-check"></i><b>G.9.4</b> A distributional regression model of the effect of cloze probability on the N400.</a></li>
</ul></li>
<li class="chapter" data-level="G.10" data-path="exercises.html"><a href="exercises.html#sec-customexercises"><i class="fa fa-check"></i><b>G.10</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="G.10.1" data-path="exercises.html"><a href="exercises.html#exr:shiftedlogn"><i class="fa fa-check"></i><b>G.10.1</b> Fitting a  shifted log-normal distribution.</a></li>
<li class="chapter" data-level="G.10.2" data-path="exercises.html"><a href="exercises.html#exr:wald"><i class="fa fa-check"></i><b>G.10.2</b> Fitting a Wald distribution.</a></li>
</ul></li>
<li class="chapter" data-level="G.11" data-path="exercises.html"><a href="exercises.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>G.11</b> Meta-analysis and measurement error models</a>
<ul>
<li class="chapter" data-level="G.11.1" data-path="exercises.html"><a href="exercises.html#exr:REMAMEExtracting"><i class="fa fa-check"></i><b>G.11.1</b> Extracting estimates from published papers</a></li>
<li class="chapter" data-level="G.11.2" data-path="exercises.html"><a href="exercises.html#exr:REMAMEBuerki"><i class="fa fa-check"></i><b>G.11.2</b> A meta-analysis of picture-word interference data</a></li>
<li class="chapter" data-level="G.11.3" data-path="exercises.html"><a href="exercises.html#exr:REMAMELiEnglish"><i class="fa fa-check"></i><b>G.11.3</b> Measurement error model for English VOT data</a></li>
</ul></li>
<li class="chapter" data-level="G.12" data-path="exercises.html"><a href="exercises.html#introduction-to-model-comparison"><i class="fa fa-check"></i><b>G.12</b> Introduction to model comparison</a></li>
<li class="chapter" data-level="G.13" data-path="exercises.html"><a href="exercises.html#bayes-factors"><i class="fa fa-check"></i><b>G.13</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="G.13.1" data-path="exercises.html"><a href="exercises.html#exr:bysubjects"><i class="fa fa-check"></i><b>G.13.1</b> Is there evidence for differences in the effect of cloze probability among the subjects?</a></li>
<li class="chapter" data-level="G.13.2" data-path="exercises.html"><a href="exercises.html#exr:bf-logn"><i class="fa fa-check"></i><b>G.13.2</b> Is there evidence for the claim that English subject relative clauses are easier to process than object relative clauses?</a></li>
<li class="chapter" data-level="G.13.3" data-path="exercises.html"><a href="exercises.html#exr:bf-logistic"><i class="fa fa-check"></i><b>G.13.3</b> In the Grodner and Gibson 2005 data, in question-response accuracies, is there evidence for the claim that sentences with subject relative clauses are easier to comprehend?</a></li>
<li class="chapter" data-level="G.13.4" data-path="exercises.html"><a href="exercises.html#exr:lognstan"><i class="fa fa-check"></i><b>G.13.4</b> Bayes factor and bounded parameters using Stan.</a></li>
</ul></li>
<li class="chapter" data-level="G.14" data-path="exercises.html"><a href="exercises.html#cross-validation"><i class="fa fa-check"></i><b>G.14</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="G.14.1" data-path="exercises.html"><a href="exercises.html#exr:logcv"><i class="fa fa-check"></i><b>G.14.1</b> Predictive accuracy of the linear and the logarithm effect of cloze probability.</a></li>
<li class="chapter" data-level="G.14.2" data-path="exercises.html"><a href="exercises.html#exr:stroopcv"><i class="fa fa-check"></i><b>G.14.2</b> Log-normal model</a></li>
<li class="chapter" data-level="G.14.3" data-path="exercises.html"><a href="exercises.html#exr:logrec"><i class="fa fa-check"></i><b>G.14.3</b> Log-normal vs rec-normal model in Stan</a></li>
</ul></li>
<li class="chapter" data-level="G.15" data-path="exercises.html"><a href="exercises.html#introduction-to-cognitive-modeling"><i class="fa fa-check"></i><b>G.15</b> Introduction to cognitive modeling</a></li>
<li class="chapter" data-level="G.16" data-path="exercises.html"><a href="exercises.html#multinomial-processing-trees"><i class="fa fa-check"></i><b>G.16</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="G.16.1" data-path="exercises.html"><a href="exercises.html#exr:mult"><i class="fa fa-check"></i><b>G.16.1</b> Modeling multiple categorical responses.</a></li>
<li class="chapter" data-level="G.16.2" data-path="exercises.html"><a href="exercises.html#exr:mpt-mnm"><i class="fa fa-check"></i><b>G.16.2</b> An alternative MPT to model the picture recognition task.</a></li>
<li class="chapter" data-level="G.16.3" data-path="exercises.html"><a href="exercises.html#exr:edit-mpt-cat"><i class="fa fa-check"></i><b>G.16.3</b> A simple MPT model that incorporates phonological complexity in the picture recognition task.</a></li>
<li class="chapter" data-level="G.16.4" data-path="exercises.html"><a href="exercises.html#exr:mpt"><i class="fa fa-check"></i><b>G.16.4</b> A more hierarchical MPT.</a></li>
<li class="chapter" data-level="G.16.5" data-path="exercises.html"><a href="exercises.html#exr:mpt-adv"><i class="fa fa-check"></i><b>G.16.5</b> <strong>Advanced</strong>: Multinomial processing trees.</a></li>
</ul></li>
<li class="chapter" data-level="G.17" data-path="exercises.html"><a href="exercises.html#mixture-models"><i class="fa fa-check"></i><b>G.17</b> Mixture models</a>
<ul>
<li class="chapter" data-level="G.17.1" data-path="exercises.html"><a href="exercises.html#exr:pcorrect"><i class="fa fa-check"></i><b>G.17.1</b> Changes in the true point values.</a></li>
<li class="chapter" data-level="G.17.2" data-path="exercises.html"><a href="exercises.html#exr:mixhier"><i class="fa fa-check"></i><b>G.17.2</b> RTs in schizophrenic patients and control.</a></li>
<li class="chapter" data-level="G.17.3" data-path="exercises.html"><a href="exercises.html#exr:mixbias"><i class="fa fa-check"></i><b>G.17.3</b> <strong>Advanced:</strong> Guessing bias in the model.</a></li>
</ul></li>
<li class="chapter" data-level="G.18" data-path="exercises.html"><a href="exercises.html#a-simple-accumulator-model-to-account-for-choice-response-time"><i class="fa fa-check"></i><b>G.18</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="G.18.1" data-path="exercises.html"><a href="exercises.html#exr:recovery"><i class="fa fa-check"></i><b>G.18.1</b> Can we recover the true point values of the parameters of a model when dealing with a contaminant distribution?</a></li>
<li class="chapter" data-level="G.18.2" data-path="exercises.html"><a href="exercises.html#exr:lnracescale"><i class="fa fa-check"></i><b>G.18.2</b> Can the log-normal race model account for fast errors?</a></li>
<li class="chapter" data-level="G.18.3" data-path="exercises.html"><a href="exercises.html#exr:lnldt"><i class="fa fa-check"></i><b>G.18.3</b> Accounting for response time and choice in the lexical decision task using the log-normal race model.</a></li>
</ul></li>
<li class="chapter" data-level="G.19" data-path="exercises.html"><a href="exercises.html#sec-priorsexercises"><i class="fa fa-check"></i><b>G.19</b> The Art and Science of  Prior Elicitation</a>
<ul>
<li class="chapter" data-level="G.19.1" data-path="exercises.html"><a href="exercises.html#exr:PriorsRCs"><i class="fa fa-check"></i><b>G.19.1</b> Develop a plausible informative prior for the difference between object and subject relative clause reading times</a></li>
<li class="chapter" data-level="G.19.2" data-path="exercises.html"><a href="exercises.html#exr:Priorslocalcoherence"><i class="fa fa-check"></i><b>G.19.2</b> Extracting an informative prior from a published paper for a future study</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-intro" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Introduction<a href="ch-intro.html#ch-intro" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The central idea we will explore in this book is how to use Bayes’ theorem to quantify uncertainty about our belief regarding a scientific question of interest, given some data. Before we delve into the details of the underlying theory and its application, it is important to have some familiarity with the following topics: basic concepts of probability, the concept of random variables, probability distributions, and the concept of likelihood. Therefore, we will begin with these topics.</p>
<p>Some of these concepts might seem abstract at first, but they are very relevant for conducting a Bayesian analysis. When reading this book for the first time, it might be helpful to do a quick pass through this chapter and return to it as needed while progressing through the rest of the book.</p>
<div id="introprob" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Probability<a href="ch-intro.html#introprob" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Informally, we all understand what the term  <em>probability</em> means. We routinely talk about things like the probability of it raining today. However, there are two distinct ways to think about probability. One can think of the probability of something happening with reference to the  frequency with which it might occur in repeated observations. Such a conception of probability is easy to imagine in cases where something can, at least in principle, occur repeatedly.</p>
<p>An example would be obtaining a 6 when tossing a die again and again. However, this frequentist view of probability is difficult to justify when talking about one-of-a-kind things, such as earthquakes; here, probability is expressing our uncertainty about the earthquake happening.</p>
<p>Both the frequency-based and the uncertain-belief perspective have their place in statistical inference, and depending on the situation, we are going to rely on both ways of thinking.</p>
<p>The statements below are not formal definitions of the axioms of probability theory; for more details (and more precise formulations), see <span class="citation">Blitzstein and Hwang (<a href="#ref-blitzstein2014introduction" role="doc-biblioref">2014</a>)</span>, <span class="citation">Ross (<a href="#ref-RossProb" role="doc-biblioref">2002</a>)</span>, <span class="citation">Kerns (<a href="#ref-kerns2014introduction" role="doc-biblioref">2014</a>)</span>, <span class="citation">Resnick (<a href="#ref-resnick2019probability" role="doc-biblioref">2019</a>)</span>, or <span class="citation">Kolmogorov (<a href="#ref-kolmogorov2018foundations" role="doc-biblioref">1933</a>)</span> (among many other books).
Keep in mind that different textbooks have slightly different ways of presenting the underlying structure of what constitutes a probability space (defined below).</p>
<p>The probability of something happening is defined to be constrained in the way described below. A concrete example of “something happening” is an outcome–call it <span class="math inline">\(\omega\)</span>–such as obtaining nine correct (<span class="math inline">\(c\)</span>) answers and one incorrect (<span class="math inline">\(i\)</span>) when we ask a subject <span class="math inline">\(10\)</span> yes-no questions (say, about the meaning of a sentence). An example would be a sequence of correct (<span class="math inline">\(c\)</span>) and incorrect (<span class="math inline">\(i\)</span>) answers, such as <span class="math inline">\(iiiiciiiii\)</span>. Another possible outcome is <span class="math inline">\(cciicicici\)</span>. The outcomes are thus all possible sequences of correct and incorrect answers, and the sample space, <span class="math inline">\(\Omega\)</span>, is the set of all possible outcomes.</p>
<p>Another important concept is events, <span class="math inline">\(E\)</span>, next; the event corresponding to obtaining one correct answer when <span class="math inline">\(10\)</span> questions are asked, is the subset of outcomes <span class="math inline">\(\{ciiiiiiiii,iciiiiiiii,iiciiiiiii,\ldots\}\)</span>; in other words, the event corresponding to obtaining one correct answer is a set containing <span class="math inline">\(10\)</span> elements. Similarly, the event corresponding to obtaining nine correct answers when <span class="math inline">\(10\)</span> questions are asked is the subset of outcomes <span class="math inline">\(\{ccccccccci,ccccccccic,\ldots \}\)</span>.</p>
<p>When we conduct an experiment, if we get a particular outcome like <span class="math inline">\(ciiiiiiiii\)</span>, then we say that the event <span class="math inline">\(\{ciiiiiiiii,iciiiiiiii,iiciiiiiii,\ldots\}\)</span> occurred.
A probability space involves two more key ingredients: A collection of subsets of <span class="math inline">\(\Omega\)</span> denoted by <span class="math inline">\(F\)</span> and called the <em>event space</em>;<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> and a real-valued function named <span class="math inline">\(P\)</span> that assigns a number (a probability) to each set in <span class="math inline">\(F\)</span>.</p>
<p>Table <a href="ch-intro.html#tab:probtab">1.1</a> summarizes the notation.</p>
<table>
<caption>
<span id="tab:probtab">TABLE 1.1: </span><span id="tab:probtab">TABLE 1.2: </span>Basic Probability Terminology for the 10-question example.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Symbol
</th>
<th style="text-align:left;">
Name
</th>
<th style="text-align:left;">
Brief explanation
</th>
<th style="text-align:left;">
Example
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\Omega\)</span>
</td>
<td style="text-align:left;">
Sample space
</td>
<td style="text-align:left;width: 3.5cm; ">
The set of all possible outcomes
</td>
<td style="text-align:left;width: 5cm; ">
All possible sequences of correct (c) or incorrect (i) answers of length 10; that is <span class="math inline">\(2^{10}=1024\)</span> sequences.
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(F\)</span>
</td>
<td style="text-align:left;">
Event space
</td>
<td style="text-align:left;width: 3.5cm; ">
A collection of subsets of <span class="math inline">\(\Omega\)</span>
</td>
<td style="text-align:left;width: 5cm; ">
All sequences with exactly 1 correct answer, or exactly 9 correct answers, etc.
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(P\)</span>
</td>
<td style="text-align:left;">
Probability fun.
</td>
<td style="text-align:left;width: 3.5cm; ">
Assigns probabilities to events in <span class="math inline">\(F\)</span>
</td>
<td style="text-align:left;width: 5cm; ">
If each question is answered correctly with probability <span class="math inline">\(\theta\)</span>, <span class="math inline">\(P(\{ciiiiiiiii\}) = (1-\theta)^9\theta\)</span>.
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\omega\)</span>
</td>
<td style="text-align:left;">
Outcome
</td>
<td style="text-align:left;width: 3.5cm; ">
An element of <span class="math inline">\(\Omega\)</span>
</td>
<td style="text-align:left;width: 5cm; ">
The particular sequence <span class="math inline">\(ciiiiiiiii\)</span>.
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(E\)</span>
</td>
<td style="text-align:left;">
Event
</td>
<td style="text-align:left;width: 3.5cm; ">
A subset of <span class="math inline">\(\Omega\)</span>
</td>
<td style="text-align:left;width: 5cm; ">
A collection of sequences <span class="math inline">\(\{ciiiiiiiii, iciiiiiiii, iiciiiiiii, \ldots\}\)</span>.
</td>
</tr>
</tbody>
</table>
<p>The probability axioms refer to the sample space <span class="math inline">\(\Omega\)</span>, event space <span class="math inline">\(F\)</span>, and probability <span class="math inline">\(P\)</span> as follows:</p>
<ul>
<li>For every event <span class="math inline">\(E\)</span> in the event space <span class="math inline">\(F\)</span>, the probability <span class="math inline">\(P(E)\)</span> is a real number between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</li>
<li>The event <span class="math inline">\(E=\Omega\)</span> belongs to <span class="math inline">\(F\)</span>, and <span class="math inline">\(P(\Omega)=1\)</span>.</li>
<li>If the events <span class="math inline">\(A_1, A_2, A_3,...\)</span> are mutually exclusive (in other words, if no two of these subsets of <span class="math inline">\(\Omega\)</span> overlap), then the probability of the event “one of <span class="math inline">\(A_1\)</span> or <span class="math inline">\(A_2\)</span> or <span class="math inline">\(A_3\)</span> or …” is given by the sum of the probability of <span class="math inline">\(A_1\)</span> occurring, of <span class="math inline">\(A_2\)</span> occurring, of <span class="math inline">\(A_3\)</span> occurring, … (this sum could be finite or infinite).</li>
</ul>
<p>Together, the triplet <span class="math inline">\((\Omega, F, P)\)</span> is called a <em>probability space</em>.</p>
<p>In the context of data analysis, we will talk about probability in the following way. Consider some data that we might have collected. This could be discrete <span class="math inline">\(0,1\)</span> responses in a question-response accuracy task, or continuous measurements of reading times in milliseconds from an eyetracking study, or multi-category responses like “yes”, “no”, or “don’t know”, or electrical potentials on the microvolts scale.</p>
<p>In any such case, we will say that the data are being generated from a  <em>random variable</em>, which we will designate with a capital letter such as <span class="math inline">\(Y\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>The actually observed outcome (a <span class="math inline">\(0,1\)</span> response; reading time, a response like “yes”, “no”, “don’t know”, etc.) will be distinguished from the random variable that generated it by using lower case <span class="math inline">\(y\)</span> for the observed outcome. We can call <span class="math inline">\(y\)</span> an instance of <span class="math inline">\(Y\)</span>; every new observed outcome can be different due to random variability.</p>
<p>We can summarize the above informal concepts relating to random variables very compactly if we re-state them in mathematical form. A mathematical statement has the advantage not only of brevity but also of reducing (and hopefully eliminating) ambiguity.</p>
<p>So, stating the definition of random variables formally <span class="citation">(following Blitzstein and Hwang <a href="#ref-blitzstein2014introduction" role="doc-biblioref">2014</a>)</span>, we define a random variable <span class="math inline">\(Y\)</span> as a function from a sample space <span class="math inline">\(\Omega\)</span> of possible  outcomes <span class="math inline">\(\omega\)</span> to the real number system:<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p><span class="math display">\[\begin{equation}
Y : \Omega \rightarrow \mathbb{R}
\end{equation}\]</span></p>
<p>The random variable associates to each outcome <span class="math inline">\(\omega \in \Omega\)</span> exactly one number <span class="math inline">\(Y(\omega) = y\)</span>. The number <span class="math inline">\(y\)</span> is a variable that represents all the possible values that the random variable generates; these values are taken to belong to the support of the random variable <span class="math inline">\(Y\)</span>: <span class="math inline">\(y \in S_Y\)</span>.</p>
<p>In our running example of asking <span class="math inline">\(10\)</span> questions and obtaining a correct or incorrect response in each of the <span class="math inline">\(10\)</span> trials, <span class="math inline">\(Y(ciiiiiiiii) = 1\)</span>, <span class="math inline">\(Y(cciiiiiiii) = 2\)</span>, etc. We will say that the number of correct responses from a subject is generated from a random variable <span class="math inline">\(Y\)</span>. Because in our running example only discrete responses are possible (the number of correct responses can be <span class="math inline">\(0, 1, 2, \ldots, 10\)</span>), this is an example of a <em>discrete random variable</em>.</p>
<p>This particular random variable <span class="math inline">\(Y\)</span> will be assumed to have a parameter <span class="math inline">\(\theta\)</span> that represents the probability of producing a particular number of correct responses (as discussed below, you will see that the random variable in question is the binomial random variable). Given some observed data that is assumed to come from a particular distribution, typically our goal is to obtain an estimate of the (unknown) value of the parameter associated with that distribution. More generally, if there is more than one parameter involved in the distribution, then our goal is to obtain an estimate of these parameters.</p>
<p>Now, suppose that in our above example, the random variable <span class="math inline">\(Y\)</span> gives us exactly one correct answer; this can be (somewhat sloppily) be written as <span class="math inline">\(Y=1\)</span>. The outcomes that could produce <span class="math inline">\(1\)</span> are any of this set of ten possible outcomes: <span class="math inline">\(ciiiiiiiii\)</span>, <span class="math inline">\(iciiiiiiii\)</span>, <span class="math inline">\(iiciiiiiii\)</span>, <span class="math inline">\(iiiciiiiii\)</span>,….
The set <span class="math inline">\(\{ciiiiiiiii,iciiiiiiii,iiciiiiiii,iiiciiiiii,...\}\)</span> is an element of <span class="math inline">\(F\)</span>, so it is an event.
The number <span class="math inline">\(y=1\)</span> thus represents this event, and will have a probability of occurring associated with it; this is defined next.</p>
<p>A discrete random variable <span class="math inline">\(Y\)</span> has associated with it a function called a  <em>probability mass function</em> or PMF. This function, which is written <span class="math inline">\(p(y)\)</span>, gives us the probability of obtaining each of these <span class="math inline">\(11\)</span> possible values (from 0 correct responses to 10). We are using lower-case <span class="math inline">\(p(y)\)</span> here to denote a function of <span class="math inline">\(y\)</span>. When we want to talk about the probability of observing <span class="math inline">\(y\)</span>, we will use <span class="math inline">\(P(y)\)</span>. In discrete random variables the value <span class="math inline">\(p(y)\)</span> will be the same as <span class="math inline">\(P(y)\)</span>; but when we turn to continuous random variables, this equality will not hold.</p>
<p>We will write that this PMF <span class="math inline">\(p(y)\)</span> depends on, or is conditional on, a particular fixed but unknown value for <span class="math inline">\(\theta\)</span>; the PMF will be written <span class="math inline">\(p(y|\theta)\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>In frequentist approaches to data analysis, only the observed data <span class="math inline">\(y\)</span> are used to draw inferences about <span class="math inline">\(\theta\)</span>. A typical question that we ask in the frequentist paradigm is: does <span class="math inline">\(\theta\)</span> have a particular value <span class="math inline">\(\theta_0\)</span>? One can obtain estimates of the unknown value of <span class="math inline">\(\theta\)</span> from the observed data <span class="math inline">\(y\)</span>, and then draw inferences about how different–or more precisely how far away–this estimate is from the hypothesized <span class="math inline">\(\theta_0\)</span>. This is the essence of null hypothesis significance testing. The conclusions from such a procedure are framed in terms of either rejecting the hypothesis that <span class="math inline">\(\theta\)</span> has value <span class="math inline">\(\theta_0\)</span>, or failing to reject this hypothesis. Here, rejecting the null hypothesis is the primary goal of the statistical hypothesis test.</p>
<p>Bayesian data analysis begins with a different question. What is common to the frequentist paradigm is the assumption that the data are generated from a random variable <span class="math inline">\(Y\)</span> and that there is a function <span class="math inline">\(p(y|\theta)\)</span> that depends on the parameter <span class="math inline">\(\theta\)</span>. Where the Bayesian approach diverges from the frequentist one is that an important goal is to express our uncertainty about <span class="math inline">\(\theta\)</span>. In other words, we treat the parameter <span class="math inline">\(\theta\)</span> itself as a random variable, which means that we assign a probability distribution <span class="math inline">\(p(\theta)\)</span> to this random variable. This distribution <span class="math inline">\(p(\theta)\)</span> is called the  <em>prior distribution</em> on <span class="math inline">\(\theta\)</span>; such a distribution could express our belief about the probability of correct responses, before we observe the data <span class="math inline">\(y\)</span>.</p>
<p>In later chapters, we will spend some time trying to understand how such a prior distribution can be defined for a range of different research problems.</p>
<p>Given such a prior distribution and some data <span class="math inline">\(y\)</span>, the end-product of a Bayesian data analysis is what is called the  <em>posterior distribution</em> of the parameter (or parameters) given the data: <span class="math inline">\(p(\theta | y)\)</span>. This posterior distribution is the probability distribution of <span class="math inline">\(\theta\)</span> after conditioning on <span class="math inline">\(y\)</span>, i.e., after the data has been observed and is therefore known. All our statistical inference is based on this posterior distribution of <span class="math inline">\(\theta\)</span>; we can even carry out hypothesis tests that are analogous (but not identical) to the likelihood ratio based frequentist hypothesis tests.</p>
<p>We already mentioned conditional probability above when discussing the probability of the data given some parameter <span class="math inline">\(\theta\)</span>, which we wrote as the PMF <span class="math inline">\(p(y|\theta)\)</span>. Conditional probability is an important concept in Bayesian data analysis, not least because it allows us to derive Bayes’ theorem. Let’s look at the definition of conditional probability next.</p>
</div>
<div id="condprob" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span>  Conditional probability<a href="ch-intro.html#condprob" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose that <span class="math inline">\(A\)</span> stands for some discrete event; an example would be “the streets are wet.” Suppose also that <span class="math inline">\(B\)</span> stands for some other discrete event; an example is “it has been raining.” We can talk about the probability of the streets being wet given that it has been raining; or more generally, the probability of <span class="math inline">\(A\)</span> given that <span class="math inline">\(B\)</span> has happened.</p>
<p>This kind of statement is written as <span class="math inline">\(Prob(A|B)\)</span> or more simply <span class="math inline">\(P(A|B)\)</span>. This is the conditional probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>. Conditional probability is defined as follows.</p>
<p><span class="math display">\[\begin{equation}
P(A|B)= \frac{P(A,B)}{P(B)} \hbox{ where } P(B)&gt;0
\end{equation}\]</span></p>
<p>The conditional probability of A given B is thus defined to be the joint probability of A and B, divided by the probability of B.
We can rearrange the above equation so that we can talk about the  joint probability of both events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> happening. This joint probability can be computed by first taking <span class="math inline">\(P(B)\)</span>, the probability that event <span class="math inline">\(B\)</span> (it has been raining) happens, and multiplying this by the probability that <span class="math inline">\(A\)</span> happens conditional on <span class="math inline">\(B\)</span>, i.e., the probability that the streets are wet given it has been raining. This multiplication will give us <span class="math inline">\(P(A,B)\)</span>, the joint probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, i.e., that it has been raining and that the streets are wet. We will write the above description as: <span class="math inline">\(P(A,B)=P(A|B)P(B)\)</span>.</p>
<p>Now, since the probability <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> happening is the same as the probability of <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> happening, i.e., since <span class="math inline">\(P(B,A)=P(A,B)\)</span>, we can equate the expansions of these two terms:</p>
<p><span class="math display">\[\begin{equation}
P(A,B) = P(A|B) P(B) \hbox{ and } P(B,A) = P(B|A)P(A)
\end{equation}\]</span></p>
<p>Equating the two expansions, we get:</p>
<p><span class="math display">\[\begin{equation}
P(A|B) P(B) = P(B|A)P(A)
\end{equation}\]</span></p>
<p>Dividing both sides by <span class="math inline">\(P(B)\)</span>:</p>
<p><span class="math display">\[\begin{equation}
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
\end{equation}\]</span></p>
<p>The above statement is  Bayes’ rule, and is the basis for all the statistical inference we will do in this book.</p>
</div>
<div id="the-law-of-total-probability" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> The  law of total probability<a href="ch-intro.html#the-law-of-total-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Related to the above discussion of conditional probability is the law of total probability. Suppose that we have <span class="math inline">\(A_1,\dots,A_n\)</span> distinct events that are pairwise disjoint which together make up the entire sample space <span class="math inline">\(\Omega\)</span>; see Figure <a href="ch-intro.html#fig:lotp">1.1</a>. Then, <span class="math inline">\(P(B)\)</span>, the probability of <span class="math inline">\(B\)</span> happening, will be the sum of the probabilities <span class="math inline">\(P(B\cap A_i)\)</span>, i.e., the sum of the joint probabilities of <span class="math inline">\(B\)</span> and each <span class="math inline">\(A\)</span> occurring (the symbol <span class="math inline">\(\cap\)</span> is the “and” operator used in set theory). The use of <span class="math inline">\(\cap\)</span> to represent joint probability is just an alternative notation to the one we used earlier (<span class="math inline">\(P(B,A_i)\)</span>); you may see both notational variants in textbooks.</p>
<p>Formally:</p>
<p><span class="math display">\[\begin{equation}
P(B) = \sum_{i=1}^n P(B \cap A_i)
\end{equation}\]</span></p>
<p>Because of the conditional probability rule, we can rewrite this as:</p>
<p><span class="math display">\[\begin{equation}
P(B) = \sum_{i=1}^n P(B | A_i) P(A_i)
\end{equation}\]</span></p>
<p>Thus, the probability of <span class="math inline">\(B\)</span> is the sum of the conditional probabilities <span class="math inline">\(P(B|A_i)\)</span> weighted by the probability <span class="math inline">\(P(A_i)\)</span>. We will see the law of total probability in action below when we talk about <em>marginal likelihood</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lotp"></span>
<img src="cc_figure/LOTPnoshadow.png" alt="An illustration of the law of total probability." width="80%" />
<p class="caption">
FIGURE 1.1: An illustration of the law of total probability.
</p>
</div>
<p>For now, this is all the probability theory we need to know!</p>
<p>The next sections expand on the idea of a random variable, the probability distributions associated with the random variable, what it means to specify a prior distribution on a parameter, and how the prior and data can be used to derive the posterior distribution of <span class="math inline">\(\theta\)</span>.</p>
<p>To make the discussion concrete, we will use an example of a discrete random variable, the binomial. After discussing this discrete random variable, we present another example, this time involving a continuous random variable, the normal random variable.</p>
<p>The binomial and normal cases serve as the canonical examples that we will need in the initial stages of this book. We will introduce other random variables as needed: in particular, we will need the uniform and beta distributions. In other textbooks, you will encounter distributions like the Poisson, gamma, and the exponential. The most commonly used distributions and their properties are discussed in most textbooks on statistics (see Further Reading at the end of this chapter).</p>
</div>
<div id="sec-binomialcloze" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span>  Discrete random variables: An example using the  binomial distribution<a href="ch-intro.html#sec-binomialcloze" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider the following sentence:</p>
<p><em>“It’s raining, I’m going to take the ….”</em></p>
<p>Suppose that our research goal is to estimate the probability, call it <span class="math inline">\(\theta\)</span>, of the word “umbrella” appearing in this sentence, versus any other word. If the sentence is completed with the word “umbrella”, we will refer to it as a success; any other completion will be referred to as a failure. This is an example of a binomial random variable: given <span class="math inline">\(n\)</span> trials, there can be only two possible outcomes in each trial, a success or a failure, and there is some true unknown probability <span class="math inline">\(\theta\)</span> of success that we want to estimate. When the number of trials <span class="math inline">\(n\)</span> is one, the random variable is called a  Bernoulli distribution. So the Bernoulli distribution is the binomial distribution with the number of trials <span class="math inline">\(n=1\)</span>.</p>
<p>One way to empirically estimate this probability of success is to carry out a  <em>cloze task</em>. In a cloze task, subjects are asked to complete a fragment of the original sentence, such as “It’s raining, I’m going to take the …”. The predictability or cloze probability of “umbrella” is then calculated as the proportion of times that the target word “umbrella” was produced as an answer by subjects.</p>
<p>Assume for simplicity that <span class="math inline">\(10\)</span> subjects are asked to complete the above sentence; each subject does this task only once. This gives us <span class="math inline">\(10\)</span> independent trials that are either coded a success (“umbrella” was produced) or as a failure (some other word was produced). We can sum up the number of successes to calculate how many of the <span class="math inline">\(10\)</span> trials had “umbrella” as a response. For example, if <span class="math inline">\(8\)</span> instances of “umbrella” are produced in <span class="math inline">\(10\)</span> trials, we would estimate the cloze probability of producing “umbrella” to be <span class="math inline">\(8/10\)</span>.</p>
<p>We can repeatedly generate simulated sequences of the number of successes in R (later on we will demonstrate how to generate such random sequences of simulated data). Here is a case where we carry out <span class="math inline">\(20\)</span> experiments, and each experiment will have <span class="math inline">\(10\)</span> trials.</p>
<p>Before we look at the R code for generating such simulated data, some notational conventions are needed to avoid confusion. In the function we use below (<code>rbinom()</code>) for generating simulated data, <code>n</code> refers to the number of experiments, which in the R function’s terminology is the number of observations. By contrast, <code>size</code> is the number of trials. This means that in the example below, <code>n</code> refers to the number of experiments conducted; because in R-speak these are called observations, we are also going to adopt this terminology. So, if <span class="math inline">\(20\)</span> experiments are done, each with <span class="math inline">\(10\)</span> trials, we will say that we have <span class="math inline">\(20\)</span> observations, each with <span class="math inline">\(10\)</span> trials.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb2-1"><a href="ch-intro.html#cb2-1" aria-hidden="true"></a><span class="co">## n=the number of observations (no. of experiments)</span></span>
<span id="cb2-2"><a href="ch-intro.html#cb2-2" aria-hidden="true"></a><span class="co">## size=the number of trials in each observation</span></span>
<span id="cb2-3"><a href="ch-intro.html#cb2-3" aria-hidden="true"></a><span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="dv">20</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>##  [1]  3  8  5  5  5  2 10  6  6  6  6  5  3  6  2  5  6  5  5  6</code></pre>
<p>The number of successes in each of the <span class="math inline">\(20\)</span> simulated observations above is being generated by a discrete random variable <span class="math inline">\(Y\)</span> with a probability distribution <span class="math inline">\(p(y|\theta)\)</span> called the <em>binomial distribution</em>.</p>
<p>For discrete random variables such as the binomial, the probability distribution <span class="math inline">\(p(y|\theta)\)</span> is called a  probability mass function  (PMF). The PMF defines the probability of each possible event. In the above example, with the number of trials <span class="math inline">\(\hbox{size}=10\)</span>, there are in principle <span class="math inline">\(11\)</span> possible events that could produce <span class="math inline">\(y=0,1,2,...,10\)</span> successes. Which of these is most probable depends on the parameter <span class="math inline">\(\theta\)</span> in the binomial distribution that represents the probability of success.</p>
<p>The left-hand side plot in Figure <a href="ch-intro.html#fig:binomplot">1.2</a> shows an example of a binomial PMF with <span class="math inline">\(10\)</span> trials, with the parameter <span class="math inline">\(\theta\)</span> fixed at <span class="math inline">\(0.5\)</span>. Setting <span class="math inline">\(\theta\)</span> to <span class="math inline">\(0.5\)</span> leads to a PMF where the most probable outcome is <span class="math inline">\(5\)</span> successes out of <span class="math inline">\(10\)</span>. If we had set <span class="math inline">\(\theta\)</span> to, say <span class="math inline">\(0.1\)</span>, then the most probable outcome would be <span class="math inline">\(1\)</span> success out of <span class="math inline">\(10\)</span>; and if we had set <span class="math inline">\(\theta\)</span> to <span class="math inline">\(0.9\)</span>, then the most probable outcome would be <span class="math inline">\(9\)</span> successes out of <span class="math inline">\(10\)</span>.</p>

<div class="figure"><span style="display:block;" id="fig:binomplot"></span>
<img src="bayescogsci_files/figure-html/binomplot-1.svg" alt="Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success." width="30%" /><img src="bayescogsci_files/figure-html/binomplot-2.svg" alt="Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success." width="30%" /><img src="bayescogsci_files/figure-html/binomplot-3.svg" alt="Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success." width="30%" />
<p class="caption">
FIGURE 1.2: Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success.
</p>
</div>
<p>The probability mass function for the binomial is written as follows. Here, it is again important to pay attention to the notation, as there is potential for confusion given the conventions in the <code>rbinom</code> function we saw earlier.</p>
<p><span class="math display">\[\begin{equation}
\mathit{Binomial}(k|n,\theta) =
\binom{n}{k} \theta^{k} (1-\theta)^{n-k}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(n\)</span> represents the total number of <strong>trials</strong>,
and corresponds to <code>size</code> in the <code>rbinom</code> function; the confusion that can arise here is that in the <code>rbinom</code> function <code>n</code> is used to represent the number of observations (or the number of experiments). It is important to remember that in the definition of the probability mass function above, <span class="math inline">\(n\)</span> represents the total number of trials in a single observation (or experiment).</p>
<p>The variable <span class="math inline">\(k\)</span> the number of successes (this could range from <span class="math inline">\(0\)</span> to <span class="math inline">\(10\)</span> in our running example), and <span class="math inline">\(\theta\)</span> the probability of success. The term <span class="math inline">\(\binom{n}{k}\)</span>, pronounced n-choose-k, represents the number of ways in which one can obtain <span class="math inline">\(k\)</span> successes in an experiment with <span class="math inline">\(n\)</span> trials. For example, <span class="math inline">\(1\)</span> success out of <span class="math inline">\(10\)</span> can occur in <span class="math inline">\(10\)</span> possible ways: the very first observation could be a <span class="math inline">\(1\)</span>, or the second observation could be a <span class="math inline">\(1\)</span>, etc.
The term <span class="math inline">\(\binom{n}{k}\)</span> expands to <span class="math inline">\(\frac{n!}{k!(n-k)!}\)</span>. In R, it is computed using the function <code>choose(n,k)</code>, with <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> representing any real number (although of course, the <code>choose()</code> function only makes sense for positive integers).</p>
<p>When we want to express the fact that the data is assumed to be generated from a binomial random variable, we will write <span class="math inline">\(Y \sim \mathit{Binomial}(n,\theta)\)</span>. If the data is generated from a random variable that has some other probability distribution <span class="math inline">\(f(\theta)\)</span>, we will write <span class="math inline">\(Y\sim f(\theta)\)</span>. In this book, we use <span class="math inline">\(f(\cdot)\)</span> synonymously with <span class="math inline">\(p(\cdot)\)</span> to represent a probability density/mass function.</p>
<div id="the-mean-and-variance-of-the-binomial-distribution" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> The mean and variance of the binomial distribution<a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is possible to analytically compute the mean (expectation) and variance of the PMF associated with the binomial random variable <span class="math inline">\(Y\)</span>.</p>
<p>The  expectation of a discrete random variable <span class="math inline">\(Y\)</span> with probability mass function f(y), is defined as</p>
<p><span class="math display">\[\begin{equation}
E[Y] = y_1 \cdot f(y_1) + \dots +  y_n \cdot f(y_n) = \sum_{i=1}^n y_i \cdot f(y_i)
\end{equation}\]</span></p>
<p>As a simple example, suppose that we toss a fair coin once. The possible events are Tails (represented as <span class="math inline">\(y_1 = 0\)</span>) and Heads (represented as <span class="math inline">\(y_2 = 1\)</span>), each with equal probability, 0.5. The expectation is:</p>
<p><span class="math display">\[\begin{equation}
E[Y] = \sum_{i=1}^{2} y_i \cdot f(y_i) = 0\cdot 0.5 + 1\cdot 0.5 = 0.5
\end{equation}\]</span></p>
<p>The expectation has the interpretation that if we were to do the experiment a large number of times and calculate the sample mean of the observations, in the long run we would approach the value <span class="math inline">\(0.5\)</span>. Another way to look at the above definition is that the expectation gives us the weighted mean of the possible outcomes, weighted by the respective probabilities of each outcome.</p>
<p>Without getting into the details of how these are derived mathematically <span class="citation">(Kerns <a href="#ref-kerns2014introduction" role="doc-biblioref">2014</a>)</span>, we just state here that the mean of <span class="math inline">\(Y\)</span> (the expectation <span class="math inline">\(E[Y]\)</span>) and the variance of <span class="math inline">\(Y\)</span> (written <span class="math inline">\(Var(Y)\)</span>) of a binomial distribution with parameter <span class="math inline">\(\theta\)</span> and <span class="math inline">\(n\)</span> trials are <span class="math inline">\(E[Y] = n\theta\)</span> and <span class="math inline">\(Var(Y) = n\theta (1-\theta)\)</span>, respectively.</p>
<p>In the binomial example above, <span class="math inline">\(n\)</span> is a fixed number because we decide on the total number of trials before running the experiment. In the PMF, <span class="math inline">\(\binom{n}{k} \theta^{k} (1-\theta)^{n-k}\)</span>, <span class="math inline">\(\theta\)</span> is also a fixed value; the only variable in a PMF is <span class="math inline">\(k\)</span>. In real experimental situations, we never know the true point value of <span class="math inline">\(\theta\)</span>. But <span class="math inline">\(\theta\)</span> can be estimated from the data. From the observed data, we can compute the estimate of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\hat \theta=k/n\)</span>. The quantity <span class="math inline">\(\hat \theta\)</span> is the observed proportion of successes, and is called the  <em>maximum likelihood estimate</em> of the true (but unknown) parameter <span class="math inline">\(\theta\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>What does the term “maximum likelihood estimate” mean? In order to understand this term, it is necessary to first understand what a  likelihood function is. Recall that in the discussion above, the PMF <span class="math inline">\(p(k|n,\theta)\)</span> assumes that <span class="math inline">\(\theta\)</span> and <span class="math inline">\(n\)</span> are fixed, and <span class="math inline">\(k\)</span> will have some value between <span class="math inline">\(0\)</span> and <span class="math inline">\(10\)</span> when the experiment is repeated multiple times.</p>
<p>The <em>likelihood function</em> refers to the PMF <span class="math inline">\(p(k|n,\theta)\)</span>, treated as a function of <span class="math inline">\(\theta\)</span>. Once we have observed a particular value for <span class="math inline">\(k\)</span>, this value is now fixed, along with the number of trials <span class="math inline">\(n\)</span>. Once <span class="math inline">\(k\)</span> and <span class="math inline">\(n\)</span> are fixed, the function <span class="math inline">\(p(k|n,\theta)\)</span> only depends on <span class="math inline">\(\theta\)</span>. Thus, the likelihood function is the same function as the PMF, but assumes that the data from the completed experiment is fixed and only the parameter <span class="math inline">\(\theta\)</span> varies (from 0 to 1). The likelihood function is written <span class="math inline">\(\mathcal{L}(\theta| k,n)\)</span>, or simply <span class="math inline">\(\mathcal{L}(\theta)\)</span>.</p>
<p>For example, suppose that we have <span class="math inline">\(n=10\)</span> trials, and observe <span class="math inline">\(k=7\)</span> successes. The likelihood function is:</p>
<p><span class="math display">\[\begin{equation}
\mathcal{L}(\theta|k=7,n=10)=
\binom{10}{7} \theta^{7} (1-\theta)^{10-7}
\end{equation}\]</span></p>
<p>If we now plot the likelihood function for all possible values of <span class="math inline">\(\theta\)</span> ranging from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>, we get the plot shown in Figure <a href="ch-intro.html#fig:binomlik-lawlargenos">1.3</a>(a).</p>
<div class="figure"><span style="display:block;" id="fig:binomlik-lawlargenos"></span>
<img src="bayescogsci_files/figure-html/binomlik-lawlargenos-1.svg" alt="(a) The likelihood function for 7 successes out of 10. (b) The plot shows the estimate of the mean proportion of successes sampled from a binomial distribution with true probability of success 0.7, with increasing sample sizes. As the sample size increases, the estimate converges to  the true value of 0.7." width="45%" /><img src="bayescogsci_files/figure-html/binomlik-lawlargenos-2.svg" alt="(a) The likelihood function for 7 successes out of 10. (b) The plot shows the estimate of the mean proportion of successes sampled from a binomial distribution with true probability of success 0.7, with increasing sample sizes. As the sample size increases, the estimate converges to  the true value of 0.7." width="45%" />
<p class="caption">
FIGURE 1.3: (a) The likelihood function for 7 successes out of 10. (b) The plot shows the estimate of the mean proportion of successes sampled from a binomial distribution with true probability of success 0.7, with increasing sample sizes. As the sample size increases, the estimate converges to the true value of 0.7.
</p>
</div>
<p>What is important about this plot is that it shows that, given the data, the maximum point is at the point <span class="math inline">\(0.7\)</span>, which corresponds to the estimated mean using the formula shown above: <span class="math inline">\(k/n = 7/10\)</span>. Thus, the maximum likelihood estimate  (MLE) gives us the most likely value that the parameter <span class="math inline">\(\theta\)</span> has, given the data. In the binomial, the proportion of successes <span class="math inline">\(k/n\)</span> can be shown to be the maximum likelihood estimate of the parameter <span class="math inline">\(\theta\)</span> <span class="citation">(e.g., see p. 339-340 of Miller and Miller <a href="#ref-millermiller" role="doc-biblioref">2004</a>)</span>.</p>
<p>A crucial point: the “most likely” value of the parameter is with respect to the data at hand. The goal is to estimate an unknown parameter value from the data. This estimated parameter value is chosen such that the probability (discrete case) or probability density (continuous case) of getting the sample values (i.e., the data) is a maximum. This parameter value is the maximum likelihood estimate (MLE).</p>
<p>This MLE from a particular sample of data need not invariably give us an accurate estimate of <span class="math inline">\(\theta\)</span>. For example, if we run our experiment with <span class="math inline">\(10\)</span> trials and get <span class="math inline">\(1\)</span> success out of <span class="math inline">\(10\)</span>, the MLE is <span class="math inline">\(0.10\)</span>. We could have just happened to observe only one success out of ten by chance, even if the true <span class="math inline">\(\theta\)</span> were <span class="math inline">\(0.7\)</span>. If we were to repeatedly run the experiment with increasing sample sizes, as the sample size increases, the MLE would converge to the true value of the parameter. Figure <a href="ch-intro.html#fig:binomlik-lawlargenos">1.3</a>(b) illustrates this point. The key point here is that with a smaller sample size, the MLE from a particular data set may or may not point to the true value.</p>
</div>
<div id="what-information-does-a-probability-distribution-provide" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> What information does a probability distribution provide?<a href="ch-intro.html#what-information-does-a-probability-distribution-provide" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Bayesian data analysis, we will constantly be asking the question: what information does a probability distribution give us? In particular, we will treat each parameter <span class="math inline">\(\theta\)</span> as a random variable; this will raise questions like: “what is the probability that the parameter <span class="math inline">\(\theta\)</span> lies between two values <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>”; and “what is the range over which we can be 95% certain that the parameter lies”? In order to be able to answer questions like these, we need to know what information we can obtain once we have decided on a probability distribution that is assumed to have generated the data, and how to extract this information using R. We therefore discuss the different kinds of information we can obtain from a probability distribution. For now we focus only on the binomial random variable introduced above.</p>
<div id="compute-the-probability-of-a-particular-outcome-discrete-case-only" class="section level4 hasAnchor" number="1.4.2.1">
<h4><span class="header-section-number">1.4.2.1</span> Compute the probability of a particular outcome (discrete case only)<a href="ch-intro.html#compute-the-probability-of-a-particular-outcome-discrete-case-only" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The binomial distribution shown in Figure <a href="ch-intro.html#fig:binomplot">1.2</a> already shows the probability of each possible event under a different value for <span class="math inline">\(\theta\)</span>. In R, there is a built-in function that allows us to calculate the probability of <span class="math inline">\(k\)</span> successes out of <span class="math inline">\(n\)</span>, given a particular value of <span class="math inline">\(k\)</span> (this number constitutes our data), the number of trials <span class="math inline">\(n\)</span>, and given a particular value of <span class="math inline">\(\theta\)</span>; this is the <code>dbinom()</code> function. For example, the probability of 5 successes out of 10 when <span class="math inline">\(\theta\)</span> is <span class="math inline">\(0.5\)</span> is (note: we are using <span class="math inline">\(k\)</span> to represent the number of successes, but the <code>dbinom()</code> function below uses <code>x</code> instead):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb4-1"><a href="ch-intro.html#cb4-1" aria-hidden="true"></a><span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">5</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 0.246</code></pre>
<p>The probabilities of success when <span class="math inline">\(\theta\)</span> is <span class="math inline">\(0.1\)</span> or <span class="math inline">\(0.9\)</span> can be computed by replacing <span class="math inline">\(0.5\)</span> above by each of these probabilities. One can just do this by giving <code>dbinom()</code> a vector of probabilities:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb6-1"><a href="ch-intro.html#cb6-1" aria-hidden="true"></a><span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">5</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.9</span>))</span></code></pre></div>
<pre><code>## [1] 0.00149 0.00149</code></pre>
<p>The probability of a particular outcome like <span class="math inline">\(k=5\)</span> successes is only computable in the discrete case. In the continuous case, the probability of obtaining a particular point value will always be zero (we discuss this when we turn to continuous probability distributions below).</p>
</div>
<div id="compute-the-cumulative-probability-of-k-or-less-more-than-k-successes" class="section level4 hasAnchor" number="1.4.2.2">
<h4><span class="header-section-number">1.4.2.2</span> Compute the  cumulative probability of k or less (more) than k successes<a href="ch-intro.html#compute-the-cumulative-probability-of-k-or-less-more-than-k-successes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Using the <code>dbinom()</code> function, we can compute the cumulative probability of obtaining 1 or less, 2 or less successes, etc. This is done through a simple summation procedure:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb8-1"><a href="ch-intro.html#cb8-1" aria-hidden="true"></a><span class="co">## the cumulative probability of obtaining</span></span>
<span id="cb8-2"><a href="ch-intro.html#cb8-2" aria-hidden="true"></a><span class="co">## 0, 1, or 2 successes out of 10 trials,</span></span>
<span id="cb8-3"><a href="ch-intro.html#cb8-3" aria-hidden="true"></a><span class="co">## with theta=0.5:</span></span>
<span id="cb8-4"><a href="ch-intro.html#cb8-4" aria-hidden="true"></a><span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">0</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>) <span class="op">+</span></span>
<span id="cb8-5"><a href="ch-intro.html#cb8-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">1</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>) <span class="op">+</span></span>
<span id="cb8-6"><a href="ch-intro.html#cb8-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">2</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 0.0547</code></pre>
<p>Mathematically, we could write the above summation as:</p>
<p><span class="math display">\[\begin{equation}
\sum_{k=0}^2 \binom{n}{k} \theta^{k} (1-\theta)^{n-k}
\end{equation}\]</span></p>
<p>An alternative to the cumbersome addition in the R code above is this more compact statement, which is identical to the above mathematical expression:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb10-1"><a href="ch-intro.html#cb10-1" aria-hidden="true"></a><span class="kw">sum</span>(<span class="kw">dbinom</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<pre><code>## [1] 0.0547</code></pre>
<p>R has a built-in function called <code>pbinom()</code> that does this summation for us. If we want to know the probability of <span class="math inline">\(2\)</span> or fewer than <span class="math inline">\(2\)</span> successes as in the above example, we can write:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb12-1"><a href="ch-intro.html#cb12-1" aria-hidden="true"></a><span class="kw">pbinom</span>(<span class="dv">2</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>, <span class="dt">lower.tail =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code>## [1] 0.0547</code></pre>
<p>The specification <code>lower.tail = TRUE</code> (the default value) ensures that the summation goes from <span class="math inline">\(2\)</span> to numbers smaller than <span class="math inline">\(2\)</span> (which lie in the lower tail of the distribution in Figure <a href="ch-intro.html#fig:binomplot">1.2</a>). If we wanted to know what the probability is of obtaining <span class="math inline">\(3\)</span> or more successes out of <span class="math inline">\(10\)</span>, we can set <code>lower.tail</code> to <code>FALSE</code>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb14-1"><a href="ch-intro.html#cb14-1" aria-hidden="true"></a><span class="kw">pbinom</span>(<span class="dv">2</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.945</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb16-1"><a href="ch-intro.html#cb16-1" aria-hidden="true"></a><span class="co">## equivalently:</span></span>
<span id="cb16-2"><a href="ch-intro.html#cb16-2" aria-hidden="true"></a><span class="co">## sum(dbinom(3:10,size = 10, prob = 0.5))</span></span></code></pre></div>
<p>The cumulative distribution function  or CDF  for a random variable <span class="math inline">\(Y\)</span> is thus related to the corresponding probability mass/density function, and is written as follows:</p>
<p><span class="math display">\[\begin{equation}
F_Y(a) = P(Y\leq a)
\end{equation}\]</span></p>
<p>The CDF can be plotted by computing the cumulative probabilities for any value <span class="math inline">\(k\)</span> or less than <span class="math inline">\(k\)</span>, where <span class="math inline">\(k\)</span> ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(10\)</span> in our running example. The CDF is shown in Figure <a href="ch-intro.html#fig:binomcdf">1.4</a>(a).</p>

<div class="figure"><span style="display:block;" id="fig:binomcdf"></span>
<img src="bayescogsci_files/figure-html/binomcdf-1.svg" alt="(a) Cumulative distribution function (CDF) of a binomial distribution with 10 trials and a 50% probability of success. (b) Inverse CDF for the same binomial distribution." width="49%" /><img src="bayescogsci_files/figure-html/binomcdf-2.svg" alt="(a) Cumulative distribution function (CDF) of a binomial distribution with 10 trials and a 50% probability of success. (b) Inverse CDF for the same binomial distribution." width="49%" />
<p class="caption">
FIGURE 1.4: (a) Cumulative distribution function (CDF) of a binomial distribution with 10 trials and a 50% probability of success. (b) Inverse CDF for the same binomial distribution.
</p>
</div>
</div>
<div id="compute-the-inverse-of-the-cumulative-distribution-function-the-quantile-function" class="section level4 hasAnchor" number="1.4.2.3">
<h4><span class="header-section-number">1.4.2.3</span> Compute the inverse of the cumulative distribution function (the quantile function)<a href="ch-intro.html#compute-the-inverse-of-the-cumulative-distribution-function-the-quantile-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can also find out the value of the variable <span class="math inline">\(k\)</span> (the quantile) such that the probability of obtaining <span class="math inline">\(k\)</span> or less than <span class="math inline">\(k\)</span> successes is some specific probability value <span class="math inline">\(p\)</span>. If we switch the x and y axes of Figure <a href="ch-intro.html#fig:binomcdf">1.4</a>(a), we obtain another very useful function, the inverse of the CDF.</p>
<p>The  inverse of the CDF (known as the  quantile function in R because it returns the quantile, the value <span class="math inline">\(k\)</span>) is available in R as the function <code>qbinom()</code>. The usage is as follows: to find out what the value <span class="math inline">\(k\)</span> of the outcome is such that the probability of obtaining <span class="math inline">\(k\)</span> or less successes is <span class="math inline">\(0.37\)</span>, type:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb17-1"><a href="ch-intro.html#cb17-1" aria-hidden="true"></a><span class="kw">qbinom</span>(<span class="dt">p =</span> <span class="fl">0.37</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<p>One can visualize the inverse CDF of the binomial as in Figure <a href="ch-intro.html#fig:binomcdf">1.4</a>(b).</p>
</div>
<div id="generate-simulated-data-from-a-mathitbinomialntheta-distribution" class="section level4 hasAnchor" number="1.4.2.4">
<h4><span class="header-section-number">1.4.2.4</span> Generate simulated data from a <span class="math inline">\(\mathit{Binomial}(n,\theta)\)</span> distribution<a href="ch-intro.html#generate-simulated-data-from-a-mathitbinomialntheta-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can generate  simulated data from a binomial distribution by specifying the number of observations or experiments (<code>n</code>), the number of trials (<code>size</code>), and the probability of success <span class="math inline">\(\theta\)</span>. In R, we do this as follows:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb19-1"><a href="ch-intro.html#cb19-1" aria-hidden="true"></a><span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 6</code></pre>
<p>The above code generates the number of successes in an experiment with <span class="math inline">\(10\)</span> trials. Repeatedly run the above code; we will get different numbers of successes each time.</p>
<p>As mentioned earlier, if there is only one trial, then instead of the binomial distribution, we have a  Bernoulli distribution. For example, if we have 10 experiments from a Bernoulli distribution, where the probability of success is 0.5, we can simulate data as follows using the function <code>rbern()</code> from the package <code>extraDistr</code>.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb21-1"><a href="ch-intro.html#cb21-1" aria-hidden="true"></a><span class="kw">rbern</span>(<span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>##  [1] 1 0 1 1 1 0 1 0 0 0</code></pre>
<p>The above kind of output can also be generated by using the <code>rbinom()</code> function: <code>rbinom(n = 10, size = 1, prob = 0.5)</code>.
When the data are generated using the <code>rbinom()</code> function in this way, one can calculate the number of successes by just summing up the vector, or computing its mean and multiplying by the number of trials, here <span class="math inline">\(10\)</span>:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb23-1"><a href="ch-intro.html#cb23-1" aria-hidden="true"></a>(y &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<pre><code>##  [1] 0 1 1 1 0 0 0 0 0 1</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb25-1"><a href="ch-intro.html#cb25-1" aria-hidden="true"></a><span class="kw">mean</span>(y) <span class="op">*</span><span class="st"> </span><span class="dv">10</span></span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb27-1"><a href="ch-intro.html#cb27-1" aria-hidden="true"></a><span class="kw">sum</span>(y)</span></code></pre></div>
<pre><code>## [1] 4</code></pre>
</div>
</div>
</div>
<div id="continuous-random-variables-an-example-using-the-normal-distribution" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span>  Continuous random variables: An example using the  normal distribution<a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will now revisit the idea of the random variable using a  continuous distribution. Imagine that we have a vector of reading time data <span class="math inline">\(y\)</span> measured in milliseconds and coming from a normal distribution. The normal distribution is defined in terms of two parameters: the  <em>location</em>, its mean value <span class="math inline">\(\mu\)</span>, which determines its center, and the  <em>scale</em>, its standard deviation, <span class="math inline">\(\sigma\)</span>, which determines how much spread there is around this center point.</p>
<p>The  probability density function  (PDF) of the normal distribution is defined as follows:</p>
<p><span class="math display">\[\begin{equation}
\mathit{Normal}(y|\mu,\sigma)=f(y)= \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(-\frac{(y-\mu)^2}{2\sigma^2} \right)
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\mu\)</span> is some true, unknown mean, and <span class="math inline">\(\sigma^2\)</span> is some true, unknown variance of the normal distribution that the reading times have been sampled from. There is a built-in function in R that computes the above function once we specify the mean <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span> (in R, this parameter is specified in terms of the standard deviation rather than the variance).</p>
<p>Figure <a href="ch-intro.html#fig:normdistrn">1.5</a> visualizes the normal distribution for particular values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, as a PDF (using <code>dnorm()</code>), a CDF (using <code>pnorm()</code>), and the inverse CDF (using <code>qnorm()</code>). It should be clear from the figure that these are three different ways of looking at the same information.</p>
<div class="figure"><span style="display:block;" id="fig:normdistrn"></span>
<img src="bayescogsci_files/figure-html/normdistrn-1.svg" alt="The PDF, CDF, and inverse CDF for the $\mathit{Normal}(\mu=500,\sigma=100)$." width="30%" /><img src="bayescogsci_files/figure-html/normdistrn-2.svg" alt="The PDF, CDF, and inverse CDF for the $\mathit{Normal}(\mu=500,\sigma=100)$." width="30%" /><img src="bayescogsci_files/figure-html/normdistrn-3.svg" alt="The PDF, CDF, and inverse CDF for the $\mathit{Normal}(\mu=500,\sigma=100)$." width="30%" />
<p class="caption">
FIGURE 1.5: The PDF, CDF, and inverse CDF for the <span class="math inline">\(\mathit{Normal}(\mu=500,\sigma=100)\)</span>.
</p>
</div>
<p>As in the discrete example, the PDF, CDF, and inverse of the CDF allow us to ask questions like:</p>
<ul>
<li>What is the probability of observing values between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> from a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>? Using the above example, we can ask what the probability is of observing values between <span class="math inline">\(200\)</span> and <span class="math inline">\(700\)</span> ms:</li>
</ul>
<div class="sourceCode" id="cb29"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb29-1"><a href="ch-intro.html#cb29-1" aria-hidden="true"></a><span class="kw">pnorm</span>(<span class="dv">700</span>, <span class="dt">mean =</span> <span class="dv">500</span>, <span class="dt">sd =</span> <span class="dv">100</span>) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dv">200</span>, <span class="dt">mean =</span> <span class="dv">500</span>, <span class="dt">sd =</span> <span class="dv">100</span>)</span></code></pre></div>
<pre><code>## [1] 0.976</code></pre>
<p>The probability that the random variable takes any specific point value is zero. This is because the probability in a continuous probability distribution is the  area under the curve, and the area at any point on the x-axis is always zero. The implication here is that it is only meaningful to ask about probabilities between two different point values; e.g., the probability that <span class="math inline">\(Y\)</span> lies between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, or <span class="math inline">\(P(a&lt;Y&lt;b)\)</span>. Notice that <span class="math inline">\(P(a&lt;Y&lt;b)\)</span> is the same statement as <span class="math inline">\(P(a\leq Y\leq b)\)</span>. Of course, for any particular point value, the PDF itself does not return the value zero; but the value returned by the PDF is not the probability of that particular value occurring. It is the density of that particular value; and if the PDF is seen as a function of the parameters, it is the likelihood of that particular value.</p>
<ul>
<li>What is the  quantile <span class="math inline">\(q\)</span> such that the probability is <span class="math inline">\(p\)</span> of observing that value <span class="math inline">\(q\)</span> or a value more extreme than <span class="math inline">\(q\)</span>? For example, we can work out the quantile <span class="math inline">\(q\)</span> such that the probability of observing <span class="math inline">\(q\)</span> or some value less than it is <span class="math inline">\(0.975\)</span>, in the <span class="math inline">\(\mathit{Normal}(500,100)\)</span> distribution. Formally, we would write this as <span class="math inline">\(P(Y&lt;a)\)</span>.</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb31-1"><a href="ch-intro.html#cb31-1" aria-hidden="true"></a><span class="kw">qnorm</span>(<span class="fl">0.975</span>, <span class="dt">mean =</span> <span class="dv">500</span>, <span class="dt">sd =</span> <span class="dv">100</span>)</span></code></pre></div>
<pre><code>## [1] 696</code></pre>
<p>The above output says that the quantile value <span class="math inline">\(q\)</span> such that <span class="math inline">\(Prob(X&lt;q)=0.975\)</span> is <span class="math inline">\(q=696\)</span>.</p>
<ul>
<li>Generate  simulated data. Given a vector of <span class="math inline">\(n\)</span> independent and identically distributed data <span class="math inline">\(y\)</span>, i.e., given that each data point is being generated independently from <span class="math inline">\(Y \sim \mathit{Normal}(\mu,\sigma)\)</span> for some values of the parameters, the sample mean and standard deviation<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> are:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\bar{y} =  \frac{\sum_{i=1}^n y_i}{n}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
sd(y) = \sqrt{\frac{\sum_{i=1}^n (y_i-
\bar{y})^2}{n}}
\end{equation}\]</span></p>
<p>For example, we can generate <span class="math inline">\(10\)</span> data points using the <code>rnorm()</code> function, and then use the simulated data to compute the mean and standard deviation:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb33-1"><a href="ch-intro.html#cb33-1" aria-hidden="true"></a>y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10</span>, <span class="dt">mean =</span> <span class="dv">500</span>, <span class="dt">sd =</span> <span class="dv">100</span>)</span>
<span id="cb33-2"><a href="ch-intro.html#cb33-2" aria-hidden="true"></a><span class="kw">mean</span>(y)</span></code></pre></div>
<pre><code>## [1] 513</code></pre>
<div class="sourceCode" id="cb35"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb35-1"><a href="ch-intro.html#cb35-1" aria-hidden="true"></a><span class="kw">sd</span>(y)</span></code></pre></div>
<pre><code>## [1] 92</code></pre>
<p>Again, the sample mean and sample standard deviation computed from a particular (simulated or real) data set need not necessarily be close to the true values of the respective parameters. Especially when sample size is small, one can end up with mis-estimates of the mean and standard deviation.</p>
<p>Incidentally, simulated data can be used to generate all kinds of statistics. For example, we can compute the lower and upper quantiles such that 95% of the simulated data are contained within these quantiles:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb37-1"><a href="ch-intro.html#cb37-1" aria-hidden="true"></a><span class="kw">quantile</span>(y, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##  2.5% 97.5% 
##   356   640</code></pre>
<p>Later on, this function will be used to generate summary statistics once we have obtained samples of a parameter after we have fit a model using Stan/brms.</p>
<div id="an-important-distinction-probability-vs.-density-in-a-continuous-random-variable" class="section level3 hasAnchor" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> An important distinction: probability vs. density in a continuous random variable<a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In continuous distributions like the normal discussed above, it is important to understand that the  probability density function or  PDF, <span class="math inline">\(p(y| \mu, \sigma)\)</span> defines a mapping from the <span class="math inline">\(y\)</span> values (the possible values that the data can have) to a quantity called the density of each possible value. We can see this function in action when we use <code>dnorm()</code> to compute, say, the density value corresponding to <span class="math inline">\(y=1\)</span> and <span class="math inline">\(y=2\)</span> in the standard normal distribution, that is, a normal distribution with a mean of zero and a standard deviation of one.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb39-1"><a href="ch-intro.html#cb39-1" aria-hidden="true"></a><span class="co">## density:</span></span>
<span id="cb39-2"><a href="ch-intro.html#cb39-2" aria-hidden="true"></a><span class="kw">dnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.242</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb41-1"><a href="ch-intro.html#cb41-1" aria-hidden="true"></a><span class="kw">dnorm</span>(<span class="dv">2</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.054</code></pre>
<p>If the density at a particular point value like <span class="math inline">\(1\)</span> is high compared to some other value (<span class="math inline">\(2\)</span> in the above example) then this point value <span class="math inline">\(1\)</span> has a higher  likelihood than <span class="math inline">\(2\)</span> in the standard normal distribution.</p>
<p>The quantity computed for the values <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> are <em>not</em> the probability of observing <span class="math inline">\(1\)</span> or <span class="math inline">\(2\)</span> in this distribution. As mentioned earlier, probability in a continuous distribution is defined as the  area under the curve, and this area will always be zero at any point value (because there are infinitely many different possible values). If we want to know the probability of obtaining values between an upper and lower bound <span class="math inline">\(b\)</span> and <span class="math inline">\(a\)</span>, i.e., <span class="math inline">\(P(a&lt;Y&lt;b)\)</span> where these are two distinct values, we must use the  cumulative distribution function or  CDF: in R, for the normal distribution, this is the <code>pnorm()</code> function. For example, the probability of observing a value between <span class="math inline">\(+2\)</span> and <span class="math inline">\(-2\)</span> in a normal distribution with mean <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(1\)</span> is:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb43-1"><a href="ch-intro.html#cb43-1" aria-hidden="true"></a><span class="kw">pnorm</span>(<span class="dv">2</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.954</code></pre>
<p>The situation is different in discrete random variables. These have a  probability mass function  (PMF) associated with them—an example is the binomial distribution that we saw earlier. There, the PMF maps the possible <span class="math inline">\(y\)</span> values to the probabilities of those values occurring. That is why, in the binomial distribution, the probability of observing exactly <span class="math inline">\(2\)</span> successes when sampling from a <span class="math inline">\(\mathit{Binomial}(n=10,\theta=0.5)\)</span> can be computed using either <code>dbinom()</code> or <code>pbinom()</code>:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb45-1"><a href="ch-intro.html#cb45-1" aria-hidden="true"></a><span class="kw">dbinom</span>(<span class="dv">2</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 0.0439</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb47-1"><a href="ch-intro.html#cb47-1" aria-hidden="true"></a><span class="kw">pbinom</span>(<span class="dv">2</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>) <span class="op">-</span><span class="st"> </span><span class="kw">pbinom</span>(<span class="dv">1</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 0.0439</code></pre>
<p>In the second line of code above, we are computing the cumulative probability of observing two or less successes, minus the probability of observing one or less successes. This gives us the probability of observing exactly two successes. The <code>dbinom()</code> gives us this same information.</p>
</div>
<div id="truncating-a-normal-distribution" class="section level3 hasAnchor" number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> Truncating a normal distribution<a href="ch-intro.html#truncating-a-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the above discussion, the support for the normal distribution ranges from minus infinity to plus infinity. One can define PDFs with a more limited support; an example would be a normal distribution whose PDF <span class="math inline">\(f(x)\)</span> is such that the lower bound is truncated at <span class="math inline">\(0\)</span> to allow only positive values. In such a case, the area under the range minus infinity to zero (<span class="math inline">\(\int_{-\infty}^0 f(x) \, \mathrm{d}x\)</span>) will be <span class="math inline">\(0\)</span> because the range lies outside the support of the truncated normal distribution. Also, if one truncates the standard normal (<span class="math inline">\(\mathit{Normal}(0,1)\)</span>) at <span class="math inline">\(0\)</span>, in order to make the area between zero and plus infinity sum up to <span class="math inline">\(1\)</span>, we would have to multiply it by <span class="math inline">\(2\)</span>, because we just halved the area under the curve. More formally and more generally, we would have to multiply the truncated distribution <span class="math inline">\(f(x)\)</span> by some factor <span class="math inline">\(k\)</span> such that the following integral sums to <span class="math inline">\(1\)</span>:</p>
<p><span class="math display" id="eq:factork">\[\begin{equation}
k \int_{0}^{\infty} f(x)\, \mathrm{d}x = 1
\tag{1.1}
\end{equation}\]</span></p>
<p>Clearly, this factor is <span class="math inline">\(k = \frac{1}{\int_{0}^{\infty} f(x)\, \mathrm{d}x}\)</span>. For the standard normal, this integral is easy to compute using R; we just calculate the complement of the cumulative distribution (CCDF):</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb49-1"><a href="ch-intro.html#cb49-1" aria-hidden="true"></a><span class="kw">pnorm</span>(<span class="dv">0</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb51-1"><a href="ch-intro.html#cb51-1" aria-hidden="true"></a><span class="co">## alternatively:</span></span>
<span id="cb51-2"><a href="ch-intro.html#cb51-2" aria-hidden="true"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dv">0</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<p>The above calculation implies that <span class="math inline">\(k\)</span> is indeed <span class="math inline">\(2\)</span>, as we informally argued (<span class="math inline">\(k = \frac{1}{0.5}=2\)</span>).</p>
<p>Also, if we had truncated the distribution at 0 to the right instead of the left (allowing only negative values), we would have to find the factor <span class="math inline">\(k\)</span> in the same way as above, except that we would have to find <span class="math inline">\(k\)</span> such that:</p>
<p><span class="math display">\[\begin{equation}
k \int_{-\infty}^{0} f(x)\, \mathrm{d}x = 1
\end{equation}\]</span></p>
<p>For the standard normal case, in R, this factor would require us to use the CDF:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb53-1"><a href="ch-intro.html#cb53-1" aria-hidden="true"></a><span class="kw">pnorm</span>(<span class="dv">0</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<p>Later in this book, we will be using such truncated distributions when doing Bayesian modeling, and when we use them, we will want to multiply the truncated distribution by the factor <span class="math inline">\(k\)</span> to ensure that it is still a proper PDF whose  area under the curve sums to <span class="math inline">\(1\)</span>. Truncated normal distributions are discussed in more detail in the online section <a href="regression-models-with-brms---extended.html#app-truncation">A.2</a>.</p>
</div>
</div>
<div id="bivariate-and-multivariate-distributions" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Bivariate and multivariate distributions<a href="ch-intro.html#bivariate-and-multivariate-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have only discussed  univariate distributions; these are distributions that involve only one variable. For example, when we talk about data generated from a Binomial distribution, or from a normal distribution, these are univariate distributions.</p>
<p>It is also possible to specify distributions with two or more dimensions. Some examples will make it clear what a bivariate (or more generally, multivariate) distribution is.</p>
<div id="example-1-discrete-bivariate-distributions" class="section level3 hasAnchor" number="1.6.1">
<h3><span class="header-section-number">1.6.1</span> Example 1:  Discrete bivariate distributions<a href="ch-intro.html#example-1-discrete-bivariate-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Starting with the discrete case, consider the discrete  bivariate distribution shown below. These are data from an experiment where, inter alia, in each trial a Likert acceptability rating and a question-response accuracy were recorded <span class="citation">(the data are from a study by Laurinavichyute <a href="#ref-AnnaLphd" role="doc-biblioref">2020</a>, used with permission here)</span>. Load the data by loading the R package <code>bcogsci</code>.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb55-1"><a href="ch-intro.html#cb55-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_discreteagrmt&quot;</span>)</span></code></pre></div>
<p>Figure <a href="ch-intro.html#fig:bivardiscrete">1.6</a> shows the  <em>joint probability mass function</em> of two random variables X and Y. The random variable <span class="math inline">\(X\)</span> consists of <span class="math inline">\(7\)</span> possible values (this is the <span class="math inline">\(1-7\)</span> Likert response scale), and the random variable <span class="math inline">\(Y\)</span> is question-response accuracy, with <span class="math inline">\(0\)</span> representing an incorrect response, and <span class="math inline">\(1\)</span> representing a correct response. One can also display Figure <a href="ch-intro.html#fig:bivardiscrete">1.6</a> as a table; see Table <a href="ch-intro.html#tab:discbivariatetabular">1.3</a>.</p>
<div class="figure"><span style="display:block;" id="fig:bivardiscrete"></span>
<img src="bayescogsci_files/figure-html/bivardiscrete-1.svg" alt="Example of a discrete bivariate distribution. In these data, in every trial, two pieces of information were collected: Likert responses and yes-no question responses. The random variable X represents Likert scale responses on a scale of 1-7. and the random variable Y represents 0, 1 (incorrect, correct) responses to comprehension questions." width="672" />
<p class="caption">
FIGURE 1.6: Example of a discrete bivariate distribution. In these data, in every trial, two pieces of information were collected: Likert responses and yes-no question responses. The random variable X represents Likert scale responses on a scale of 1-7. and the random variable Y represents 0, 1 (incorrect, correct) responses to comprehension questions.
</p>
</div>
<table>
<caption><span id="tab:discbivariatetabular">TABLE 1.3: </span>The joint PMF for two random variables X and Y.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left"><span class="math inline">\(x=1\)</span></th>
<th align="left"><span class="math inline">\(x=2\)</span></th>
<th align="left"><span class="math inline">\(x=3\)</span></th>
<th align="left"><span class="math inline">\(x=4\)</span></th>
<th align="left"><span class="math inline">\(x=5\)</span></th>
<th align="left"><span class="math inline">\(x=6\)</span></th>
<th align="left"><span class="math inline">\(x=7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(y=0\)</span></td>
<td align="left"><span class="math inline">\(0.018\)</span></td>
<td align="left"><span class="math inline">\(0.023\)</span></td>
<td align="left"><span class="math inline">\(0.04\)</span></td>
<td align="left"><span class="math inline">\(0.043\)</span></td>
<td align="left"><span class="math inline">\(0.063\)</span></td>
<td align="left"><span class="math inline">\(0.049\)</span></td>
<td align="left"><span class="math inline">\(0.055\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y=1\)</span></td>
<td align="left"><span class="math inline">\(0.031\)</span></td>
<td align="left"><span class="math inline">\(0.053\)</span></td>
<td align="left"><span class="math inline">\(0.086\)</span></td>
<td align="left"><span class="math inline">\(0.096\)</span></td>
<td align="left"><span class="math inline">\(0.147\)</span></td>
<td align="left"><span class="math inline">\(0.153\)</span></td>
<td align="left"><span class="math inline">\(0.142\)</span></td>
</tr>
</tbody>
</table>
<p>For each possible pair of values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we have a joint probability <span class="math inline">\(p_{X,Y}(x,y)\)</span>. Given such a bivariate distribution, there are two useful quantities we can compute: the <em>marginal</em> distributions (<span class="math inline">\(p_{X}\)</span> and <span class="math inline">\(p_Y\)</span>), and the <em>conditional</em> distributions (<span class="math inline">\(p_{X|Y}\)</span> and <span class="math inline">\(p_{Y|X}\)</span>). Table <a href="ch-intro.html#tab:discbivariatetabular">1.3</a> shows the joint probability mass function <span class="math inline">\(p_{X,Y}(x,y)\)</span>.</p>
<div id="sec-marginalizing" class="section level4 hasAnchor" number="1.6.1.1">
<h4><span class="header-section-number">1.6.1.1</span> Marginal distributions<a href="ch-intro.html#sec-marginalizing" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The  marginal distribution <span class="math inline">\(p_Y\)</span> is defined as follows. <span class="math inline">\(S_{X}\)</span> is the support of <span class="math inline">\(X\)</span>, i.e., all the possible values of <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[\begin{equation}
p_{Y}(y)=\sum_{x\in S_{X}}p_{X,Y}(x,y).\label{eq-marginal-pmf}
\end{equation}\]</span></p>
<p>Similarly, the marginal distribution <span class="math inline">\(p_X\)</span> is defined as:</p>
<p><span class="math display">\[\begin{equation}
p_{X}(x)=\sum_{y\in S_{Y}}p_{X,Y}(x,y).\label{eq-marginal-pmf2}
\end{equation}\]</span></p>
<p><span class="math inline">\(p_Y\)</span> is computed, by summing up the rows; and <span class="math inline">\(p_X\)</span> by summing up the columns. We can see why this is called the marginal distribution; the result appears in the margins of the table. In the code below, the object <code>probs</code> contains bivariate PMF shown in Table <a href="ch-intro.html#tab:discbivariatetabular">1.3</a>.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb56-1"><a href="ch-intro.html#cb56-1" aria-hidden="true"></a><span class="co"># P(Y)</span></span>
<span id="cb56-2"><a href="ch-intro.html#cb56-2" aria-hidden="true"></a>(PY &lt;-<span class="st"> </span><span class="kw">rowSums</span>(probs))</span></code></pre></div>
<pre><code>##   y=0   y=1 
## 0.291 0.709</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb58-1"><a href="ch-intro.html#cb58-1" aria-hidden="true"></a><span class="kw">sum</span>(PY) <span class="co">## sums to 1</span></span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb60-1"><a href="ch-intro.html#cb60-1" aria-hidden="true"></a><span class="co"># P(X)</span></span>
<span id="cb60-2"><a href="ch-intro.html#cb60-2" aria-hidden="true"></a>(PX &lt;-<span class="st"> </span><span class="kw">colSums</span>(probs))</span></code></pre></div>
<pre><code>##    x=1    x=2    x=3    x=4    x=5    x=6    x=7 
## 0.0491 0.0766 0.1257 0.1394 0.2102 0.2020 0.1969</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb62-1"><a href="ch-intro.html#cb62-1" aria-hidden="true"></a><span class="kw">sum</span>(PX) <span class="co">## sums to 1</span></span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>The marginal probabilities sum to <span class="math inline">\(1\)</span>, as they should. Table <a href="ch-intro.html#tab:discbivariatetabularmarginal">1.4</a> shows the marginal probabilities.</p>
<table>
<caption><span id="tab:discbivariatetabularmarginal">TABLE 1.4: </span>The joint PMF for two random variables X and Y, along with the marginal distributions of X and Y.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left"><span class="math inline">\(x=1\)</span></th>
<th align="left"><span class="math inline">\(x=2\)</span></th>
<th align="left"><span class="math inline">\(x=3\)</span></th>
<th align="left"><span class="math inline">\(x=4\)</span></th>
<th align="left"><span class="math inline">\(x=5\)</span></th>
<th align="left"><span class="math inline">\(x=6\)</span></th>
<th align="left"><span class="math inline">\(x=7\)</span></th>
<th align="left"><span class="math inline">\(P(Y)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(y=0\)</span></td>
<td align="left"><span class="math inline">\(0.018\)</span></td>
<td align="left"><span class="math inline">\(0.023\)</span></td>
<td align="left"><span class="math inline">\(0.04\)</span></td>
<td align="left"><span class="math inline">\(0.043\)</span></td>
<td align="left"><span class="math inline">\(0.063\)</span></td>
<td align="left"><span class="math inline">\(0.049\)</span></td>
<td align="left"><span class="math inline">\(0.055\)</span></td>
<td align="left"><span class="math inline">\(0.291\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y=1\)</span></td>
<td align="left"><span class="math inline">\(0.031\)</span></td>
<td align="left"><span class="math inline">\(0.053\)</span></td>
<td align="left"><span class="math inline">\(0.086\)</span></td>
<td align="left"><span class="math inline">\(0.096\)</span></td>
<td align="left"><span class="math inline">\(0.147\)</span></td>
<td align="left"><span class="math inline">\(0.153\)</span></td>
<td align="left"><span class="math inline">\(0.142\)</span></td>
<td align="left"><span class="math inline">\(0.709\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(P(X)\)</span></td>
<td align="left"><span class="math inline">\(0.049\)</span></td>
<td align="left"><span class="math inline">\(0.077\)</span></td>
<td align="left"><span class="math inline">\(0.126\)</span></td>
<td align="left"><span class="math inline">\(0.139\)</span></td>
<td align="left"><span class="math inline">\(0.21\)</span></td>
<td align="left"><span class="math inline">\(0.202\)</span></td>
<td align="left"><span class="math inline">\(0.197\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>To compute the marginal distribution of <span class="math inline">\(X\)</span>, one is summing over all the <span class="math inline">\(Y\)</span>’s; and to compute the marginal distribution of <span class="math inline">\(Y\)</span>, one sums over all the <span class="math inline">\(X\)</span>’s. We say that we are <em>marginalizing out</em> the random variable that we are summing over. One can also visualize the two marginal distributions using barplots (Figure <a href="ch-intro.html#fig:marginalbarplot">1.7</a>).</p>
<div class="figure"><span style="display:block;" id="fig:marginalbarplot"></span>
<img src="bayescogsci_files/figure-html/marginalbarplot-1.svg" alt="The marginal distributions of the random variables X and Y, presented as barplots." width="672" />
<p class="caption">
FIGURE 1.7: The marginal distributions of the random variables X and Y, presented as barplots.
</p>
</div>
</div>
<div id="conditional-distributions" class="section level4 hasAnchor" number="1.6.1.2">
<h4><span class="header-section-number">1.6.1.2</span> Conditional distributions<a href="ch-intro.html#conditional-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For computing conditional distributions, recall that conditional probability (see section <a href="ch-intro.html#condprob">1.2</a>) is defined as:</p>
<p><span class="math display">\[\begin{equation}
p_{X\mid Y}(x\mid y) = \frac{p_{X,Y}(x,y)}{p_Y(y)}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{equation}
p_{Y\mid X}(y\mid x) = \frac{p_{X,Y}(x,y)}{p_X(x)}
\end{equation}\]</span></p>
<p>The  conditional distribution of a random variable <span class="math inline">\(X\)</span> given that <span class="math inline">\(Y=y\)</span>, where <span class="math inline">\(y\)</span> is some specific (fixed) value, is:</p>
<p><span class="math display">\[\begin{equation}
p_{X\mid Y} (x\mid y) = \frac{p_{X,Y}(x,y)}{p_Y(y)} \quad \hbox{provided } p_Y(y)=P(Y=y)&gt;0
\end{equation}\]</span></p>
<p>As an example, let’s consider how <span class="math inline">\(p_{X\mid Y}\)</span> would be computed.
The possible values of <span class="math inline">\(y\)</span> are <span class="math inline">\(0,1\)</span>, and so we have to find the conditional distribution (defined above) for each of these values. That is, we have to find <span class="math inline">\(p_{X\mid Y}(x\mid y=0)\)</span>, and <span class="math inline">\(p_{X\mid Y}(x\mid y=1)\)</span>.</p>
<p>Let’s do the calculation for <span class="math inline">\(p_{X\mid Y}(x\mid y=0)\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
p_{X\mid Y} (1\mid 0) =&amp; \frac{p_{X,Y}(1,0)}{p_Y(0)}\\
    =&amp;  \frac{0.018}{0.291}\\
    =&amp; 0.062
\end{split}
\end{equation}\]</span></p>
<p>This conditional probability value will occupy the cell <span class="math inline">\(X=1\)</span>, <span class="math inline">\(Y=0\)</span> in Table <a href="ch-intro.html#tab:discbivariatetabularconditional">1.5</a> summarizing the conditional probability distribution <span class="math inline">\(p_{X|Y}\)</span>. In this way, one can fill in the entire table, which will then represent the conditional distributions <span class="math inline">\(p_{X|Y=0}\)</span> and <span class="math inline">\(p_{X|Y=1}\)</span>. The reader may want to take a few minutes to complete Table <a href="ch-intro.html#tab:discbivariatetabularconditional">1.5</a>. After the conditional probabilities have been computed, the rows should sum up to <span class="math inline">\(1\)</span>.</p>
<table>
<caption>
<span id="tab:discbivariatetabularconditional">TABLE 1.5: </span>A table for listing conditional distributions of X given Y.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
<span class="math inline">\(x=1\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(x=2\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(x=3\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(x=4\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(x=5\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(x=6\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(x=7\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(p_{X|Y(x|y=0)}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.062\)</span>
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(p_{X|Y(x|y=1)}\)</span>
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>
<p>Similarly, one can construct a table that shows <span class="math inline">\(p_{Y|X}\)</span>.</p>
</div>
<div id="covariance-and-correlation" class="section level4 hasAnchor" number="1.6.1.3">
<h4><span class="header-section-number">1.6.1.3</span> Covariance and correlation<a href="ch-intro.html#covariance-and-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Here, we briefly define the covariance and correlation of two discrete random variables. For detailed examples and discussion, see the references at the end of the chapter.
Informally, if there is a high probability that large values of a random variable <span class="math inline">\(X\)</span> are associated with large values of another random variable <span class="math inline">\(Y\)</span>, we will say that the  covariance between the two random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, written <span class="math inline">\(Cov(X,Y)\)</span>, is positive.</p>
<p>The covariance of two (discrete) random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as follows. <span class="math inline">\(E[\cdot]\)</span> refers to the expectation of a random variable.</p>
<p><span class="math display">\[\begin{equation}
Cov(X,Y) = E[(X - E[X])(Y-E[Y])]
\end{equation}\]</span></p>
<p>It is possible to show that this is equivalent to:</p>
<p><span class="math display">\[\begin{equation}
Cov(X,Y) = E[XY] - E[X]E[Y]
\end{equation}\]</span></p>
<p>The  expectation E[XY] is defined to be:</p>
<p><span class="math display">\[\begin{equation}
E[XY]=\sum_x \sum_y xy f_{X,Y}(x,y)
\end{equation}\]</span></p>
<p>If the standard deviations of the two random variables is <span class="math inline">\(\sigma_X\)</span> and <span class="math inline">\(\sigma_Y\)</span>, the  correlation between the two random variables, <span class="math inline">\(\rho_{XY}\)</span>, is defined as:</p>
<p><span class="math display">\[\begin{equation}
\rho_{XY} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y}
\end{equation}\]</span></p>
</div>
</div>
<div id="sec-contbivar" class="section level3 hasAnchor" number="1.6.2">
<h3><span class="header-section-number">1.6.2</span> Example 2: Continuous bivariate distributions<a href="ch-intro.html#sec-contbivar" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider now the continuous bivariate case; this time, we will use simulated data. Consider two normal random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, each of which coming from, for example, a <span class="math inline">\(\mathit{Normal}(0,1)\)</span> distribution, with some correlation <span class="math inline">\(\rho_{X,Y}\)</span> between the two random variables.</p>
<p>A bivariate distribution for two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, each of which comes from a normal distribution, is expressed in terms of the means and standard deviations of each of the two distributions, and the correlation <span class="math inline">\(\rho_{XY}\)</span> between them. The standard deviations and correlation in a bivariate distribution are expressed in a special form of a <span class="math inline">\(2\times 2\)</span> matrix called a  variance-covariance matrix <span class="math inline">\(\Sigma\)</span>. If <span class="math inline">\(\rho_{XY}\)</span> is the correlation between the two random variables, and <span class="math inline">\(\sigma _{X}\)</span> and <span class="math inline">\(\sigma _{Y}\)</span> the respective standard deviations, the variance-covariance matrix is written as:</p>
<p><span class="math display">\[\begin{equation}\label{eq:covmatfoundations}
\Sigma
=
\begin{pmatrix}
\sigma _{X}^2  &amp; \rho_{XY}\sigma _{X}\sigma _{Y}\\
\rho_{XY}\sigma _{X}\sigma _{Y}    &amp; \sigma _{Y}^2\\
\end{pmatrix}
\end{equation}\]</span></p>
<p>The off-diagonals of this matrix contain the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as follows:</p>
<p><span class="math display">\[\begin{equation}\label{eq:jointpriordistfoundations}
\begin{pmatrix}
  X \\
  Y \\
\end{pmatrix}
\sim
\mathcal{N}_2 \left(
\begin{pmatrix}
  0 \\
  0 \\
\end{pmatrix},
\Sigma
\right)
\end{equation}\]</span></p>
<p>The joint PDF is written with reference to the two variables <span class="math inline">\(f_{X,Y}(x,y)\)</span>. It has the property that the volume under the surface that is bounded by the density function sums to 1—this sum-to-1 property is the same idea that we encountered in the univariate cases (the normal and binomial distributions), except that we are talking about a bivariate distribution here, so we talk about the volume under the surface rather than the area under the curve.</p>
<p>Formally, we would write the volume as a double integral: we are summing up the volume under the surface representing the joint density for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (hence the two integrals).</p>
<p><span class="math display">\[\begin{equation}
\iint_{S_{X,Y}} f_{X,Y}(x,y)\, \mathrm{d}x \mathrm{d}y = 1
\end{equation}\]</span></p>
<p>Here, the terms <span class="math inline">\(\mathrm{d}x\)</span> and <span class="math inline">\(\mathrm{d}y\)</span> express the fact that we are summing up the volume under the surface.</p>
<p>The joint CDF would be written as follows. The equation below gives us the probability of observing a value like <span class="math inline">\((u,v)\)</span> or some value smaller than that (i.e., some <span class="math inline">\((u&#39;,v&#39;)\)</span>, such that <span class="math inline">\(u&#39;&lt;u\)</span> and <span class="math inline">\(v&#39;&lt;v\)</span>).</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
F_{X,Y}(u,v) =&amp; P(X&lt;u,Y&lt;v)\\
             =&amp; \int_{-\infty}^u \int_{-\infty}^v f_{X,Y}(x,y)\, \mathrm{d}y \mathrm{d}x\\
\end{split}
\end{equation}\]</span></p>
<p>Just as in the discrete case, the  marginal distributions can be derived by marginalizing out the other random variable:</p>
<p><span class="math display">\[\begin{equation}
f_X(x) = \int_{S_Y} f_{X,Y}(x,y)\, \mathrm{d}y, \quad f_Y(y) = \int_{S_X} f_{X,Y}(x,y)\, \mathrm{d}x
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(S_X\)</span> and <span class="math inline">\(S_Y\)</span> are the respective supports.</p>
<p>Here, the integral sign <span class="math inline">\(\int\)</span> is the continuous equivalent of the summation sign <span class="math inline">\(\sum\)</span> in the discrete case. Luckily, we will never have to compute such integrals ourselves; but it is important to appreciate how a marginal distribution arises from a bivariate distribution—by integrating out or marginalizing out the other random variable.</p>
<p>A visualization will help. The figures below show a bivariate distribution with zero correlation (Figure <a href="ch-intro.html#fig:zerocor">1.8</a>), a negative (Figure <a href="ch-intro.html#fig:negcor">1.9</a>) and a positive correlation (Figure <a href="ch-intro.html#fig:poscor">1.10</a>).</p>
<div class="figure"><span style="display:block;" id="fig:zerocor"></span>
<img src="bayescogsci_files/figure-html/zerocor-1.svg" alt="A bivariate normal distribution with zero correlation. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot." width="672" />
<p class="caption">
FIGURE 1.8: A bivariate normal distribution with zero correlation. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:negcor"></span>
<img src="bayescogsci_files/figure-html/negcor-1.svg" alt="A bivariate normal distribution with a negative  correlation of -0.6. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot." width="672" />
<p class="caption">
FIGURE 1.9: A bivariate normal distribution with a negative correlation of -0.6. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:poscor"></span>
<img src="bayescogsci_files/figure-html/poscor-1.svg" alt="A bivariate normal distribution with a positive  correlation of 0.6. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot." width="672" />
<p class="caption">
FIGURE 1.10: A bivariate normal distribution with a positive correlation of 0.6. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot.
</p>
</div>
<p>In this book, we will make use of such multivariate distributions a lot, and it will soon become important to know how to generate simulated bivariate or multivariate data that is correlated. So let’s look at that next.</p>
</div>
<div id="sec-generatebivariatedata" class="section level3 hasAnchor" number="1.6.3">
<h3><span class="header-section-number">1.6.3</span> Generate simulated bivariate (or multivariate) data<a href="ch-intro.html#sec-generatebivariatedata" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we want to generate <span class="math inline">\(100\)</span> pairs of correlated continuous data, with correlation <span class="math inline">\(\rho=0.6\)</span>. The two random variables both have a normal PDF, and have mean <span class="math inline">\(0\)</span>, and standard deviations <span class="math inline">\(5\)</span> and <span class="math inline">\(10\)</span>, respectively.</p>
<p>Here is how we would generate such data. First, define a variance-covariance matrix; then, use the multivariate analog of the <code>rnorm()</code> function, <code>mvrnorm()</code>, from the <code>MASS</code> package to generate <span class="math inline">\(100\)</span> data points.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb64-1"><a href="ch-intro.html#cb64-1" aria-hidden="true"></a><span class="co">## define a variance-covariance matrix:</span></span>
<span id="cb64-2"><a href="ch-intro.html#cb64-2" aria-hidden="true"></a>Sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">5</span> <span class="op">^</span><span class="st"> </span><span class="dv">2</span>, <span class="dv">5</span> <span class="op">*</span><span class="st"> </span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.6</span>, <span class="dv">5</span> <span class="op">*</span><span class="st"> </span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.6</span>, <span class="dv">10</span> <span class="op">^</span><span class="st"> </span><span class="dv">2</span>),</span>
<span id="cb64-3"><a href="ch-intro.html#cb64-3" aria-hidden="true"></a>                <span class="dt">byrow =</span> <span class="ot">FALSE</span>, <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb64-4"><a href="ch-intro.html#cb64-4" aria-hidden="true"></a><span class="co">## generate data:</span></span>
<span id="cb64-5"><a href="ch-intro.html#cb64-5" aria-hidden="true"></a>u &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">Sigma =</span> Sigma)</span>
<span id="cb64-6"><a href="ch-intro.html#cb64-6" aria-hidden="true"></a><span class="kw">head</span>(u, <span class="dt">n =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##       [,1]  [,2]
## [1,] -2.50 -14.5
## [2,]  6.71  13.1
## [3,] 10.66  24.7</code></pre>
<p>Figure <a href="ch-intro.html#fig:poscordata">1.11</a> confirms that the simulated data are positively correlated.</p>
<div class="figure"><span style="display:block;" id="fig:poscordata"></span>
<img src="bayescogsci_files/figure-html/poscordata-1.svg" alt="The relationship between two positively correlated random variables, generated by simulating data using the R function mvrnorm from the MASS library." width="672" />
<p class="caption">
FIGURE 1.11: The relationship between two positively correlated random variables, generated by simulating data using the R function mvrnorm from the MASS library.
</p>
</div>
</div>
<div id="sec-decomposevcovmatrix" class="section level3 hasAnchor" number="1.6.4">
<h3><span class="header-section-number">1.6.4</span> Decomposing a variance-covariance matrix<a href="ch-intro.html#sec-decomposevcovmatrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One final useful fact about the variance-covariance matrix—one that we will need later—is that it can be decomposed into the component standard deviations and an underlying correlation matrix. For example, consider the matrix above:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb66-1"><a href="ch-intro.html#cb66-1" aria-hidden="true"></a>Sigma</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]   25   30
## [2,]   30  100</code></pre>
<p>One can decompose the matrix as follows. The matrix can be seen as the product of a diagonal matrix of the standard deviations and the correlation matrix:</p>
<p><span class="math display">\[\begin{equation}
\begin{pmatrix}
5 &amp;   0\\
0 &amp;  10\\
\end{pmatrix}
\begin{pmatrix}
  1.0 &amp; 0.6\\
 0.6  &amp; 1.0\\
\end{pmatrix}
\begin{pmatrix}
 5 &amp;   0\\
0 &amp;  10\\
\end{pmatrix}
\end{equation}\]</span></p>
<p>One can reassemble the variance-covariance matrix by pre-multiplying and post-multiplying the correlation matrix with the diagonal matrix containing the standard deviations:<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p><span class="math display">\[\begin{equation}
\begin{pmatrix}
5 &amp;   0\\
0 &amp;  10\\
\end{pmatrix}
\begin{pmatrix}
  1.0 &amp; 0.6\\
 0.6  &amp; 1.0\\
\end{pmatrix}
\begin{pmatrix}
 5 &amp;   0\\
0 &amp;  10\\
\end{pmatrix}
=
\begin{pmatrix}
25  &amp; 30\\
30  &amp; 100\\
\end{pmatrix}
\end{equation}\]</span></p>
<p>Using R (the symbol <code>%*%</code> is the matrix multiplication operator in R):</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb68-1"><a href="ch-intro.html#cb68-1" aria-hidden="true"></a><span class="co">## sds:</span></span>
<span id="cb68-2"><a href="ch-intro.html#cb68-2" aria-hidden="true"></a>(sds &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">10</span>))</span></code></pre></div>
<pre><code>## [1]  5 10</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb70-1"><a href="ch-intro.html#cb70-1" aria-hidden="true"></a><span class="co">## diagonal matrix:</span></span>
<span id="cb70-2"><a href="ch-intro.html#cb70-2" aria-hidden="true"></a>(sd_diag &lt;-<span class="st"> </span><span class="kw">diag</span>(sds))</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    5    0
## [2,]    0   10</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb72-1"><a href="ch-intro.html#cb72-1" aria-hidden="true"></a><span class="co">## correlation matrix:</span></span>
<span id="cb72-2"><a href="ch-intro.html#cb72-2" aria-hidden="true"></a>(corrmatrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">0.6</span>, <span class="fl">0.6</span>, <span class="dv">1</span>), <span class="dt">ncol =</span> <span class="dv">2</span>))</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  1.0  0.6
## [2,]  0.6  1.0</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb74-1"><a href="ch-intro.html#cb74-1" aria-hidden="true"></a>sd_diag <span class="op">%*%</span><span class="st"> </span>corrmatrix <span class="op">%*%</span><span class="st"> </span>sd_diag</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]   25   30
## [2,]   30  100</code></pre>
<p>This decomposition and reassembly of the variance-covariance matrix will become important when we start building hierarchical models in Stan.</p>
</div>
</div>
<div id="sec-marginal" class="section level2 hasAnchor" number="1.7">
<h2><span class="header-section-number">1.7</span> An important concept: The marginal likelihood (integrating out a parameter)<a href="ch-intro.html#sec-marginal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here, we introduce a concept that will turn up many times in this book. The concept we unpack here is called “integrating out a parameter.” We will need this when we encounter Bayes’ rule in the next chapter, and when we use Bayes factors later in the book (chapter <a href="ch-bf.html#ch-bf">13</a>).</p>
<p>Integrating out a parameter refers to the following situation. The example used here discusses the binomial distribution, but the approach is generally applicable for any distribution.</p>
<p>Suppose we have a binomial random variable <span class="math inline">\(Y\)</span> with PMF <span class="math inline">\(p(Y)\)</span>. Suppose also that this PMF is defined in terms of parameter <span class="math inline">\(\theta\)</span> that can have only three possible values, <span class="math inline">\(0.1, 0.5, 0.9\)</span>, each with equal probability. In other words, the probability that <span class="math inline">\(\theta\)</span> is <span class="math inline">\(0.1, 0.5,\)</span> or <span class="math inline">\(0.9\)</span> is <span class="math inline">\(1/3\)</span> each.</p>
<p>We stick with our earlier example of <span class="math inline">\(n=10\)</span> trials and <span class="math inline">\(k=7\)</span> successes.
The likelihood function then is</p>
<p><span class="math display">\[\begin{equation}
p(k=7,n=10|\theta) = \binom{10}{7} \theta^7 (1-\theta)^{3}
\end{equation}\]</span></p>
<p>There is a related concept of marginal likelihood, which we can write here as <span class="math inline">\(p(k=7,n=10)\)</span>. Marginal likelihood is the likelihood computed by “marginalizing” out the parameter <span class="math inline">\(\theta\)</span>: for each possible value that the parameter <span class="math inline">\(\theta\)</span> can have, we compute the likelihood at that value and multiply that likelihood with the probability/density of that <span class="math inline">\(\theta\)</span> value occurring. Then we sum up each of the products computed in this way. Mathematically, this means that we carry out the following operation.</p>
<p>In our example, there are three possible values of <span class="math inline">\(\theta\)</span>,
call them <span class="math inline">\(\theta_1=0.1\)</span>, <span class="math inline">\(\theta_2=0.5\)</span>, and <span class="math inline">\(\theta_3=0.9\)</span>. Each has probability <span class="math inline">\(1/3\)</span>; so <span class="math inline">\(p(\theta_1)=p(\theta_2)=p(\theta_3)=1/3\)</span>. Given this information, we can compute the marginal likelihood as follows:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
p(k=7,n=10) =&amp; \binom{10}{7} \theta_1^7 (1-\theta_1)^{3} \times p(\theta_1) \\
            +&amp; \binom{10}{7} \theta_2^7 (1-\theta_2)^{3}\times p(\theta_2) \\
            +&amp; \binom{10}{7} \theta_3^7 (1-\theta_3)^{3}\times p(\theta_3)
\end{split}
\end{equation}\]</span></p>
<p>Writing the <span class="math inline">\(\theta\)</span> values and their probabilities, we get:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
p(k=7,n=10) =&amp; \binom{10}{7} 0.1^7 (1-0.1)^{3} \times \frac{1}{3} \\
            +&amp; \binom{10}{7} 0.5^7 (1-0.5)^{3}\times \frac{1}{3} \\
            +&amp; \binom{10}{7} 0.9^7 (1-0.9)^{3}\times \frac{1}{3}
\end{split}
\end{equation}\]</span></p>
<p>Taking the common factors (<span class="math inline">\(\frac{1}{3}\)</span> and <span class="math inline">\(\binom{10}{7}\)</span>) out:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
p(k=7,n=10) =&amp; \frac{1}{3} \binom{10}{7} \big[ 0.1^7 (1-0.1)^{3} \\
+&amp;  0.5^7 (1-0.5)^{3} \\
+&amp;  0.9^7 (1-0.9)^{3} \big] \\
=&amp; 0.058
\end{split}
\end{equation}\]</span></p>
<p>Thus, a marginal likelihood is a kind of weighted sum of the likelihood, weighted by the possible values of the parameter.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p>The above example was contrived, because we stated that the parameter <span class="math inline">\(\theta\)</span> has only three possible discrete values. Now consider the case where the parameter <span class="math inline">\(\theta\)</span> can have all possible values between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>; every possible value is equally likely. Formally, what this means is that the possible values of <span class="math inline">\(\theta\)</span> can be described in terms of the uniform distribution with lower bound <span class="math inline">\(0\)</span> and upper bound <span class="math inline">\(1\)</span> <span class="citation">(for more details on this distribution, see section 5.2 in Blitzstein and Hwang <a href="#ref-blitzstein2014introduction" role="doc-biblioref">2014</a>)</span>. This is a continuous distribution, and because the area under the distribution has to sum to <span class="math inline">\(1\)</span>, the density of every possible value of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(1\)</span>.</p>
<p>In this example, the summation now has to be done over a continuous space <span class="math inline">\([0,1]\)</span>. The way this summation is expressed in mathematics is through the  integral symbol:</p>
<p><span class="math display">\[\begin{equation}
p(k=7,n=10) = \int_0^1 \binom{10}{7} \theta^7 (1-\theta)^{3}\, p(\theta) d\theta
\end{equation}\]</span></p>
<p>Because <span class="math inline">\(p(\theta)=1\)</span> for all <span class="math inline">\(\theta\)</span> (in this particular case), we can just write:</p>
<p><span class="math display">\[\begin{equation}
p(k=7,n=10) = \int_0^1 \binom{10}{7} \theta^7 (1-\theta)^{3}\,  d\theta
\end{equation}\]</span></p>
<p>This statement is computing something similar to what we computed above with the three discrete parameter values, except that the summation is being done over a continuous space ranging from 0 to 1. We say that the parameter <span class="math inline">\(\theta\)</span> has been integrated out, or marginalized. Integrating out a parameter will be a very common operation in this book, but we will never have to do the calculation ourselves. For the above case, we can compute the integral in R:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb76-1"><a href="ch-intro.html#cb76-1" aria-hidden="true"></a>BinLik &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {</span>
<span id="cb76-2"><a href="ch-intro.html#cb76-2" aria-hidden="true"></a>  <span class="kw">choose</span>(<span class="dv">10</span>, <span class="dv">7</span>) <span class="op">*</span><span class="st"> </span>theta<span class="op">^</span><span class="dv">7</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">3</span></span>
<span id="cb76-3"><a href="ch-intro.html#cb76-3" aria-hidden="true"></a>}</span>
<span id="cb76-4"><a href="ch-intro.html#cb76-4" aria-hidden="true"></a><span class="kw">integrate</span>(BinLik, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value</span></code></pre></div>
<pre><code>## [1] 0.0909</code></pre>
<p>The value that is output by the <code>integrate()</code> function above is the marginal likelihood.</p>
<p>This completes our discussion of random variables and probability distributions. Next, we summarize what we have learned so far about univariate distributions.</p>
</div>
<div id="summary-of-some-useful-r-functions" class="section level2 hasAnchor" number="1.8">
<h2><span class="header-section-number">1.8</span> Summary of some useful R functions<a href="ch-intro.html#summary-of-some-useful-r-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Table <a href="ch-intro.html#tab:dpqrfunctions">1.6</a> summarizes the different functions relating to PMFs and PDFs, using the binomial and normal as examples.</p>
<table>
<caption><span id="tab:dpqrfunctions">TABLE 1.6: </span> Important R functions relating to random variables.</caption>
<thead>
<tr class="header">
<th></th>
<th align="center">Discrete</th>
<th align="center">Continuous</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Example:</td>
<td align="center"><span class="math inline">\(\mathit{Binomial}(y|n,\theta)\)</span></td>
<td align="center"><span class="math inline">\(\mathit{Norma}l(y|\mu,\sigma)\)</span></td>
</tr>
<tr class="even">
<td>Likelihood function</td>
<td align="center"><code>dbinom()</code></td>
<td align="center"><code>dnorm()</code></td>
</tr>
<tr class="odd">
<td>Prob Y=y</td>
<td align="center"><code>dbinom()</code></td>
<td align="center">always 0</td>
</tr>
<tr class="even">
<td>Prob <span class="math inline">\(Y\geq y, Y\leq y, y_1&lt;Y&lt;y_2\)</span></td>
<td align="center"><code>pbinom()</code></td>
<td align="center"><code>pnorm()</code></td>
</tr>
<tr class="odd">
<td>Inverse CDF</td>
<td align="center"><code>qbinom()</code></td>
<td align="center"><code>qnorm()</code></td>
</tr>
<tr class="even">
<td>Generate simulated data</td>
<td align="center"><code>rbinom()</code></td>
<td align="center"><code>rnorm()</code></td>
</tr>
</tbody>
</table>
<p>Later on, we will use other distributions, such as the uniform, beta, etc., and each of these has their own set of <code>d-p-q-r</code> functions in R. One can look up these different distributions in, for example, <span class="citation">Blitzstein and Hwang (<a href="#ref-blitzstein2014introduction" role="doc-biblioref">2014</a>)</span>.</p>
</div>
<div id="summary" class="section level2 hasAnchor" number="1.9">
<h2><span class="header-section-number">1.9</span> Summary<a href="ch-intro.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter briefly reviewed some very basic concepts in probability theory, univariate discrete and continuous random variables, and bivariate distributions. An important set of functions we encountered are the d-p-q-r family of functions for different distributions; these are very useful for understanding the properties of commonly used distributions, visualizing distributions, and for simulating data. Distributions will play a central role in this book; for example, knowing how to visualize distributions will be important for deciding on prior distributions for parameters. Other important ideas we learned about were marginal and conditional probability, marginal likelihood, and how to define multivariate distributions; these concepts will play an important role in Bayesian statistics.</p>
</div>
<div id="further-reading" class="section level2 hasAnchor" number="1.10">
<h2><span class="header-section-number">1.10</span> Further reading<a href="ch-intro.html#further-reading" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An informal but useful discussion about probability appears in <span class="citation">Spiegelhalter (<a href="#ref-spiegelhalter2024probability" role="doc-biblioref">2024</a>)</span>.
A quick review of the mathematical foundations needed for statistics is available in the short book by <span class="citation">Fox (<a href="#ref-fox2009mathematical" role="doc-biblioref">2009</a>)</span>, as well as <span class="citation">Gill (<a href="#ref-gill2006essential" role="doc-biblioref">2006</a>)</span>.
A more comprehensive introduction to the mathematical background needed for advanced Bayesian modeling is <span class="citation">Jordan and Smith (<a href="#ref-jordan2008mathematical" role="doc-biblioref">2008</a>)</span>. <span class="citation">Morin (<a href="#ref-morin2016probability" role="doc-biblioref">2016</a>)</span> and <span class="citation">Blitzstein and Hwang (<a href="#ref-blitzstein2014introduction" role="doc-biblioref">2014</a>)</span> are accessible introductions to probability theory. <span class="citation">Ross (<a href="#ref-RossProb" role="doc-biblioref">2002</a>)</span> offers a more advanced treatment which discusses random variable theory and illustrates applications of probability theory. Some basic results about random variables are also discussed in the above-mentioned textbooks (for example, the expectation and variance of sums of random variables); some of these results will be needed in later chapters of the present book.
A good formal introduction to mathematical statistics (covering classical frequentist theory) is <span class="citation">Miller and Miller (<a href="#ref-millermiller" role="doc-biblioref">2004</a>)</span>. The freely available book by <span class="citation">Kerns (<a href="#ref-kerns2014introduction" role="doc-biblioref">2014</a>)</span> introduces frequentist and Bayesian statistics from the ground up in a very comprehensive and systematic manner; the source code for the book is available from <a href="https://github.com/gjkerns/IPSUR" class="uri">https://github.com/gjkerns/IPSUR</a>. The open-access book, <em>Probability and Statistics: a simulation-based introduction</em>, by Bob Carpenter is also worth studying: <a href="https://github.com/bob-carpenter/prob-stats" class="uri">https://github.com/bob-carpenter/prob-stats</a>. A thorough introduction to the matrix algebra needed for statistics, with examples using R, is provided in <span class="citation">Fieller (<a href="#ref-fieller" role="doc-biblioref">2016</a>)</span>. Commonly used probability distributions are presented in detail in <span class="citation">Miller and Miller (<a href="#ref-millermiller" role="doc-biblioref">2004</a>)</span>, <span class="citation">Blitzstein and Hwang (<a href="#ref-blitzstein2014introduction" role="doc-biblioref">2014</a>)</span>, and <span class="citation">Ross (<a href="#ref-RossProb" role="doc-biblioref">2002</a>)</span>. A useful reference for continuous univariate distributions is <span class="citation">Johnson, Kotz, and Balakrishnan (<a href="#ref-johnson1995continuous" role="doc-biblioref">1995</a>)</span>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references hanging-indent">
<div id="ref-blitzstein2014introduction">
<p>Blitzstein, Joseph K., and Jessica Hwang. 2014. <em>Introduction to Probability</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-fieller">
<p>Fieller, Nick. 2016. <em>Basics of Matrix Algebra for Statistics with R</em>. Boca Raton, FL: CRC Press.</p>
</div>
<div id="ref-fox2009mathematical">
<p>Fox, John. 2009. <em>A Mathematical Primer for Social Statistics</em>. Vol. 159. Sage.</p>
</div>
<div id="ref-gill2006essential">
<p>Gill, Jeff. 2006. <em>Essential Mathematics for Political and Social Research</em>. Cambridge University Press Cambridge.</p>
</div>
<div id="ref-johnson1995continuous">
<p>Johnson, Norman L., Samuel Kotz, and Narayanaswamy Balakrishnan. 1995. <em>Continuous Univariate Distributions, Volume 2</em>. Vol. 289. John Wiley; Sons.</p>
</div>
<div id="ref-jordan2008mathematical">
<p>Jordan, Dominic, and Peter Smith. 2008. <em>Mathematical Techniques: An Introduction for the Engineering, Physical, and Mathematical Sciences</em>. Oxford University Press, Oxford.</p>
</div>
<div id="ref-kerns2014introduction">
<p>Kerns, G. J. 2014. <em>Introduction to Probability and Statistics Using R</em>. Second Edition.</p>
</div>
<div id="ref-kolmogorov2018foundations">
<p>Kolmogorov, Andreı̆ Nikolaevich. 1933. <em>Foundations of the Theory of Probability: Second English Edition</em>. Courier Dover Publications.</p>
</div>
<div id="ref-AnnaLphd">
<p>Laurinavichyute, Anna. 2020. “Similarity-Based Interference and Faulty Encoding Accounts of Sentence Processing.” Dissertation, University of Potsdam. <a href="https://publishup.uni-potsdam.de/frontdoor/index/index/docId/50966">https://publishup.uni-potsdam.de/frontdoor/index/index/docId/50966</a>.</p>
</div>
<div id="ref-millermiller">
<p>Miller, I., and M. Miller. 2004. <em>John E. Freund’s Mathematical Statistics with Applications</em>. Upper Saddle River, NJ: Prentice Hall.</p>
</div>
<div id="ref-morin2016probability">
<p>Morin, David J. 2016. <em>Probability: For the Enthusiastic Beginner</em>. Createspace Independent Publishing Platform.</p>
</div>
<div id="ref-resnick2019probability">
<p>Resnick, Sidney. 2019. <em>A Probability Path</em>. Springer.</p>
</div>
<div id="ref-RossProb">
<p>Ross, Sheldon. 2002. <em>A First Course in Probability</em>. Pearson Education.</p>
</div>
<div id="ref-spiegelhalter2024probability">
<p>Spiegelhalter, David J. 2024. “Why Probability Probably Doesn’t Exist (but It Is Useful to Act Like It Does).” <em>Nature</em> 636 (8043): 560–63.</p>
</div>
<div id="ref-steyer2017probability">
<p>Steyer, Rolf, and Werner Nagel. 2017. <em>Probability and Conditional Expectation: Fundamentals for the Empirical Sciences</em>. Vol. 5. John Wiley &amp; Sons.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>When the sample space <span class="math inline">\(\Omega\)</span> is finite, any subset of <span class="math inline">\(\Omega\)</span> can be an event; in this case, the event space <span class="math inline">\(F\)</span> is the collection of all possible subsets of <span class="math inline">\(\Omega\)</span>. In set theory, a set of all the subsets of another set is called a power set, and the number of subsets of any set with <span class="math inline">\(n\)</span> elements is <span class="math inline">\(2^n\)</span>. For example, for a standard six-sided die, the sample space is the set <span class="math inline">\(\Omega=\{1,2,3,4,5,6\}\)</span> and the event space <span class="math inline">\(F\)</span> will contain <span class="math inline">\(2^6 = 64\)</span> sets: the empty set, six one-element sets, <span class="math inline">\(15\)</span> two-element sets, <span class="math inline">\(20\)</span> three-element sets, …, and one six-element set. Here are three assumptions we will always make about any event space: (a) Both the empty set and universal set (<span class="math inline">\(\Omega\)</span>) belong to the event space <span class="math inline">\(F\)</span>; (b) if <span class="math inline">\(E\)</span> is an event, then so is the complement of <span class="math inline">\(E\)</span>; and (c) for any list of events <span class="math inline">\(A_1, A_2,...\)</span> (finite or infinite), the phrase “<span class="math inline">\(A_1\)</span> or <span class="math inline">\(A_2\)</span> or …” describes another event.<a href="ch-intro.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Here, we use <span class="math inline">\(Y\)</span>, but we could have used any letter, such as <span class="math inline">\(X, Z,...\)</span>. Later on, in some situations we will use Greek letters like <span class="math inline">\(\theta, \mu, \sigma\)</span> to represent a random variable.<a href="ch-intro.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>The actual formal definition of random variable is more complex, and it is based on measure theory. A more rigorous definition can be found in, for example, <span class="citation">Steyer and Nagel (<a href="#ref-steyer2017probability" role="doc-biblioref">2017</a>)</span> and <span class="citation">Resnick (<a href="#ref-resnick2019probability" role="doc-biblioref">2019</a>)</span>.<a href="ch-intro.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>A notational aside: In frequentist treatments, the PMF would usually be written <span class="math inline">\(p(y;\theta)\)</span>, i.e., with a semi-colon rather than the conditional distribution marked by the vertical bar. The semi-colon is intended to indicate that in the frequentist paradigm, the parameters are fixed point values; by contrast, in the Bayesian paradigm, parameters are random variables. This has the consequence that for the Bayesian, the distribution of <span class="math inline">\(y\)</span>, <span class="math inline">\(p(y)\)</span> is really a conditional distribution, conditional on a random variable, here <span class="math inline">\(\theta\)</span>. For the frequentist, <span class="math inline">\(p(y)\)</span> requires some point value for <span class="math inline">\(\theta\)</span>, but it cannot be a conditional distribution because <span class="math inline">\(\theta\)</span> is not a random variable. We define conditional distributions later in this section.<a href="ch-intro.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Looking ahead to the rest of the book, in the Bayesian approach, the parameter of interest, <span class="math inline">\(\theta\)</span> in the present example, never has just a true unknown point value associated with it; instead, we use a probability distribution to express our belief about plausible values of the parameter. However, throughout this book, when generating simulated data, we will often use point values for parameters. These point values are adopted simply to evaluate the behavior of the model being investigated.<a href="ch-intro.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>R will compute the standard deviation by dividing by <span class="math inline">\(n-1\)</span>, not <span class="math inline">\(n\)</span>; this is because dividing by <span class="math inline">\(n\)</span> gives a biased estimate <span class="citation">(chapter 10 of Miller and Miller <a href="#ref-millermiller" role="doc-biblioref">2004</a>)</span>. This is not an important detail for our purposes, and in any case for large <span class="math inline">\(n\)</span> it doesn’t really matter whether one divides by <span class="math inline">\(n\)</span> or <span class="math inline">\(n-1\)</span>.<a href="ch-intro.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>There is a built-in convenience function, <code>sdcor2cov</code> in the <code>SIN</code> package that does this calculation, taking the vector of standard deviations (not the diagonal matrix) and the correlation matrix to yield the variance-covariance matrix: <code>sdcor2cov(stddev = sds, corr = corrmatrix)</code>.<a href="ch-intro.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Where does the above formula come from? It falls out from the law of total probability discussed above.<a href="ch-intro.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="about-the-authors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-introBDA.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
