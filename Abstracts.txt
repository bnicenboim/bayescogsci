Introduction to Bayesian Data Analysis for Cognitive Science

Six keywords:

Bayesian statistics
Bayesian hierarchical models
Generalized linear model
Computational cognitive modeling
Bayesian hypothesis testing
Bayesian model comparison


Chapter 1: Introduction

This chapter introduces the basics of probability theory informally, including
important concepts such as the law of total probability, and conditional
probability. The chapter also introduces the key properties of discrete and
continuous random variables, focusing on the binomial and normal random
variables. Two other important topics covered are bivariate/multivariate
discrete and continuous distributions, and the idea of integrating out a
variable.


Chapter 2: Introduction to Bayesian data analysis

This chapter introduces the foundations ideas behind Bayesian inference,
starting with deriving Bayes' rule, and then showing, through a simple example
involving the binomial likelihood and the beta distribution, how the posterior
distribution of a parameter can be analytically derived given a likelihood
function and a prior distribution for the parameter the parameter. The chapter
also discusses how the prior, likelihood, and posterior can be graphically
summarized. An important observation that is demonstrated is that the posterior
mean is a compromise between the prior mean and the maximum likelihood estimate.

Chapter 3: Computational Bayesian data analysis

This chapter introduces the basic usage of the brms library in R, focusing on a
model with a normal likelihood. The chapter demonstrates how the posterior distribution
of a parameter can be derived using sampling rather than through the analytical
approach discussed in chapter 2. The sensitivity of the posterior to the prior
specification is discussed, and different categories of prior (ranging from
flat, uninformative priors to informative priors) are summarized. Two other
important concepts introduced are prior and posterior predictive distributions.
The influence of the likelihood function in determining the posterior is also
discussed.

Chapter 4: Bayesian regression models

Here, we introduce the simple regression model, including a predictor variable, and
decide on a likelihood and priors, implementing the model in brms. We introduce
here the lognormal likelihood and show how to communicate the results of the
modeling. Logistic regression is also introduced in this chapter, with a
discussion of the implications of prior choice in logistic regression models;
back-transformation of the estimated parameters from log-odds space to
probability space is also discussed.

Chapter 5: Bayesian hierarchical models

This chapter introduces an important class of model, the hierarchical model. In
particular, here we consider data with repeated measurements from a group (e.g.,
subjects in an experiment), and show how the model can take into account the
repeated measures nature of the data in an experiment design. Five types of
hierarchical regression models are introduced: the no-pooling model, the varying
intercepts model, and the varying intercepts and slopes model (with and without
a correlation between them), and a distributional regression model, in which
each grouping variable (e.g., subject) is assumed to have its own residual
standard deviation parameter.

Chapter 6: The Art and Science of Prior Elicitation

Here we discuss how one can derive/elicit priors for parameters in a model. A
practical example from psycholinguistics is presented: the priors for the
parameters in a hierarchical regression model of self-paced reading data are
developed step by step in order to illustrate how one can reason about the
priors in a model. The chapter also discusses how one can systematically elicit
priors from domain experts, or from meta-analyses, and using posteriors from
previous experiments when planning the analysis for a future study.

Chapter 7: Workflow

This chapter discusses the process of model-building in detail, focusing on
asking and answering questions such as: are the model assumptions consistent
with domain knowledge; how sensitive is the model to prior choice; and does the
model adequately characterize the data? An important model-validation method
called simulation-based calibration is discussed.

Chapter 8: Contrast coding

This chapter introduces the theory behind contrast coding for planned
experiments that usually have a factorial design. The central idea introduced
here is that of the hypothesis matrix, and an R package called hypr is
introduced for defining contrasts. Different types of contrast coding are
discussed: sum contrasts, Helmert contrasts, sliding or successive difference
contrasts, and polynomial contrasts.

Chapter 9: Contrast coding for designs with two predictor variables

This chapter introduces contrast coding for experiment designs that have
multifactorial designs (the 2x2 factorial design is the most common example of
these). Analysis of variance coding and nested contrast coding are introduced.

Chapter 10: Introduction to the probabilistic programming language Stan

The basic building blocks of the programming language Stan are introduced in
this chapter. Stan is used to develop two simple examples that involve the
normal and binomial likelihoods. Then, the Stan syntax for simple regression
models is introduced.

Chapter 11: Hierarchical models and reparameterization

This chapter extends chapter 10 by incrementally introducing the Stan syntax for building
increasingly complex hierarchical models: varying intercepts models,
and varying intercepts and slopes models with and without correlations.

Chapter 12: Custom distributions

Sometimes it becomes necessary to define a custom probability density function (PDF).
The basics of writing one's own PDF are discussed here, including a discussion about how
to include a Jacobian adjustment, when a a change of variables is required. Two simple examples are discussed to illustrate the process.

Chapter 13: Evidence synthesis and measurements with error

Two important and relatively underutilized classes of hierarchical model are
introduced here: random-effects meta-analysis models, and measurement error
models. Meta-analysis models are useful for synthesizing and quantifying what is
known from previously published or publicly available data. Measurement error
models take into account the fact that a dependent variable (and/or an
independent variable) may have associated with it some variability; this class
of model takes this variability into account in the regression modeling
framework.

Chapter 14: Introduction to model comparison

Having defined more than one model, an obvious question that arises is:
which model represents the data generative process better? One graphical
tool to evaluate models is prior and posterior predictive checks. However,
there are more quantitative ways to investigate model fit. In this short
chapter, we discuss some important ideas to consider when engaging in model
comparison.

Chapter 15: Bayes factors

The Bayesian analog to the frequentist likelihood ratio test (aka the ANOVA) is the Bayes factor.
In this chapter, we first explain the definition of the Bayes factor, and then illustrate its
application via some simple examples. An important detail regarding the Bayes factor
is that it can be very sensitive to the prior specification; we discuss the nuances
of and the pitfalls associated with computing Bayes factors.

Chapter 16: Cross-validation

An alternative to using Bayes factors for model comparison is cross-validation.
In an approach called k-fold cross-validation, k subsets of the data (called the
training sets) are used to compute the posteriors, and then the model fit is
evaluated on k sets of held-out data. The average deviation between the model
predictions and the held-out data is computed; this average fit is called the
expected log pointwise density (elpd). A more computationally intensive approach
is to hold out a single data point and compute the posteriors on the remaining
data; this procedure is then repeated for every data point, and then the
estimated elpd computed. This is called leave-one-out cross-validation (LOO-CV). A faster
approximation of LOO-CV is available in the loo package, this is called PSIS-LOO.
We discuss the details of computing estimated elpd values in Stan as well as brms.

Chapter 17: Introduction to cognitive modeling

This chapter transitions to building customized process models within Stan;
the focus is on defining latent (unobservable) processes that represent
theoretical assumptions about the particular cognitive process being modeled.
In this chapter, we discuss the characteristics and types of such models, and the advantages of
developing these kinds of process models.

Chapter 18: Multinomial processing trees

This chapter introduces a class of cognitive model that is very widely used in
mathematical psychology. These models allow the researcher to define
probabilistically occurring latent (not directly observable) sub-events that can
lead to an observed categorical behavioral response. We show through a practical example how
the models can be implemented in Stan, and how such models can be made
hierarchical.

Chapter 19: Mixture models

Here, we discuss an important class of model whereby the data are assumed to come
from two or more distinct distributions. A multivariate mixture model is discussed
via a practical example; such a model includes two dependent variables simultaneously
(in our example, accuracies and response times). A hierarchical version of
such a model is also illustrated.

Chapter 20: A simple accumulator model to account for choice response time

This chapter introduces the log-normal race model, a type of sequential sampling
model used to account for both response times and accuracy in decision-making
tasks. We apply the model to a lexical decision task, using hierarchical
modeling to analyze how word frequency affects performance. Posterior predictive
checks, including quantile probability plots, are used to evaluate model fit.
The chapter also discusses how to extend the model to account for contaminant
responses--anticipation which are unrelated to the decision. Finally, we discuss
the model's limitations, particularly its inability to capture fast errors, and
we suggest ways to address these issues with more complex models.
