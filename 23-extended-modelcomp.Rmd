# Model comparison - Extended


## Credible intervals should not be used to reject a null hypothesis {#app-null}

Researchers often incorrectly use \index{Credible intervals for null hypothesis testing} credible intervals for null hypothesis testing, that is, to test whether a parameter $\beta$ is zero or not. A common approach is to check whether zero is included in the 95\% credible interval for the parameter $\beta$; if it is, then the null hypothesis that the effect is zero is accepted; and if zero is outside the interval, then the null is rejected. For example, in a  tutorial paper that two of the authors of this book wrote [@NicenboimVasishth2016], we incorrectly suggest that the credible interval can be used to reject the hypothesis that the $\beta$ is zero. This is generally not the correct approach. 
The problem with this approach is that it is a  heuristic that will work in some cases and might be misleading in others [for an example, see @SampleSizeCBB2021]. Unfortunately, when they will work or not is in fact not well-defined.

Why is the credible-interval approach only a \index{Heuristic} heuristic?
One line of (generally incorrect) reasoning that justifies looking at the overlap between credible intervals and zero is based on the fact that the most likely values of $\beta$ lie within 95% credible interval.^[This is also strictly true only in a highest density interval (HDI), this is a credible interval where all the points within the interval have a higher probability density than points outside the interval. However,  when posterior distributions are symmetrical, these intervals are virtually identical to the equal-tail intervals we use in this book.] This entails that if zero is outside the interval, it must have a low probability density. This is true, but it's meaningless: Regardless of where zero lies (or any point value), zero will have a probability mass of exactly zero since we are dealing with a continuous distribution.
The lack of overlap doesn't tell us how much posterior probability the null model has. 

A partial solution could be to look at a probability interval close to zero rather than zero (e.g., an interval of, say, $-2$ to $2$ ms in a response time experiment), so that we obtain a non-zero probability mass. While the lack of overlap would be slightly more informative, excluding a small interval can be problematic when the prior probability mass of that interval is very small to begin with (as was the case with the regularizing priors we assigned to our parameters). @rouder2018bayesian show that if prior probability mass is added to the point value zero using a \index{Spike-and-slab prior} *spike-and-slab* prior (or if probability mass is  added to the small interval close to zero if one considers that equivalent to the null model), looking at whether zero is in the 95% credible interval is analogous to the Bayes factor. Unfortunately, the *spike-and-slab* prior cannot be incorporated in Stan, because it relies on a discrete parameter. However, other programming tools (like PyMC3, JAGS, or Turing) can be used if such a prior needs to be fit; see the further readings at the end of the chapter.

Rather than looking at the overlap of the 95% credible interval, we might be tempted to conclude that there is evidence for an effect because the probability that a parameter is positive is high, that is $P(\beta > 0) >> 0.5$. However, the same logic from the previous paragraph renders this meaningless. Given that the probability mass of a point value, $P(\beta = 0)$, is zero, what we can conclude from $P(\beta > 0) >> 0.5$ is that $\beta$ is very likely to be positive rather than negative, but we can't make any assertions about whether $\beta$ is exactly zero. 

As we saw, the main problem with these heuristics is that they ignore that the \index{Null model} null model is a separate hypothesis. In many situations, the null hypothesis may not be of interest, and it might be perfectly fine to base our conclusions on credible intervals or $P(\beta > 0)$. The problem arises when these heuristics are used to provide \index{Evidence in favor or against the null hypothesis} evidence in favor or against the null hypothesis. If one wants to argue about the evidence in favor of or against a null hypothesis, Bayes factors or cross-validation will be needed. These are discussed in the next two chapters.

How can credible intervals be used sensibly? The \index{Region of practical equivalence} region of practical equivalence \index{ROPE} (ROPE) approach [@spiegelhalter1994bayesian; @Freedman1984; and, more recently, @kruschke2018bayesian; @kruschke2014doing] is a reasonable alternative to hypothesis testing and arguing for or against a null. This approach is related to the spike-and-slab discussion above. In the ROPE approach, one can define a range of values for a target parameter that is predicted before the data are seen. Of course, there has to be a principled justification for choosing this range a priori; an example of a principled justification would be the prior predictions of a computational model. Then, the overlap (or lack thereof) between this predicted range and the observed credible interval can be used to infer whether one has estimates consistent (or partly consistent) with the predicted range. Here, we are not ruling out any null hypothesis, and we are not using the credible interval to make a decision like "the null hypothesis is true/false."  

There is one situation where credible intervals could arguably be used to carry out a null hypothesis test. When priors are flat, credible intervals can show frequentist properties, making it reasonable to check whether zero falls within the credible interval. For example, @newall2023evaluation use credible intervals as confidence intervals after calibration. They explicitly verify that 5% of the 95% credible intervals exclude zero when no effect exists. When using such an approach, a verification step would be necessary. We don't discuss this approach any further because our aim in this part of the book is not to derive frequentist statistics from Bayesian analysis, but to use Bayesian methods for obtaining posterior probabilities and Bayes factors, focusing on Bayesian hypothesis testing.

