# Contrast coding with two predictor variables {#ch-coding2x2}

Chapter\ \@ref(ch-contr) provides a basic introduction into contrast coding in situations where there is one predictor variable, i.e., one factor, whose effects can be estimated using a specified contrast matrix. Here, we investigate how contrast coding generalizes to situations where there is more than one predictor variable. This could either be a situation where two factors are present or where one factor is paired with a continuous predictor variable, i.e., a covariate. We first discuss contrast coding for the case of two factors (for $2 \times 2$ designs; section\ \@ref(sec-MR-ANOVA)) and then go on to investigate situations where one predictor is a factor and the other predictor is a covariate (section\ \@ref(sec-contrast-covariate)). One problem in the analysis of interactions occurs in situations where the model is not linear, but has some non-linear link function, for example in logistic models, or when assuming a log-normally distributed dependent variable. In these situations, the model makes predictions for each condition (i.e., design cell) at the latent level of the linear model. Sometimes it is important to translate these model predictions to the level of the observations (e.g., to probabilities in a logistic regression model). We will discuss how this can be implemented in section\ \@ref(sec-interactions-NLM). 

## Contrast coding in a factorial \index{$2 \times 2$ design} $2 \times 2$ design {#sec-MR-ANOVA}

In section\ \@ref(sec-4levelFactor) of chapter \@ref(ch-contr), we used a data set with one 4-level factor. Here, we assume that the same four means come from an $A(2) \times B(2)$ between-subject-factor design rather than an F(4) between-subject-factor design. Load the simulated data and show summary statistics in Table\ \@ref(tab:cTab4Means) and in Figure \@ref(fig:twobytwosimdatFig). The means and standard deviations are exactly the same as in Figure \@ref(fig:helmertsimdatFig) and in Table\ \@ref(tab:cTab3Means).

```{r, echo=FALSE, message=FALSE}
data("df_contrasts4")
table4 <- df_contrasts4 %>% group_by(A, B) %>% # plot interaction
  summarize(N = length(DV), M = mean(DV), SD = sd(DV), SE = SD / sqrt(N))
GM <- mean(table4$M) # Grand Mean

table4a <- as.data.frame(table4)
names(table4a) <- c(
  "Factor A", "Factor B", "N data",
  "Means", "Std. dev.", "Std. errors"
)
```

```{r twobytwosimdatFig, fig=TRUE, include=TRUE, echo=FALSE, cache=FALSE, fig.height=2.2, fig.cap = "Means and error bars (showing standard errors) for a simulated data set with a two-by-two  between-subjects factorial design."}
#(plot2 <- qplot(x = A, y = M, group = B, linetype = B, shape = B, data = table4, geom = c("point", "line")) +
(plot2 <- ggplot(aes(x = A, y = M, group = B, linetype = B, shape = B), data = table4) +
  geom_point() + geom_line() +
  labs(y = "Dependent variable", x = "Factor A", colour = "Factor B", linetype = "Factor B", shape = "Factor B") +
  geom_errorbar(aes(max = M + SE, min = M - SE), width = 0))
```

```{r cTab4Means, echo=FALSE, results = "asis"}
# apa_table(table4a, placement="b", digits=1,
#    caption="Summary statistics per condition for the simulated data.")
kable(table4a %>%
     mutate(across(where(is.numeric), ~ paste0("$",round(.x,1),"$"))),
  digits = 1, booktabs = TRUE, vline = "", # format="latex",
  escape = FALSE,
  caption = "Summary statistics per condition for the simulated data."
)
```


In order to carry out a $2\times 2$ \index{Analysis of variance} ANOVA-type \index{Main effects} (main effects and \index{Interaction} interaction) analysis, one needs \index{Sum contrasts} sum contrasts in the linear model. (This is true for factors with two levels, but does not generalize to factors with more levels.) The results of such an analysis are shown in Table\ \@ref(tab:table18b). (This analysis uses sum contrasts: $-1$/$+1$; however, scaled sum contrasts $-0.5$/$+0.5$ could also be used; in this case, the priors would need to be adapted to the respective scale.)

```{r, echo=TRUE, message=FALSE}
contrasts(df_contrasts4$A) <- contr.sum(2)
contrasts(df_contrasts4$B) <- contr.sum(2)
```

```{r fitABsum, echo=TRUE, message=FALSE, results="hide"}
fit_AB.sum <- brm(DV ~ A * B,
                  data = df_contrasts4,
                  family = gaussian(),
                  prior = c(prior(normal(20, 50), class = Intercept),
                            prior(normal(0, 50), class = sigma),
                            prior(normal(0, 50), class = b)))
```
```{r, echo=FALSE, eval=FALSE}
fixef(fit_AB.sum)
```

```{r, echo=FALSE}
tab18 <- data.frame(round(data.frame(fixef(fit_AB.sum)), 2))
names(tab18) <- c("Estimate", "Est. Error", "2.5\\%", "97.5\\%")
tab18 <- as.data.frame(cbind(Predictor = row.names(tab18), tab18))
row.names(tab18) <- NULL
```


```{r table18b, echo=FALSE, eval=TRUE, results="asis"}

kable(tab18 %>%
  mutate(across(where(is.numeric), ~ paste0("$",formatC(round(.x, 2), format = "f", digits=2), "$"))),
  #mutate(across(where(is.numeric), ~ paste0("$",round(.x,2),"$"))),
  #mutate(across(where(is.numeric), ~ paste0("",round(.x,2),""))),
  digits = 2, booktabs = TRUE, vline = "",
  escape= FALSE, #align=c("l",rep("r",4)),
  caption = "Bayesian linear model of a 2 x 2 design with sum contrasts.")

```


Next, we reproduce the $A(2) \times B(2)$ - ANOVA with contrasts specified for the corresponding one-way $F(4)$ ANOVA, that is by treating the $2 \times 2 = 4$ condition means as four levels of a single factor $F$. In other words, we go back to the data frame simulated for the analysis of repeated contrasts (see chapter\ \@ref(ch-contr), section\ \@ref(sec-4levelFactor)). We first define weights for condition means according to our hypotheses, invert this matrix, and use it as the contrast matrix for factor $F$. We define weights of $1/4$ and $-1/4$. We do so because (a) we want to compare the mean of two conditions to the mean of two other conditions (e.g., factor $A$ compares $\frac{F1 + F2}{2}$ to $\frac{F3 + F4}{2}$). Moreover, (b) we want coefficients to code half the difference between condition means, reflecting sum contrasts. Together $(a+b)$, this yields weights of $1/2 \cdot 1/2 = 1/4$. The resulting contrast matrix contains contrast coefficients of $+1$ or $-1$, showing that we successfully implemented sum contrasts. The results are identical to the previous models.

```{r, echo=FALSE}
ginv2 <- function(x) { # define a function to make the output nicer
  fractions(provideDimnames(ginv(x), base = dimnames(x)[2:1]))
}
```

```{r, echo=TRUE, message=FALSE}
HcInt <-
  rbind(A = c(F1 = 1 / 4, F2 = 1 / 4, F3 = -1 / 4, F4 = -1 / 4),
        B = c(F1 = 1 / 4, F2 = -1 / 4, F3 = 1 / 4, F4 = -1 / 4),
        AxB = c(F1 = 1 / 4, F2 = -1 / 4, F3 = -1 / 4, F4 = 1 / 4))
t(fractions(HcInt))
(XcInt <- ginv2(HcInt))
contrasts(df_contrasts3$F) <- XcInt
```
```{r fit4sum, echo=TRUE, message=FALSE, results="hide"}
fit_F4.sum <- brm(DV ~ F,
                  data = df_contrasts3,
                  family = gaussian(),
                  prior = c(prior(normal(20, 50), class = Intercept),
                            prior(normal(0, 50), class = sigma),
                            prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_F4.sum)
```

This shows that it is possible to specify the contrasts not only for each factor (e.g., here in the $2 \times 2$ design) separately. Instead, one can also pool all experimental conditions (or design cells) into one large factor (here factor $F$ with $4$ levels), and specify the contrasts for the main effects and for the interactions in the resulting one large contrast matrix simultaneously.

In this approach, it can again be very useful to apply the \index{\texttt{hypr}} `hypr` package to construct contrasts for a \index{$2 \times 2$ design} $2 \times 2$ design. The first parameter estimates the main effect $A$, i.e., it compares the average of $F1$ and $F2$ to the average of $F3$ and $F4$. The second parameter estimates the main effect $B$, i.e., it compares the average of $F1$ and $F3$ to the average of $F2$ and $F4$. We code direct differences between the averages, i.e., we implement scaled sum contrasts instead of sum contrasts. This is shown below: the contrast matrix contains coefficients of $+1/2$ and $-1/2$ instead of $+1$ and $-1$. The \index{Interaction} interaction term estimates the difference between differences, i.e., the difference between $F1 - F2$ and $F3 - F4$.

```{r, echo=TRUE, message=FALSE}
hAxB <- hypr(A = (F1 + F2) / 2 ~ (F3 + F4) / 2,
             B = (F1 + F3) / 2 ~ (F2 + F4) / 2,
             AxB = (F1 - F2) ~ (F3 - F4))
hAxB
contrasts(df_contrasts3$F) <- contr.hypothesis(hAxB)
```
```{r, echo=TRUE, message=FALSE, results="hide"}
fit_F4hypr <- brm(DV ~ F,
                  data = df_contrasts3,
                  family = gaussian(),
                  prior = c(prior(normal(20, 50), class = Intercept),
                            prior(normal(0, 50), class = sigma),
                            prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_F4hypr)
```

The results show that the estimates for the main effects have (approximately) double the size as compared to the $\pm 1$ sum contrasts--this is the result of the scaling that we applied. I.e., the main effects now directly estimate the difference between averages. The interaction estimates the difference between differences. Importantly, if we change the scaling of the contrasts, we also need to adapt the scaling of the priors. If appropriate priors are used, then both contrasts would lead to the same hypothesis tests if one were doing hypothesis testing using Bayes factors. Thus, the `hypr` package can be used to code hypotheses in a $2 \times 2$ design.

An alternative way to code main effects and interactions is to use the `ifelse` command in R. For example, if we want to use $\pm 1$ sum contrasts in the above example, we can specify the contrasts for the main effects as vectors:

```{r}
A <- ifelse(df_contrasts3$F %in% c("F1", "F2"), -1, 1)
B <- ifelse(df_contrasts3$F %in% c("F1", "F3"), -1, 1)
```

Now, defining the interaction is simply a matter of multiplying the two vectors:

```{r}
AxB <- A * B
```

Now the main effects and interaction can be directly interpreted as differences between averages and as differences between differences. If one wants the interaction term to be on the same scale as the main effects, it would need to be multiplied by 2, however, then its interpretation would not be straightforward (i.e., "the difference between differences") any more, but a scaled variant of this.
Thus, putting the interaction on the same scale as the main effects has the advantage that it ensures that the prior has the same scaling across the different terms. However, it has the disadvantage that the interpretation of the interaction term is not straightforward.

```{r}
## rescale:
(AxB <- A * B * 2)
```

Instead of using $\pm 1$ coding, an alternative is to use $\pm 1/2$ coding when using this approach:

```{r}
A <- ifelse(df_contrasts3$F %in% c("F1", "F2"), -1 / 2, 1 / 2)
B <- ifelse(df_contrasts3$F %in% c("F1", "F3"), -1 / 2, 1 / 2)
## interaction:
(AxB <- A * B)
```

This kind of vector-based contrast coding is convenient for more complex designs, such as $2\times 2\times 2$ factorial designs. However, in such complex designs, the `ifelse()` command can become cumbersome, so a better approach is to use the `merge()` function in `R`. 

First, define a data frame that represents the main effects and interaction contrasts for the four conditions.

```{r}
(contr_coding <- data.frame(Condition = c("F1", "F2", "F3", "F4"),
                            A = c(-1 / 2, -1 / 2, 1 / 2, 1 / 2),
                            B = c(-1 / 2, 1 / 2, -1 / 2, 1 / 2),
                            AxB = c(1 / 2, -1 / 2, -1 / 2, 1 / 2)))
```

Then, one can merge this data frame with the data frame that contains the data:

```{r}
(df_contrasts3 <- merge(df_contrasts3,
                        contr_coding,
                        by.x = "F",
                        by.y = "Condition"))
```

### \index{Nested effects} Nested effects {#nestedEffects}

One can estimate effects that do not correspond directly to main effects and interaction of the traditional ANOVA. For example, in a $2 \times 2$ experimental design, where factor $A$ codes word frequency (low/high) and factor $B$ is part of speech (noun/verb), one can estimate the effect of word frequency within nouns and the effect of word frequency within verbs. Formally, $A_{B1}$ versus $A_{B2}$ are nested within levels of $B$. Said differently, \index{Simple effects} simple effects of factor $A$ are estimated for each of the levels of factor $B$.
In this version, we estimate the main effect of part of speech ($B$; as in traditional ANOVA). Instead of also estimating the second main effect word frequency, $A$, and the interaction, we estimate (1) the differences between the two levels of word frequency $A$ in the first level of $B$ (i.e., nouns), and (2) the difference between the two levels of word frequency $A$ in the second level of $B$ (i.e., verbs). In other words, we estimate the differences between the two levels of $A$ in each of the levels of $B$. Often, researchers have hypotheses about these differences, and not about the interaction.

```{r, echo=TRUE, message=FALSE}
HcNes <-
  rbind(B = c(F1 = 1 / 2, F2 = -1 / 2, F3 = 1 / 2, F4 = -1 / 2),
        B1xA = c(F1 = -1, F2 = 0, F3 = 1, F4 = 0),
        B2xA = c(F1 = 0, F2 = -1, F3 = 0, F4 = 1))
t(fractions(HcNes))
(XcNes <- ginv2(HcNes))
contrasts(df_contrasts3$F) <- XcNes
```
```{r, echo=TRUE, message=FALSE, results="hide"}
fit_Nest <- brm(DV ~ F,
                data = df_contrasts3,
                family = gaussian(),
                prior = c(prior(normal(20, 50), class = Intercept),
                          prior(normal(0, 50), class = sigma),
                          prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_Nest)
```

The regression coefficients estimate the \index{Grand mean} grand mean, the difference for the \index{Main effects} main effect of part of speech ($B$) and the two differences (for $A$; i.e., simple main effects) within the two levels (noun and verb) of part of speech ($B$).

These \index{Custom nested contrasts} custom nested contrasts' columns are scaled versions of the corresponding \index{Hypothesis matrix} hypothesis matrix. This is the case because the columns are orthogonal. This illustrates one advantage of \index{Orthogonal contrasts} orthogonal contrasts for the interpretation of regression coefficients: the underlying comparisons being estimated are already clear from the contrast matrix.

There is also a built-in R formula specification for nested designs. The order of factors in the formula from left to right specifies a top-down order of nesting within levels, i.e., here factor $A$ (word frequency) is nested within levels of the factor $B$ (part of speech). This yields the same result as our previous result based on custom nested contrasts:

```{r, echo=TRUE, message=FALSE, results="hide"}
contrasts(df_contrasts4$A) <- c(-0.5, +0.5)
contrasts(df_contrasts4$B) <- c(+0.5, -0.5)
fit_Nest2 <- brm(DV ~ B / A,
                 data = df_contrasts4,
                 family = gaussian(),
                 prior = c(prior(normal(20, 50), class = Intercept),
                           prior(normal(0, 50), class = sigma),
                           prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_Nest2)
```

In cases such as these, where $A_{B1}$ vs. $A_{B2}$ are nested within levels of $B$, it is necessary to include the effect of $B$ (part of speech) in the model, even if one is only interested in the effect of $A$ (word frequency) within levels of $B$ (part of speech). Leaving out factor $B$ in this particular case of a between-subjects deisn would increase posterior uncertainty if the data are fully balanced, and could lead to biases in parameter estimation if the data are not fully balanced.

Again, we show how nested contrasts can be easily implemented using \index{\texttt{hypr}} `hypr`:

```{r, echo=TRUE, message=FALSE}
hNest <- hypr(B = (F1 + F3) / 2 ~ (F2 + F4) / 2,
              B1xA = F3 ~ F1,
              B2xA = F4 ~ F2)
hNest
contrasts(df_contrasts3$F) <- contr.hypothesis(hNest)
```
```{r, echo=TRUE, message=FALSE, results="hide"}
fit_NestHypr <-
  brm(DV ~ F,
      data = df_contrasts3,
      family = gaussian(),
      prior = c(prior(normal(20, 50), class = Intercept),
                prior(normal(0, 50), class = sigma),
                prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_NestHypr)
```

Of course, we can also ask the reverse question: Are there differences for part of speech ($B$) in the levels of word frequency ($A$; in addition to estimating the main effect of word frequency, $A$)? That is, do nouns differ from verbs for low-frequency words ($B_{A1}$) and do nouns differ from verbs for high-frequency words ($B_{A2}$)?

```{r, echo=TRUE, message=FALSE}
hNest2 <- hypr(A = (F1 + F2) / 2 ~ (F3 + F4) / 2,
               A1xB = F2 ~ F1,
               A2xB = F4 ~ F3)
hNest2
contrasts(df_contrasts3$F) <- contr.hypothesis(hNest2)
```
```{r, echo=TRUE, message=FALSE, results="hide"}
fit_Nest2Hypr <-
  brm(DV ~ F,
      data = df_contrasts3,
      family = gaussian(),
      prior = c(prior(normal(20, 50), class = Intercept),
                prior(normal(0, 50), class = sigma),
                prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_Nest2Hypr)
```

Regression coefficients estimate the \index{Grand mean} grand mean, the difference for the \index{Main effects} main effect of word frequency ($A$) and the two part of speech effects (for $B$; i.e., \index{Simple effects} simple main effects) within levels of word frequency ($A$).

An important issue to keep in mind is that if, using nested contrasts, one finds a large effect in one nested comparison (e.g., `FA2xB`) but no difference or a smaller effect in the other comparison (here, `FA1xB`), one cannot conclude that there is an interaction; to argue for an interaction, one would have to test for the main effects and interaction using the ANOVA contrast coding [@nieuwenhuis2011erroneous].

### \index{Interaction} Interactions between contrasts

In a $2 \times 2$ experimental design, the results from sum contrasts are equivalent to typical ANOVA results that we see in frequentist analyses. This means that sum contrasts assess the main effects and the interactions. One interesting question that arises here is: what would happen in a \index{$2 \times 2$ design} $2 \times 2$ design if we had used \index{Treatment contrasts} treatment contrasts instead of sum contrasts? Is it still possible to meaningfully interpret the results from the treatment contrasts in a simple $2 \times 2$ design?

This leads us to a very important principle in interpreting results from contrasts: When interactions between contrasts are included in a model, then the results for one contrast actually depend on the specification of the other contrast(s) in the analysis! This may be counter-intuitive at first, but it is very important and essential to keep in mind when interpreting results from contrasts. How does this work in detail?

The general rule to remember is that the effect of one contrast measures its effect at the location $0$ of the other contrast(s) in the analysis. This can be seen in the regression equation of a $2 \times 2$ design with factors $A$ and $B$:

\begin{equation}
E[Y] = \alpha + \beta_A A + \beta_B B + \beta_{A \times B} A \times B
\end{equation}

If we set the predictor $B$ to zero, then the equation simplifies to:

\begin{equation}
E[Y] = \alpha + \beta_A A
\end{equation}

Thus, now we can see the "pure" effect of $A$.

What does that mean practically? Let us consider the example that we use two treatment contrasts in a $2 \times 2$ design. Here are the results from the linear model

```{r, echo=TRUE, message=FALSE, results="hide"}
contrasts(df_contrasts4$A) <- c(0, 1)
contrasts(df_contrasts4$B) <- c(0, 1)
fit_treatm <- brm(DV ~ B * A,
                  data = df_contrasts4,
                  family = gaussian(),
                  prior = c(prior(normal(20, 50), class = Intercept),
                            prior(normal(0, 50), class = sigma),
                            prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_treatm)
```

Let's take a look at the effect of factor $A$. How can we interpret what this measures? This effect actually estimates the effect of factor $A$ at the "location" where factor $B$ is coded as $0$. Factor $B$ is coded as a treatment contrast, that is, it codes a zero at its baseline condition, which is $B1$. Thus, the effect of factor $A$ estimates the effect of $A$ nested within the baseline condition of $B$, i.e., a \index{Simple effects} simple effect. We take a look at the data presented in Figure\ \@ref(fig:twobytwosimdatFig), what this \index{Nested effects} nested effect should be. Figure\ \@ref(fig:twobytwosimdatFig) shows that the effect of factor $A$ nested in $B1$ is $0$. If we now compare this to the results from the linear model, it is indeed clear that the effect of factor $A$ is exactly estimated as $0$. As expected, when factor $B$ is coded as a treatment contrast, the effect of factor $A$ estimates the effect of $A$ nested within the baseline level of factor $B$.

Next, consider the effect of factor $B$. According to the same logic, this effect estimates the effect of factor $B$ at the "location" where factor $A$ is $0$. Factor $A$ is also coded as a treatment contrast, that is, it codes its baseline condition $A1$ as $0$. The effect of factor $B$ estimates the effect of $B$ nested within the baseline condition of $A$. Figure\ \@ref(fig:twobytwosimdatFig) shows that this effect should be $10$.

How do we know what the "location" is, where a contrast applies? For the treatment contrasts discussed here, it is possible to reason this through because all contrasts are coded as $0$ or $1$. How can one derive the "location" in general? What we can do is to look at the comparisons that are estimated when using the treatment contrasts (or in case we use Bayes factors, which \index{Hypothesis} hypotheses are tested) in the presence of an interaction between them by using the \index{Generalized matrix inverse} generalized matrix inverse. We go back to the default treatment contrasts. Then we extract the contrast matrix from the design matrix:

```{r, echo=TRUE, message=FALSE}
contrasts(df_contrasts4$A) <- contr.treatment(2)
contrasts(df_contrasts4$B) <- contr.treatment(2)
XcTr <- df_contrasts4 %>%
  group_by(A, B) %>%
  summarise() %>%
  model.matrix(~ 1 + A * B, .) %>%
  as.data.frame() %>%
  as.matrix()
rownames(XcTr) <- c("A1_B1", "A1_B2", "A2_B1", "A2_B2")
XcTr
```

This shows the treatment contrast for factors $A$ and $B$, and their interaction. We can now assign this \index{Contrast matrix} contrast matrix to a \index{\texttt{hypr}} `hypr` object. `hypr` automatically converts the contrast matrix into a \index{Hypothesis matrix} hypothesis matrix, such that we can read from the hypothesis matrix which comparison are being estimated by the different contrasts.

```{r, echo=TRUE}
htr <- hypr() # initialize empty hypr object
cmat(htr) <- XcTr # assign contrast matrix to hypr object
htr # look at the resulting hypothesis matrix
```

The same result is obtained by applying the generalized inverse to the contrast matrix (this is what `hypr()` does as well). An important fact is that when we apply the generalized inverse to the contrast matrix, we obtain the corresponding hypothesis matrix [for details, see @schadHowCapitalizePriori2020].

```{r, echo=TRUE}
t(ginv2(XcTr))
```

As discussed above, the effect of factor $A$ estimates its effect nested within the baseline level of factor $B$. Likewise, the effect of factor $B$ estimates its effect nested within the baseline level of factor $A$. The term $A:B$ always tests the interaction between both factors, irrespective of the centering of $A$ and $B$.

How does this work for \index{Sum contrasts} sum contrasts? They do not have a baseline condition that is coded as $0$. In sum contrasts, the average of the contrast coefficients is $0$. Therefore, effects estimate the average effect across factor levels, i.e., they estimate a \index{Main effects} main effect. This is what is typically also tested in standard \index{Analysis of variance} ANOVA. Let's look at the example shown in Table\ \@ref(tab:table18b): given that factor $B$ has a sum contrast, the main effect of factor $A$ is estimated as the average across levels of factor $B$. Figure\ \@ref(fig:twobytwosimdatFig) shows that the effect of factor $A$ in level B1 is $10 - 10 = 0$, and in level $B2$ it is $20 - 40 = -20$. The average effect across both levels is $(0 - 20)/2 = -10$. Due to the sum contrast coding, we have to divide this by two, yielding an expected effect of $-10 / 2 = -5$. This is exactly what the effect of factor $A$ measures (see Table\ \@ref(tab:table18b), *Estimate* for $A1$).

Similarly, factor $B$ estimates its effect at the location $0$ of factor $A$. Again, $0$ is exactly the mean of the contrast coefficients from factor $A$, which is coded as a sum contrast. Therefore, factor $B$ estimates the effect of $B$ averaged across factor levels of $A$, i.e., the main effect of $B$. For factor level $A1$, factor $B$ has an effect of $10 - 20 = -10$. For factor level $A2$, factor $B$ has an effect of $10 - 40 = -30$. The average effect is $(-10 - 30)/2 = -20$, which again needs to be divided by $2$ due to the sum contrast. This yields exactly the estimate of $-10$ that is also reported in Table\ \@ref(tab:table18b) (*Estimate* for B1).

Again, we look at the hypothesis matrix for the main effects and the interaction:

```{r, echo=TRUE, message=FALSE}
contrasts(df_contrasts4$A) <- contr.sum(2)
contrasts(df_contrasts4$B) <- contr.sum(2)
XcSum <- df_contrasts4 %>%
  group_by(A, B) %>%
  summarise() %>%
  model.matrix(~ 1 + A * B, .) %>%
  as.data.frame() %>%
  as.matrix()
rownames(XcSum) <- c("A1_B1", "A1_B2", "A2_B1", "A2_B2")

hsum <- hypr() # initialize empty hypr object
cmat(hsum) <- XcSum # assign contrast matrix to hypr object
hsum # look at the resulting hypothesis matrix
```

This shows that each of the effects now does not compute nested comparisons any more, but that they rather estimate their effect averaged across conditions of the other factor. The averaging involves using weights of $1/2$ in the hypothesis matrix. Moreover, the regression coefficients in the sum contrast measure half the distance between conditions, leading to weights of $1/2 \cdot 1/2 = 1/4$ in the hypothesis matrix.

The general rule to remember from these examples is that when \index{Interaction} interactions between contrasts are estimated, what an effect of a factor estimates depends on the contrast coding of the other factors in the design! The effect of a factor estimates the effect nested within the location zero of the other contrast(s) in an analysis. If another contrast is centered, and zero is the average of this other contrasts' coefficients, then the contrast of interest estimates the average or \index{Main effects} main effect, averaged across the levels of the other factor. Importantly, this property, that the coding of the other factor determines the estimates of one factor, holds only when the interaction between two contrasts is included into a model. If the interaction is omitted and only effects are estimated, then there is no such influence.

This may be a very surprising result for interactions of contrasts. However, it is also essential to interpreting contrast coefficients involved in interactions. It is particularly relevant for the analysis of the default treatment contrast, where the main effects estimate \index{Nested effects} nested effects rather than average effects.

## One factor and one \index{Covariate} covariate {#sec-contrast-covariate}

### Estimating a \index{Group} group difference and controlling for a covariate

In this section we treat the case where there are again two predictor variables for one dependent variable, but where one predictor variable is a discrete factor, and the other is a continuous covariate. Let's assume we have measured some response time (RT), e.g., in a lexical decision task. We want to predict the response time based on each subject's IQ, and we expect that higher IQ leads to shorter response times. Moreover, we have two groups of each 30 subjects. These are coded as factor $F$, with factor levels $F1$ and $F2$. We assume that these two groups have obtained different training programs to optimize their response times on the task. Group $F1$ obtained a control training, whereas group $F2$ obtained training to improve lexical decisions. We want to estimate the extent to which the training for better lexical decisions in group $F2$ leads to shorter response times compared to the control group $F1$. This is our main question of interest here, i.e., the extent to which the training program in $F2$ leads to faster response times compared to the control group $F1$. We load the data, which is a simulated data set.

```{r, echo=TRUE}
data("df_contrasts5")
```

Our main effect of interest is the factor $F$. We want to estimate its effect on response times and code it using scaled sum contrasts, such that negative parameter estimates would yield support for our hypothesis that response times are faster in the training group $F2$:

```{r, echo=TRUE}
(contrasts(df_contrasts5$F) <- c(-0.5, +0.5))
```

We run a `brms` model to estimate the effect of factor $F$, i.e., how strongly the response times in the two groups differ from each other.

```{r, echo=TRUE, message=FALSE, results="hide"}
fit_RT_F <- brm(RT ~ F,
                data = df_contrasts5,
                family = gaussian(),
                prior = c(prior(normal(200, 50), class = Intercept),
                          prior(normal(0, 50), class = sigma),
                          prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_RT_F)
```

```{r, echo=FALSE, message=FALSE, results="hide"}
tab5 <- df_contrasts5 %>%
  group_by(F) %>%
  summarize(M = mean(RT), SE = sd(RT) / sqrt(n()))
```

(ref:figRTF) Means and error bars (showing standard errors) for a simulated data set of response times for two different groups of subjects, who have obtained a training in lexical decisions ($F2$) versus have obtained a control training ($F1$).

```{r figRTF, fig=TRUE, include=TRUE, echo=FALSE, cache=FALSE, fig.width=2.5, fig.height=2.2, fig.cap = "(ref:figRTF)"}
(plot_RT <- ggplot(data = tab5, aes(x = F, y = M, ymin = M - SE, ymax = M + SE)) +
  ## geom_bar(stat = "identity") +
  geom_errorbar(width = 0.2) +
   geom_point() +
  labs(y = "Response times (ms)", x = "Group (F)"))
```

We find (see model estimates and data shown in Figure \@ref(fig:figRTF)) that response times in group $F2$ are roughly $25$ ms faster than in group $F1$ (Estimate of $-24$).  This suggests that as expected, the training program that group $F2$ obtained seems to be successful in speeding up response times. Recall that one cannot just look at the 95% credible interval and check whether zero is outside the interval to declare that we have found an effect. To make a discovery claim, we need to run a Bayes factor analysis on this data set to directly test this hypothesis, and this may or may not provide evidence for a difference in response times between groups.

Let's assume we have allocated subjects to the two groups randomly. Let's say that we also measured the IQ of each person using an IQ test. We did so because we expected that IQ could have an influence on response times, and we wanted to control for this influence. We now can check whether the two groups had the same average IQ.

```{r, echo=TRUE, message=FALSE}
df_contrasts5 %>%
  group_by(F) %>%
  summarize(M.IQ = mean(IQ))
```

Group $F2$ not only obtained additional training and had faster response times, group $F2$ also had a higher IQ (mean of 115) on average than group $F1$ (mean IQ = 85). Thus, the random allocation of subjects to the two groups seems to have created--by chance--a difference in IQs. Now we can ask the question: why might response times in group $F2$ be faster than in group $F1$? Is this because of the training program in $F2$? Or is this simply because the average IQ in group $F2$ was higher than in group $F1$? To investigate this question, we add both predictor variables simultaneously in a `brms` model. Before we enter the continuous IQ variable, we center it, by subtracting its mean. Centering covariates is generally good practice. Moreover, it is often important to \index{Z-transform} $z$-transform the covariate, i.e., to not only subtract the mean, but also to divide by its standard deviation (this can be done as follows: `df_contrasts5$IQ.s <- scale(df_contrasts5$IQ)`). The reason why this is often important is that the sampler doesnâ€™t work well if predictors have different scales. For the simple models we use here, the sampler works without $z$-transformation. However, for more realistic and more complex models, $z$-transformation of covariates is often very important. Importantly, when changing the scale of the predictor, then the scale of the prior has to be changed accordingly. Here, we set the same priors for the contrast and for the covariate IQ for convenience. In realistic models, these priors have to be set separately.

```{r, echo=TRUE, message=FALSE, results="hide"}
df_contrasts5 <- df_contrasts5 %>%
  mutate(c_IQ = IQ - mean(IQ))
fit_RT_F_IQ <- brm(RT ~ F + c_IQ,
                   data = df_contrasts5,
                   family = gaussian(),
                   prior = c(prior(normal(200, 50), class = Intercept),
                             prior(normal(0, 50), class = sigma),
                             prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_RT_F_IQ)
```

The results from the `brms` model now show that the difference in response times between groups (i.e., factor $F$) is not estimated to be $-25$ ms any more, but instead, the estimate is about $+7$ ms, and the 95\% credible interval spans the range $-20$ to $33$. Thus, there doesn't seem to be much reason to believe any more that the groups would differ. At the same time, we see that the predictor variable IQ shows a negative effect (Estimate = $-1$ with 95\% credible interval: $-1.7$ to $-0.4$), suggesting that--as expected--response times seem to be faster in subjects with higher IQ.


(ref:figRTFIQ) Response times as a function of individual IQ for two groups with a lexical decision training ($F2$) versus a control training ($F1$). Points indicate individual subjects, and lines with error bands indicate linear regression lines.

```{r figRTFIQ, fig=TRUE, include=TRUE, echo=FALSE, cache=FALSE, message=FALSE, fig.height=2.2, fig.cap = "(ref:figRTFIQ)"}
(plot_RT_IQ <- ggplot(data = df_contrasts5, aes(x = IQ, y = RT, colour = F, linetype = F)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(y = "Response times (ms)"))
```

This result can also be seen in Figure \@ref(fig:figRTFIQ), which shows that response times decrease with increasing IQ, as suggested by the `brms` model. However, the heights of the two \index{Regression line} regression lines do not differ from each other, consistent with the observation in the `brms` model that the effect of factor $F$ did not seem to differ from zero. That is, factor $F$ in the `brms` model estimates the difference in height of the regression line between both groups.
That the height does not differ and the effect of $F$ is estimated to be close to zero suggests that in fact group $F2$ showed faster response times not because of their additional training program. Instead, they had faster response times simply because their IQ was by chance higher on average compared to the control group $F1$. This analysis is the Bayesian equivalence of the frequentist \index{Analysis of covariance} "analysis of covariance" (ANCOVA), where it's possible to estimate a group difference after "controlling for" the influence of a covariate.

We can also see in Figure \@ref(fig:figRTFIQ) that the two regression lines for the two groups are exactly parallel to each other. That is, the influence of IQ on response times seems to be exactly the same in both groups. This is actually an assumption in the ANCOVA analysis that needs to be checked in the data. That is, if we want to estimate the difference between groups after controlling for a covariate (here IQ), we have to investigate whether the influence of the covariate is the same in both groups. We can estimate this by including an \index{Interaction} interaction term between the factor and the covariate in the `brms` model:

```{r, echo=TRUE, message=FALSE, results="hide"}
fit_RT_FxIQ <-
  brm(RT ~ F * c_IQ,
      data = df_contrasts5,
      family = gaussian(),
      prior = c(prior(normal(200, 50), class = Intercept),
                prior(normal(0, 50), class = sigma),
                prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_RT_FxIQ)
```

The estimate for the interaction (the term `F1:c_IQ`) is very small here (close to $0$) and the 95\% credible intervals clearly overlap with zero, showing that the two regression lines are estimated to be very similar, or parallel, to each other. If this is the case, then it is possible to correct for IQ when estimating the group difference. (From the perspective of causal inference, one might need additional requirements to draw strong conclusions here, such as measuring the covariate before the dependent variable, or reasoning based on theoretical plausibility.)

### Estimating differences in slopes

We now take a look at a different data set.

```{r, echo=TRUE}
data("df_contrasts6")
levels(df_contrasts6$F) <- c("simple", "complex")
```

This again contains data from response times (RT) in two groups. Let's assume the two groups have performed two different response time tasks, where one simple RT task doesn't rely on much cognitive processing (group "simple"), whereas the other task is more complex and depends on complex cognitive operations (group "complex"). We therefore expect that RTs in the simple task should be independent of IQ, whereas in the complex task, individuals with a high IQ should be faster in responding compared to individuals with low IQ. Thus, our primary hypothesis of interest states that the influence of IQ on RT differs between conditions. This means that we are interested in the difference between slopes. A slope in a linear regression assesses how strongly the dependent variable (here RT) changes with an increase of one unit on the covariate (here IQ), it thus assesses how "steep" the regression line is. Our hypothesis thus states that the \index{Regression line} regression lines differ between groups.

```{r figRTFxIQ, fig=TRUE, include=TRUE, echo=FALSE, cache=FALSE, message=FALSE, fig.height=2.2, fig.cap = "Response times as a function of individual IQ for two groups performing a simple versus a complex task. Points indicate individual subjects' responses, and lines with error bands show the fitted linear regression lines."}
(plot_RTxIQ <- ggplot(data = df_contrasts6, aes(x = IQ, y = RT, colour = F, linetype = F)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(y = "Response times (ms)"))
```

The results, displayed in Figure \@ref(fig:figRTFxIQ), suggest that the data seem to support our hypothesis. For the subjects performing the complex task, response times seem to decrease with increasing IQ, whereas for subjects performing the simple task, response times seem to be independent of IQ. As stated before, our primary hypothesis relates to the difference in slopes. Statistically speaking, this is assessed in the \index{Interaction} interaction between the factor and the covariate. Thus, we run a `brms` model where the interaction is included. Importantly, we first use scaled \index{Sum contrasts} sum contrasts for the group effect, and again center the \index{Covariate} covariate IQ.

```{r, echo=TRUE, message=FALSE, results="hide"}
contrasts(df_contrasts6$F) <- c(-0.5, +0.5)
df_contrasts6$c_IQ <- df_contrasts6$IQ - mean(df_contrasts6$IQ)
fit_RT_FxIQ2 <- brm(RT ~  F * c_IQ,
                    data = df_contrasts6,
                    family = gaussian(),
                    prior = c(prior(normal(200, 50), class = Intercept),
                              prior(normal(0, 50), class = sigma),
                              prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_RT_FxIQ2)
```

We can see that the main effect of IQ (term `c_IQ`) is negative ($-0.8$) with 95\% credible intervals $-1.5$ to $-0.2$, suggesting that overall response times decrease with increasing IQ. This is qualified by the interaction term, which is estimated to be negative ($-1.6$), with 95\% credible intervals $-2.9$ to $-0.3$. This suggests that the slope in the complex group (which was coded as $+0.5$ in the scaled sum contrast) is more negative than the slope in the simple group (which was coded as $-0.5$ in the scaled sum contrast). Thus, the interaction assesses the difference between slopes.

We can also run a model, where the \index{Nested slope} nested slopes are estimated, i.e., the slope of IQ in the simple group and the slope of IQ in the complex group. This can be implemented by using the nested coding that we learned about in the previous section:

```{r, echo=TRUE, message=FALSE, results="hide"}
fit_RT_FnIQ2 <-
  brm(RT ~ F / c_IQ,
      data = df_contrasts6,
      family = gaussian(),
      prior = c(prior(normal(200, 50), class = Intercept),
                prior(normal(0, 50), class = sigma),
                prior(normal(0, 50), class = b)))
```
```{r, echo=TRUE}
fixef(fit_RT_FnIQ2)
```

Now we see that the slope of IQ in the simple group (`Fsimple:c_IQ`) is estimated to be $0$, with credible intervals clearly including zero. By contrast, the slope in the complex group (`Fcomplex:c_IQ`) is estimated as $-1.6$ (95\% CrI = $[-2.5,-0.7]$). This is consistent with our hypothesis that high IQ speeds up response times for the complex but not for the simple task. (To obtain evidence for this effect, we need Bayes factors, see chapter\ \@ref(ch-bf).)
We can also see from the \index{Nested analysis} nested analysis that the difference in slopes between conditions is $-1.6 - 0.0 = -1.6$. This is exactly the value for the interaction term that we estimated in the previous model, demonstrating that \index{Interaction} interaction terms assess the difference between slopes; i.e., they estimate in how far the regression lines in the two conditions are parallel, with an estimate of $0$ indicating perfectly parallel lines.


Notice that we can compute posterior samples for the nested slopes from the model with the interaction. That is, we can take the model that estimates main effects and the interaction, and compute posterior samples for the slope of IQ in the simple task and the slope of IQ in the complex task. First, we extract the posterior samples from the model.

```{r}
df_postSamp_RT_FxIQ2 <- as_draws_df(fit_RT_FxIQ2)
```

Then, we take a look at the contrast coefficients for the group factor:

```{r, echo=TRUE}
contrasts(df_contrasts6$F)
```

They show a value of $-0.5$ for the simple group. Thus, to compute the slope for the simple group we have to take the overall slope for `c_IQ` and subtract $-0.5$ times the estimate for the interaction:

```{r}
df_postSamp_RT_FxIQ2 <- df_postSamp_RT_FxIQ2 %>%
  mutate(b_c_IQ_simple = b_c_IQ - 0.5 * `b_F1:c_IQ`)
```

Likewise, to estimate the slope for the complex group we have to take the overall slope for `c_IQ` and add $+0.5$ times the estimate for the interaction:

```{r}
df_postSamp_RT_FxIQ2 <- df_postSamp_RT_FxIQ2 %>%
  mutate(b_c_IQ_complex = b_c_IQ + 0.5 * `b_F1:c_IQ`)
```

```{r}
c(IQc.c_simple = mean(df_postSamp_RT_FxIQ2$b_c_IQ_simple),
  IQc.c_complex = mean(df_postSamp_RT_FxIQ2$b_c_IQ_complex))
```

The results show that the posterior means for the slope of `c_IQ` are $0$ and $-1.6$ for the simple and the complex groups, as we had found above in the nested analysis. This is the case because the data are sufficiently informative compared to the data. When the data are sparse or when working with Bayes factors, priors will have to be set carefully for each specific analysis.

In most situations one should always center \index{Covariate} covariates before including them into a model. If covariates are not centered, then the effects (here the effect for the factor) cannot be interpreted as main effects any more.

One can also do analyses with \index{Interaction} interactions between a \index{Covariate} covariate and a factor, but by using different contrast codings. For example, if we use \index{Treatment contrasts} treatment contrasts for the factor, then the main effect of `c_IQ` assesses not the average slope of `c_IQ` across conditions, but instead the \index{Nested slope} nested slope of `c_IQ` within the baseline group of the treatment contrast. The interaction still assesses the difference in slopes between groups. In a situation where there are more than two groups, when one estimates the interaction of contrasts with a covariate, then the contrasts define which slopes are compared with each other in the interaction terms. For example, when using sum contrasts in an example where the influence of IQ is measured on response times for nouns, verbs, and adjectives, then there are two interaction terms: these assess (1) whether the slope of IQ for nouns is different from the average slope across conditions, and (2) whether the slope of IQ for verbs is different from the average slope across conditions. If one uses repeated contrasts in a situation where the influence of IQ on response times is estimated for word frequency conditions "low", "medium-low"m "medium-high"m and "high", then there are three interaction terms (one for each contrast). The first interaction term estimates the difference in slopes between "low" and "medium-low" word frequencies, the second interaction term estimates the difference in slopes between "medium-low" and "medium-high" word frequencies, and the third interaction term estimates the difference in slopes between "medium-high" and "high" word frequency conditions. Thus, the logic of how contrasts specify certain comparisons between conditions extends directly to the situation where differences in slopes are estimated.

## Interactions in generalized linear models (with non-linear link functions) and non-linear models {#sec-interactions-NLM}

Next, we look at \index{Generalized linear models} generalized linear models, where a linear predictor is passed through a non-linear \index{Link function} link function to predict the dependent variable. Examples of generalized linear models include \index{Logistic regression} logistic regression models and models assuming  a Poisson distribution. Even though a log-normal model is a linear model on a log-transformed dependent variable, the same techniques apply to this type of model since the logarithm transform is not linear. Here, we treat an example with a logistic model in a $2 \times 2$ factorial between-subject design. The logistic model has the following non-linear link function called the \index{Logistic function} logistic function: $P(y=1 \mid \eta) = \frac{1}{1 + \exp(-\eta)}$, where $\eta$ is the latent linear predictor. For example, in our $2 \times 2$ factorial design with main effects A and B and their interaction, $\eta$ is computed as a linear combination of the intercept plus the main effects and their interaction: $\eta = 1 + \beta_A x_A + \beta_B x_B + \beta_{A \times B} x_{A \times B}$.

Thus, there is a latent level of linear predictions ($\eta$), which are then passed through a non-linear link function to predict the probability that the observed data is a success ($P(y = 1)$). We will use this logistic model to analyze an example data set where the dependent variable is dichotomous, coded as either a $1$ (indicating success) or a $0$ (indicating failure).

We load a simulated data set where the dependent variable codes whether a subject performed a task successfully ($pDV = 1$) or not ($pDV = 0$). Moreover, the data set has two between-subject factors A and B. The means for each of the four conditions are shown in Table\ \@ref(tab:cTab7Means).

```{r}
data("df_contrasts7")
```

```{r cTab7Means, echo=FALSE, message=FALSE, results = "asis"}
table7 <- df_contrasts7 %>% group_by(A, B) %>% # plot interaction
  summarize(N = length(pDV), M = mean(pDV))
GM <- mean(table7$M) # Grand Mean
table7a <- as.data.frame(table7)
names(table7a) <- c(
  "Factor A", "Factor B", "N data",
  "Means"
)
# apa_table(table7a, placement="b", digits=2,
#    caption="Summary statistics per condition for the simulated data.")
kable(table7a %>%
  as.data.frame() %>%
  mutate(across(where(is.double), ~ paste0("$",formatC(round(.x, 1),format = "f", digits=1), "$"))) %>%
   mutate(across(where(is.integer), ~ paste0("$",.x, "$"))),
  escape =FALSE, booktabs = T, vline = "", digits = 2, caption = "Summary statistics per condition for the simulated data.") # format="latex",
```

To analyze this data, we use scaled \index{Sum contrasts} sum contrasts, as we had done above for the \index{$2 \times 2$ design} $2 \times 2$ design with response times as the dependent variable; this allows us to interpret the coefficients directly as \index{Main effects} main effects. Next, we fit a `brms` model. The model specification is the same as the model with response times--with two differences: First, the \index{\texttt{family}} `family` argument is now specified as \index{\texttt{bernoulli}} \index{\texttt{logit}} `family = bernoulli(link = "logit")` to indicate the logistic model. We do not specify a prior for `sigma`, since there is no residual standard deviation in a logistic model.

```{r, echo=TRUE, message=FALSE, results="hide"}
contrasts(df_contrasts7$A) <- c(-0.5, +0.5)
contrasts(df_contrasts7$B) <- c(-0.5, +0.5)
fit_pDV_AB.sum <- brm(pDV ~ A * B,
                      data = df_contrasts7,
                      family = bernoulli(link = "logit"),
                      prior = c(prior(normal(0, 3), class = Intercept),
                                prior(normal(0, 3), class = b)))
```
```{r, echo=TRUE}
fixef(fit_pDV_AB.sum)
```

The results from this analysis show that the estimates for the two main effects ($A1$ and $B1$) as well as the interaction ($A1:B1$) are positive and the 95\% credible intervals do not include zero. If we want to make a discovery claim, we would need to perform Bayes factor analyses to investigate the evidence that there is for each of the effects.

Next, we discuss how we can obtain \index{Model predictions} model predictions for each of the four experimental conditions for this generalized linear model. To obtain such predictions, we first take a look at the \index{Contrast matrix} contrast matrix. We simultaneously have contrasts for two main effects and one interaction:

```{r, echo=TRUE, message=FALSE, results="hide", echo = FALSE}
tab7 <- df_contrasts7 %>%
  group_by(A, B) %>%
  summarize() %>%
  model.matrix(~ A * B, data = .) %>%
  as.data.frame()
row.names(tab7) <- c("A1_B1", "A1_B2", "A2_B1", "A2_B2")
tab7
```

We obtain the posterior samples for the estimates from the model:

```{r, echo=TRUE}
df_postSamp_pDV <- as.data.frame(fit_pDV_AB.sum)
```

From these, we can compute the posterior samples for the linear predictions for each group. We see in the contrast matrix how we have to combine the posterior samples for the intercept, main effects, and interaction to obtain \index{Latent linear prediction} latent linear predictions for each condition. The first condition (design cell $A1$, $B1$) has a weight of $1$ for the intercept, and then weights of $-0.5$ (for the main effect of $A$), $-0.5$ (for the main effect of $B$), and $0.25$ (for the interaction). The posterior samples for the other conditions are computed accordingly.

```{r, echo=TRUE}
df_postSamp_pDV <- df_postSamp_pDV %>%
  mutate(A1_B1 = b_Intercept - 0.5 * b_A1 - 0.5 * b_B1 +
           0.25 * `b_A1:B1`,
         A1_B2 = b_Intercept - 0.5 * b_A1 + 0.5 * b_B1 -
           0.25 * `b_A1:B1`,
         A2_B1 = b_Intercept + 0.5 * b_A1 - 0.5 * b_B1 -
           0.25 * `b_A1:B1`,
         A2_B2 = b_Intercept + 0.5 * b_A1 + 0.5 * b_B1 +
           0.25 * `b_A1:B1`)
```

Now, we have computed posterior samples for estimates of the latent linear predictor $\eta$ for each experimental condition. We can look at the posterior means:

```{r, echo=TRUE}
df_postSamp_pDV %>%
  select(A1_B1, A1_B2, A2_B1, A2_B2) %>%
  colMeans()
```


This shows that these values are not on the probability scale. Instead, they are on the \index{Log-odds} (log-odds) scale of the latent linear predictor $\eta$. For presentation and interpretation of the results, it might be much more informative to look at the condition means in terms of the probabilities of success in each of the four conditions. Given that we have the linear predictions for each condition, this can be easily computed by sending all posterior samples for the linear predictions through the link function. Applying the \index{Logistic function} logistic function \index{\texttt{plogis}} (`plogis()` in R) transforms the linear predictors to the \index{Probability scale} probability scale:^[The same (with lower precision) can be achieved using `1/(1+exp(-.))`.]

```{r, echo=TRUE}
df_postSamp_pDV <- df_postSamp_pDV %>%
  mutate(p_A1_B1 = plogis(A1_B1),
         p_A1_B2 = plogis(A1_B2),
         p_A2_B1 = plogis(A2_B1),
         p_A2_B2 = plogis(A2_B2))
```

Now, we have posterior samples for each condition on the probability scale. We can take a look at the posterior means, and see that these closely correspond to the probabilities in the data that we have seen above in Table\ \@ref(tab:cTab7Means).

```{r, echo=TRUE}
df_postSamp_pDV %>%
  select(p_A1_B1, p_A1_B2, p_A2_B1, p_A2_B2) %>%
  colMeans()
```

Of course, the advantage is that we now have posterior samples for these conditions available, and can compute posterior 95\% credible intervals (also see Figure \@ref(fig:figpDV)). Rather than computing these manually, the function \index{\texttt{conditional\_effects}} `conditional_effects()` can do this for us. By default, all main effects and two-way interactions estimated in the model are shown (this can be changed by including, for example, `effects = "A:B"`).


```{r}
conditional_effects(fit_pDV_AB.sum, robust = FALSE)[]
```

We plot the two-way \index{Interaction} interaction using `brms`,  embedding the `conditional_effects()` call in `plot(.)[[1]]`. This allows us to select the first (and here the only) `ggplot2`  element and to customize it.

(ref:figpDv) Means and 95 percent posterior credible intervals for a simulated data set of successful task performance in a $2 \times 2$ design.

```{r figpDV, include=TRUE, fig.width=3.5, fig.height=2.2, fig.cap = "(ref:figpDv)"}
plot(conditional_effects(fit_pDV_AB.sum,
                         effects = "A:B",
                         robust = FALSE),
     plot = FALSE)[[1]] +
  labs(y = "Success probability")
```

As a final remark, certain types of interactions might be present at the probability scale, but not at the logit scale (or the other way round); this can happen for any non-linear transformation [see @loftus1978interpretation; @wagenmakers2012interpretation].

## Summary

To summarize, we showed that for contrasts in the context of $2 \times 2$ designs, depending on the contrast coding, the factors estimated nested effects or main effects and interactions. We also saw that it is possible to code contrasts for a $2 \times 2$ design by creating one factor comprising all design cells, and by specifying all effects of interest in a single contrast matrix. In designs with one factor and one covariate it is possible to "control" for group-differences for differences in the covariate (ANCOVA), or to estimate the extent to which regression slopes are parallel in different experimental conditions. Last, in generalized linear models with non-linear link functions it is possible to obtain posterior samples not only on the latent scale of linear predictors, but also on the scale of the response.

## Further reading

Analysis of variance is discussed in detail in @dead. A practical book on ANOVA using R is @faraway2002practical.
