# (PART) Model comparison {-}
# Introduction to model comparison {#ch-comparison}

A key goal of cognitive science is to decide which theory under consideration accounts for the experimental data better. This can be accomplished by implementing the theories (or some aspects of them) as Bayesian models and comparing their predicting power. Thus, \index{Model comparison} model comparison and \index{Hypothesis testing} hypothesis testing are closely related ideas. There are two Bayesian perspectives on model comparison: a \index{Prior predictive perspective} *prior* predictive perspective based on the \index{Bayes factor} Bayes factor using \index{Marginal likelihood} marginal likelihoods, and a \index{Posterior predictive perspective} *posterior* predictive perspective based on \index{Cross-validation} cross-validation. The main characteristic difference between the prior predictive approach (Bayes factor) versus the posterior predictive approach (cross-validation) is the following: The Bayes factor examines how well the model (prior and likelihood) explains the experimental data. By contrast, the posterior predictive approach assesses model predictions for held-out data after seeing most of the data. 

## Prior predictive vs. posterior predictive model comparison

The \index{Predictive accuracy} predictive accuracy of the Bayes factor is only based on its \index{Prior predictive distribution} prior predictive distribution.  In Bayes factor analyses, the prior model predictions are used to evaluate the support that the data give to the model. By contrast, in cross-validation, the model is fit to a large subset of the data (i.e., the \index{Training data} training data). The posterior distributions of the parameters of this fitted model are then used to make predictions for held-out or \index{Validation data} validation data, and \index{Model fit} model fit is assessed on this subset of the data. Typically, this process is repeated several times, until the subsets of the entire data set are assessed as held-out data. This approach attempts to assess whether the model will generalize to truly new, \index{Unobserved data} unobserved data. Of course, the \index{Held-out data} held-out data is usually not "truly new" because it is part of the data that was collected, but at least it is data that the model has not been exposed to. That is, the predictive accuracy of cross-validation methods is based on how well the \index{Posterior predictive distribution} posterior predictive distribution that is fit to most of the data (i.e., the training data) characterizes \index{Out-of-sample data} out-of-sample data (i.e., the test or held-out data). Notice that one could in principle use the posterior predictive approach using truly new data, by just repeating the experiment with new subjects and then treating that new data as held-out data.

Coming back to Bayes factors, the prior predictive distribution is obviously highly sensitive to the priors: it evaluates the probability of the observed data under prior assumptions. By contrast, the posterior predictive distribution is less dependent on the priors because the priors are combined with the likelihood (and are thus less influential, given sufficient data) before making predictions for held-out validation data.

Jaynes [-@jaynes2003probability, Chapter 20] compares these two  perspectives to  "a cruel realist" and "a fair judge".  According to Jaynes, Bayes factor adopts the posture of a cruel realist, who "judge[s] each model taking into account the prior information we actually have pertaining to it; that is, we penalize a model if we do not have the best possible prior information about its parameters, although that is not really a fault of the model itself."  By contrast, cross-validation adopts the posture of a scrupulously fair judge, "who insists that fairness in comparing models requires that each is delivering the best performance of which it is capable, by giving each the best possible prior probability for its parameters (similarly, in Olympic games we might consider it unfair to judge two athletes by their performance when one of them is sick or injured; the fair judge might prefer to compare them when both are doing their absolute best)."

## Some important points to consider when comparing models

Regardless of whether we use Bayes factor or cross-validation or any other method for model comparison, there are several important points that one should keep in mind:

1. Although the objective of model comparison might ultimately be to find out which of the models under consideration generalizes better, this generalization can only be done well within the range of the observed data [see @VehtariLampinen2002;  @VehtariOjanen2012]. That is, if one hypothesis implemented as the model $\mathcal{M}_1$ shows to be superior to a second hypothesis, implemented as the model $\mathcal{M}_2$, according to Bayes factor and/or cross-validation and evaluated with a young western university student population, this doesn't mean that $\mathcal{M}_1$ will be superior to $\mathcal{M}_2$ when it is evaluated with a broader population  [and in fact it seems that many times it won't, see @henrich_heine_norenzayan_2010]. However, if we can't generalize even within the range of the observed data (e.g., university students in the northern part of the western hemisphere), there is no hope of generalizing outside of that range (e.g., non-University students). @navarroDevilDeepBlue2018 argues that one of the most important functions of a model is to encourage directed exploration of new territory; our view is that this makes sense only if historical data can also be accounted for. In practice, what that means for us is that evaluating a model's performance should be carried out using \index{Historical benchmark data} historical benchmark data in addition to any new data one has; just using isolated pockets of new data to evaluate a model is not convincing. For examples from psycholinguistics of model evaluation using historical benchmark data, see @NicenboimPreactivation2019 and @YadavetalJML2022.

2. Model comparison can provide a quantitative way to evaluate models, but this cannot replace understanding the qualitative patterns in the data [see, e.g., @navarroDevilDeepBlue2018]. A model can provide a good fit by behaving in a way that contradicts our substantive knowledge. For example, @lisson_2020 examine two computational models of sentence comprehension. One of the models yielded higher predictive accuracy when the parameter that is related to the probability of correctly comprehending a sentence was higher for impaired subjects (individuals with aphasia) than for the control population. This contradicts  \index{Domain knowledge} domain knowledge---impaired subjects are generally observed to show worse performance than unimpaired control subjects---and led to a re-evaluation of the model.

3. Model comparison is based on finding the most "useful model" for characterizing our data, but neither the Bayes factor nor cross-validation (nor any other method that we are aware of) guarantees selecting the \index{Model closest to the truth} model closest to the truth (even with enough data). This is related to our previous point: A model that's closest to the true generating data process is not guaranteed to produce the best (prior or posterior) predictions, and a model with a clearly wrong generating data process is not guaranteed to produce poor (prior or posterior) predictions. See @WangGelman2014difficulty, for an example with cross-validation; and @navarroDevilDeepBlue2018 for a toy example with Bayes factors.

4.  One should also check that the precision of the data being modeled is high; if an effect is being modeled that has high uncertainty (the posterior distribution of the target parameter is widely spread out), then any measure of model fit can be uninformative because we don't have accurate estimates of the effect of interest. In the Bayesian context, this implies that the posterior predictive distributions of the effects generated by the model should be theoretically plausible and reasonably constrained, and the target parameter of interest should have as high precision as possible; this implies that we need to have sufficient data if we want to obtain precise estimates of the parameter of interest. What counts as sufficient will depend on the topic being studied.^[As an example from psycholinguistics, strong garden-path effects like those elicited by "The horse (that was) raced past the barn fell" [@bever1970cognitive] may be easy to detect with high precision with a relatively small number of subjects, but subtle effects such as local coherence [@taboretal04] will probably require a much larger sample size to detect the effect with high precision [@lcpaape2024].] Later in this part of the book, we will discuss the adverse impact of imprecision in the data on model comparison (see section \@ref(sec-BFvar)). We will show that, in the face of low precision, we generally won't learn much from model comparison.  

5. When comparing a \index{Null model} null model with an \index{Alternative model} alternative model, it is important to be clear about what the null model specification is. For example, in section \@ref(sec-mcvivs), we encountered the correlated varying intercepts and varying slopes model for the Stroop effect. The `brms` formula for the full model  was:

`n400 ~ 1 + c_cloze + (1 + c_cloze | subj)`

The formal statement of this model is: 

 \begin{equation}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + c\_cloze_n \cdot (\beta+ u_{subj[n],2}),\sigma)
 \end{equation}


If we want to test the null hypothesis that centered cloze has no effect on the dependent variable, one null model is:

`n400 ~ 1 + (1 + c_cloze | subj) (Model M0a)`

Formally, this would be stated as follows (the $\beta$ term is removed as it is assumed to be $0$):

 \begin{equation}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + c\_cloze_n \cdot u_{subj[n],2},\sigma)
 \end{equation}


In model `M0a`, by-subject variability is allowed; just the population-level (or fixed) effect of centered cloze is assumed to be zero. This is called a \index{Nested model comparison} nested model comparison, because the null model is subsumed in the full model.

An alternative null model could remove only the \index{Varying slopes} varying slopes:

`n400 ~ 1 + c_cloze + (1 | subj) (Model M0b)`

Formally:

 \begin{equation}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + c\_cloze_n \cdot \beta,\sigma)
 \end{equation}


Model `M0b`, which is also nested inside the full model, can be used to test a different null hypothesis than `M0a` above: is the between-subject variability in the centered cloze effect zero? 

Yet another possibility is to remove both the population-level and group-level (or random) effects of centered cloze:

`n400 ~ 1 + (1 | subj) (Model M0c)`


Formally:

 \begin{equation}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1},\sigma)
 \end{equation}


Model `M0c` is also nested inside the full model, but it now has two parameters missing instead of one: $\beta$ and $u_{subj[n],1}$. Usually, it is best to compare models by removing one parameter; otherwise one cannot be sure which parameter was responsible for our rejecting or accepting the null hypothesis. 

\Begin{extra}
<div class="extra">

```{theorem, null}
**Credible intervals should not be used to reject a null hypothesis**
```

Researchers often incorrectly use \index{Credible intervals for null hypothesis testing} credible intervals for null hypothesis testing, that is, to test whether a parameter $\beta$ is zero or not. A common approach is to check whether zero is included in the 95\% credible interval for the parameter $\beta$; if it is, then the null hypothesis that the effect is zero is accepted; and if zero is outside the interval, then the null is rejected. For example, in a  tutorial paper that two of the authors of this book wrote [@NicenboimVasishth2016], we incorrectly suggest that the credible interval can be used to reject the hypothesis that the $\beta$ is zero. This is generally not the correct approach. 
The problem with this approach is that it is a  heuristic that will work in some cases and might be misleading in others [for an example, see @SampleSizeCBB2021]. Unfortunately, when they will work or not is in fact not well-defined.

Why is the credible-interval approach only a \index{Heuristic} heuristic?
One line of (generally incorrect) reasoning that justifies looking at the overlap between credible intervals and zero is based on the fact that the most likely values of $\beta$ lie within 95% credible interval.^[This is also strictly true only in a highest density interval (HDI), this is a credible interval where all the points within the interval have a higher probability density than points outside the interval. However,  when posterior distributions are symmetrical, these intervals are virtually identical to the equal-tail intervals we use in this book.] This entails that if zero is outside the interval, it must have a low probability density. This is true, but it's meaningless: Regardless of where zero lies (or any point value), zero will have a probability mass of exactly zero since we are dealing with a continuous distribution.
The lack of overlap doesn't tell us how much posterior probability the null model has. 

A partial solution could be to look at a probability interval close to zero rather than zero (e.g., an interval of, say, $-2$ to $2$ ms in a response time experiment), so that we obtain a non-zero probability mass. While the lack of overlap would be slightly more informative, excluding a small interval can be problematic when the prior probability mass of that interval is very small to begin with (as was the case with the regularizing priors we assigned to our parameters). @rouder2018bayesian show that if prior probability mass is added to the point value zero using a \index{Spike-and-slab prior} *spike-and-slab* prior (or if probability mass is  added to the small interval close to zero if one considers that equivalent to the null model), looking at whether zero is in the 95% credible interval is analogous to the Bayes factor. Unfortunately, the *spike-and-slab* prior cannot be incorporated in Stan, because it relies on a discrete parameter. However, other programming tools (like PyMC3, JAGS, or Turing) can be used if such a prior needs to be fit; see the further readings at the end of the chapter.

Rather than looking at the overlap of the 95% credible interval, we might be tempted to conclude that there is evidence for an effect because the probability that a parameter is positive is high, that is $P(\beta > 0) >> 0.5$. However, the same logic from the previous paragraph renders this meaningless. Given that the probability mass of a point value, $P(\beta = 0)$, is zero, what we can conclude from $P(\beta > 0) >> 0.5$ is that $\beta$ is very likely to be positive rather than negative, but we can't make any assertions about whether $\beta$ is exactly zero. 

As we saw, the main problem with these heuristics is that they ignore that the \index{Null model} null model is a separate hypothesis. In many situations, the null hypothesis may not be of interest, and it might be perfectly fine to base our conclusions on credible intervals or $P(\beta > 0)$. The problem arises when these heuristics are used to provide \index{Evidence in favor or against the null hypothesis} evidence in favor or against the null hypothesis. If one wants to argue about the evidence in favor of or against a null hypothesis, Bayes factors or cross-validation will be needed. These are discussed in the next two chapters.

How can credible intervals be used sensibly? The \index{Region of practical equivalence} region of practical equivalence \index{ROPE} (ROPE) approach [@spiegelhalter1994bayesian; @Freedman1984; and, more recently, @kruschke2018bayesian; @kruschke2014doing] is a reasonable alternative to hypothesis testing and arguing for or against a null. This approach is related to the spike-and-slab discussion above. In the ROPE approach, one can define a range of values for a target parameter that is predicted before the data are seen. Of course, there has to be a principled justification for choosing this range a priori; an example of a principled justification would be the prior predictions of a computational model. Then, the overlap (or lack thereof) between this predicted range and the observed credible interval can be used to infer whether one has estimates consistent (or partly consistent) with the predicted range. Here, we are not ruling out any null hypothesis, and we are not using the credible interval to make a decision like "the null hypothesis is true/false."  

There is one situation where credible intervals could arguably be used to carry out a null hypothesis test. When priors are flat, credible intervals can show frequentist properties, making it reasonable to check whether zero falls within the credible interval. For example, @newall2023evaluation use credible intervals as confidence intervals after calibration. They explicitly verify that 5% of the 95% credible intervals exclude zero when no effect exists. When using such an approach, a verification step would be necessary. We don't discuss this approach any further because our aim in this part of the book is not to derive frequentist statistics from Bayesian analysis, but to use Bayesian methods for obtaining posterior probabilities and Bayes factors, focusing on Bayesian hypothesis testing.


</div>
\End{extra}



## Further reading
@rp and @pittWhenGoodFit2002 argue for the need of going beyond "a good fit" (this is a good posterior predictive check in the context of Bayesian data analysis) and argue for the need of model comparison and a focus on measuring the generalizability of a model. @navarroDevilDeepBlue2018  deals with the problematic aspects  of model selection in the context of psychological literature and cognitive modeling. Fabian Dablander's blog post, https://fabiandablander.com/r/Law-of-Practice.html, shows a very clear comparison between Bayes factor and PSIS-LOO-CV.  @rodriguez2021and provides JAGS code for fitting models with spike-and-slab priors. Fabian Dablander has a comprehensive blog post on how to implement a Gibbs sampler in R when using such a prior: https://fabiandablander.com/r/Spike-and-Slab.html. @YadavetalJML2022 uses 17 different data sets for model comparison using cross-validation, holding out each data set successively; this is an example of evaluating the predictive performance of a model on truly new data. 
